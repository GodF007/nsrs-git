# 日志聚合架构设计详解

基于ELK Stack的NSRS号卡资源管理系统日志聚合解决方案

## 目录

- [日志聚合概述](#日志聚合概述)
- [核心组件设计](#核心组件设计)
- [ELK Stack集成](#elk-stack集成)
- [日志规范](#日志规范)
- [业务场景应用](#业务场景应用)
- [监控告警](#监控告警)
- [最佳实践](#最佳实践)

## 日志聚合概述

### 设计目标

- **集中收集**: 统一收集所有微服务的日志数据
- **实时处理**: 实时解析、过滤和转换日志
- **快速检索**: 支持全文检索和复杂查询
- **可视化展示**: 提供丰富的日志分析和可视化
- **告警通知**: 基于日志内容的智能告警

### 架构图

```
NSRS日志聚合架构：

┌─────────────────────────────────────────────────────────────┐
│                    微服务集群                                │
├─────────────────────────────────────────────────────────────┤
│ ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐ │
│ │User Svc │ │IMSI Svc │ │SIM Svc  │ │Bill Svc │ │Report   │ │
│ │         │ │         │ │         │ │         │ │Svc      │ │
│ └────┬────┘ └────┬────┘ └────┬────┘ └────┬────┘ └────┬────┘ │
└──────┼──────────┼──────────┼──────────┼──────────┼──────┘
       │          │          │          │          │
       ▼          ▼          ▼          ▼          ▼
┌─────────────────────────────────────────────────────────────┐
│                    Filebeat                                 │
│              (日志收集代理)                                  │
└─────────────────────┬───────────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────────┐
│                   Logstash                                  │
│            (日志解析与转换)                                  │
└─────────────────────┬───────────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────────┐
│                 Elasticsearch                               │
│              (日志存储与索引)                                │
└─────────────────────┬───────────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────────┐
│                   Kibana                                    │
│              (日志可视化分析)                                │
└─────────────────────────────────────────────────────────────┘
```

## 核心组件设计

### 1. 日志级别枚举

```java
/**
 * 日志级别
 */
public enum LogLevel {
    TRACE("TRACE", "跟踪", 0),
    DEBUG("DEBUG", "调试", 1),
    INFO("INFO", "信息", 2),
    WARN("WARN", "警告", 3),
    ERROR("ERROR", "错误", 4),
    FATAL("FATAL", "致命", 5);
    
    private final String code;
    private final String description;
    private final int level;
    
    LogLevel(String code, String description, int level) {
        this.code = code;
        this.description = description;
        this.level = level;
    }
    
    // getters...
}

/**
 * 日志类型
 */
public enum LogType {
    BUSINESS("BUSINESS", "业务日志"),
    SYSTEM("SYSTEM", "系统日志"),
    ACCESS("ACCESS", "访问日志"),
    ERROR("ERROR", "错误日志"),
    AUDIT("AUDIT", "审计日志"),
    PERFORMANCE("PERFORMANCE", "性能日志");
    
    private final String code;
    private final String description;
    
    LogType(String code, String description) {
        this.code = code;
        this.description = description;
    }
    
    // getters...
}
```

### 2. 日志实体设计

```java
/**
 * 结构化日志实体
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class StructuredLog {
    
    /**
     * 日志ID
     */
    private String logId;
    
    /**
     * 时间戳
     */
    private LocalDateTime timestamp;
    
    /**
     * 日志级别
     */
    private LogLevel level;
    
    /**
     * 日志类型
     */
    private LogType type;
    
    /**
     * 服务名称
     */
    private String serviceName;
    
    /**
     * 实例ID
     */
    private String instanceId;
    
    /**
     * 链路追踪ID
     */
    private String traceId;
    
    /**
     * Span ID
     */
    private String spanId;
    
    /**
     * 用户ID
     */
    private String userId;
    
    /**
     * 请求ID
     */
    private String requestId;
    
    /**
     * 日志消息
     */
    private String message;
    
    /**
     * 异常信息
     */
    private String exception;
    
    /**
     * 堆栈跟踪
     */
    private String stackTrace;
    
    /**
     * 业务模块
     */
    private String module;
    
    /**
     * 操作类型
     */
    private String operation;
    
    /**
     * 扩展字段
     */
    private Map<String, Object> extra;
    
    /**
     * 主机名
     */
    private String hostname;
    
    /**
     * IP地址
     */
    private String ipAddress;
}
```

### 3. 日志工具类

```java
/**
 * 结构化日志工具类
 */
@Component
@Slf4j
public class StructuredLogger {
    
    private static final String SERVICE_NAME = "nsrs-service";
    private static final ObjectMapper objectMapper = new ObjectMapper();
    
    /**
     * 记录业务日志
     */
    public void logBusiness(String module, String operation, String message, Object... args) {
        StructuredLog log = createBaseLog(LogType.BUSINESS, LogLevel.INFO)
            .module(module)
            .operation(operation)
            .message(formatMessage(message, args))
            .build();
        
        outputLog(log);
    }
    
    /**
     * 记录错误日志
     */
    public void logError(String module, String operation, String message, Throwable throwable) {
        StructuredLog log = createBaseLog(LogType.ERROR, LogLevel.ERROR)
            .module(module)
            .operation(operation)
            .message(message)
            .exception(throwable.getClass().getSimpleName())
            .stackTrace(getStackTrace(throwable))
            .build();
        
        outputLog(log);
    }
    
    /**
     * 记录审计日志
     */
    public void logAudit(String userId, String operation, String resource, 
                        String action, Map<String, Object> details) {
        Map<String, Object> extra = new HashMap<>();
        extra.put("resource", resource);
        extra.put("action", action);
        if (details != null) {
            extra.putAll(details);
        }
        
        StructuredLog log = createBaseLog(LogType.AUDIT, LogLevel.INFO)
            .userId(userId)
            .operation(operation)
            .message(String.format("用户 %s 对资源 %s 执行 %s 操作", userId, resource, action))
            .extra(extra)
            .build();
        
        outputLog(log);
    }
    
    /**
     * 记录性能日志
     */
    public void logPerformance(String operation, long duration, Map<String, Object> metrics) {
        Map<String, Object> extra = new HashMap<>();
        extra.put("duration", duration);
        if (metrics != null) {
            extra.putAll(metrics);
        }
        
        StructuredLog log = createBaseLog(LogType.PERFORMANCE, LogLevel.INFO)
            .operation(operation)
            .message(String.format("操作 %s 耗时 %d ms", operation, duration))
            .extra(extra)
            .build();
        
        outputLog(log);
    }
    
    private StructuredLog.StructuredLogBuilder createBaseLog(LogType type, LogLevel level) {
        return StructuredLog.builder()
            .logId(UUID.randomUUID().toString())
            .timestamp(LocalDateTime.now())
            .level(level)
            .type(type)
            .serviceName(SERVICE_NAME)
            .instanceId(getInstanceId())
            .traceId(getTraceId())
            .spanId(getSpanId())
            .requestId(getRequestId())
            .hostname(getHostname())
            .ipAddress(getIpAddress());
    }
    
    private void outputLog(StructuredLog log) {
        try {
            String jsonLog = objectMapper.writeValueAsString(log);
            
            // 根据日志级别输出到不同的logger
            switch (log.getLevel()) {
                case ERROR:
                case FATAL:
                    log.error(jsonLog);
                    break;
                case WARN:
                    log.warn(jsonLog);
                    break;
                case DEBUG:
                    log.debug(jsonLog);
                    break;
                case TRACE:
                    log.trace(jsonLog);
                    break;
                default:
                    log.info(jsonLog);
            }
        } catch (Exception e) {
            log.error("输出结构化日志失败", e);
        }
    }
    
    private String formatMessage(String message, Object... args) {
        if (args == null || args.length == 0) {
            return message;
        }
        return String.format(message, args);
    }
    
    private String getStackTrace(Throwable throwable) {
        StringWriter sw = new StringWriter();
        PrintWriter pw = new PrintWriter(sw);
        throwable.printStackTrace(pw);
        return sw.toString();
    }
    
    private String getTraceId() {
        // 从SkyWalking获取TraceId
        return TraceContext.traceId();
    }
    
    private String getSpanId() {
        // 从SkyWalking获取SpanId
        return TraceContext.spanId();
    }
    
    private String getRequestId() {
        // 从请求上下文获取RequestId
        return RequestContextHolder.getRequestId();
    }
    
    private String getInstanceId() {
        return System.getProperty("instance.id", "unknown");
    }
    
    private String getHostname() {
        try {
            return InetAddress.getLocalHost().getHostName();
        } catch (Exception e) {
            return "unknown";
        }
    }
    
    private String getIpAddress() {
        try {
            return InetAddress.getLocalHost().getHostAddress();
        } catch (Exception e) {
            return "unknown";
        }
    }
}
```

## ELK Stack集成

### 1. ELK集群部署

```yaml
# docker-compose-elk.yml
version: '3.8'

services:
  # Elasticsearch集群
  elasticsearch-master:
    image: elasticsearch:7.17.0
    container_name: elasticsearch-master
    environment:
      - node.name=elasticsearch-master
      - cluster.name=nsrs-elk-cluster
      - discovery.seed_hosts=elasticsearch-data1,elasticsearch-data2
      - cluster.initial_master_nodes=elasticsearch-master
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - xpack.security.enabled=false
      - node.master=true
      - node.data=false
      - node.ingest=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elk-es-master-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    networks:
      - elk-network

  elasticsearch-data1:
    image: elasticsearch:7.17.0
    container_name: elasticsearch-data1
    environment:
      - node.name=elasticsearch-data1
      - cluster.name=nsrs-elk-cluster
      - discovery.seed_hosts=elasticsearch-master,elasticsearch-data2
      - cluster.initial_master_nodes=elasticsearch-master
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms4g -Xmx4g"
      - xpack.security.enabled=false
      - node.master=false
      - node.data=true
      - node.ingest=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elk-es-data1:/usr/share/elasticsearch/data
    networks:
      - elk-network

  elasticsearch-data2:
    image: elasticsearch:7.17.0
    container_name: elasticsearch-data2
    environment:
      - node.name=elasticsearch-data2
      - cluster.name=nsrs-elk-cluster
      - discovery.seed_hosts=elasticsearch-master,elasticsearch-data1
      - cluster.initial_master_nodes=elasticsearch-master
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms4g -Xmx4g"
      - xpack.security.enabled=false
      - node.master=false
      - node.data=true
      - node.ingest=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elk-es-data2:/usr/share/elasticsearch/data
    networks:
      - elk-network

  # Logstash
  logstash:
    image: logstash:7.17.0
    container_name: logstash
    environment:
      - "LS_JAVA_OPTS=-Xms2g -Xmx2g"
    volumes:
      - ./logstash/config:/usr/share/logstash/config
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    ports:
      - "5044:5044"
      - "9600:9600"
    depends_on:
      - elasticsearch-master
    networks:
      - elk-network

  # Kibana
  kibana:
    image: kibana:7.17.0
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch-master:9200
      - SERVER_NAME=kibana
      - SERVER_HOST=0.0.0.0
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch-master
    networks:
      - elk-network

  # Filebeat
  filebeat:
    image: elastic/filebeat:7.17.0
    container_name: filebeat
    user: root
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/log:/var/log:ro
    depends_on:
      - logstash
    networks:
      - elk-network

volumes:
  elk-es-master-data:
  elk-es-data1:
  elk-es-data2:

networks:
  elk-network:
    driver: bridge
```

### 2. Logstash配置

```ruby
# logstash/pipeline/nsrs-logs.conf
input {
  beats {
    port => 5044
  }
}

filter {
  # 解析JSON格式的日志
  if [fields][log_type] == "application" {
    json {
      source => "message"
    }
    
    # 解析时间戳
    date {
      match => [ "timestamp", "yyyy-MM-dd'T'HH:mm:ss.SSS" ]
      target => "@timestamp"
    }
    
    # 添加地理位置信息
    if [ipAddress] {
      geoip {
        source => "ipAddress"
        target => "geoip"
      }
    }
    
    # 解析异常堆栈
    if [exception] {
      mutate {
        add_field => { "has_exception" => "true" }
      }
    }
    
    # 添加索引前缀
    mutate {
      add_field => { "index_prefix" => "nsrs-logs" }
    }
  }
  
  # 解析访问日志
  if [fields][log_type] == "access" {
    grok {
      match => { 
        "message" => "%{COMBINEDAPACHELOG}" 
      }
    }
    
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    
    mutate {
      add_field => { "index_prefix" => "nsrs-access" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch-master:9200"]
    index => "%{[index_prefix]}-%{+YYYY.MM.dd}"
    template_name => "nsrs-logs"
    template_pattern => "nsrs-*"
    template => "/usr/share/logstash/templates/nsrs-template.json"
  }
  
  # 输出到控制台用于调试
  stdout {
    codec => rubydebug
  }
}
```

### 3. Filebeat配置

```yaml
# filebeat/filebeat.yml
filebeat.inputs:
- type: container
  paths:
    - '/var/lib/docker/containers/*/*.log'
  processors:
    - add_docker_metadata:
        host: "unix:///var/run/docker.sock"
    - decode_json_fields:
        fields: ["message"]
        target: ""
        overwrite_keys: true

- type: log
  enabled: true
  paths:
    - /var/log/nsrs/*.log
  fields:
    log_type: application
  fields_under_root: true
  multiline.pattern: '^\d{4}-\d{2}-\d{2}'
  multiline.negate: true
  multiline.match: after

processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
  - add_cloud_metadata: ~
  - add_kubernetes_metadata: ~

output.logstash:
  hosts: ["logstash:5044"]

logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644
```

## 日志规范

### 1. 日志格式规范

```java
/**
 * 日志格式配置
 */
@Configuration
public class LoggingConfiguration {
    
    @Bean
    public Logger structuredLogger() {
        LoggerContext context = (LoggerContext) LoggerFactory.getILoggerFactory();
        
        // JSON格式的Appender
        JsonEncoder jsonEncoder = new JsonEncoder();
        jsonEncoder.setContext(context);
        jsonEncoder.start();
        
        FileAppender<ILoggingEvent> fileAppender = new FileAppender<>();
        fileAppender.setContext(context);
        fileAppender.setName("JSON_FILE");
        fileAppender.setFile("/var/log/nsrs/application.json");
        fileAppender.setEncoder(jsonEncoder);
        fileAppender.start();
        
        Logger logger = context.getLogger("STRUCTURED");
        logger.addAppender(fileAppender);
        logger.setLevel(Level.INFO);
        logger.setAdditive(false);
        
        return logger;
    }
}
```

### 2. 日志切面

```java
/**
 * 日志记录切面
 */
@Aspect
@Component
@Slf4j
public class LoggingAspect {
    
    @Autowired
    private StructuredLogger structuredLogger;
    
    /**
     * 方法执行日志
     */
    @Around("@annotation(logOperation)")
    public Object logMethodExecution(ProceedingJoinPoint joinPoint, 
                                   LogOperation logOperation) throws Throwable {
        
        String className = joinPoint.getTarget().getClass().getSimpleName();
        String methodName = joinPoint.getSignature().getName();
        Object[] args = joinPoint.getArgs();
        
        long startTime = System.currentTimeMillis();
        
        // 记录方法开始日志
        structuredLogger.logBusiness(
            logOperation.module(),
            methodName,
            "方法开始执行: %s.%s",
            className, methodName
        );
        
        try {
            Object result = joinPoint.proceed();
            
            long duration = System.currentTimeMillis() - startTime;
            
            // 记录方法成功日志
            Map<String, Object> metrics = new HashMap<>();
            metrics.put("className", className);
            metrics.put("methodName", methodName);
            metrics.put("success", true);
            
            structuredLogger.logPerformance(
                methodName, duration, metrics
            );
            
            return result;
            
        } catch (Exception e) {
            // 记录方法异常日志
            structuredLogger.logError(
                logOperation.module(),
                methodName,
                String.format("方法执行异常: %s.%s", className, methodName),
                e
            );
            throw e;
        }
    }
    
    /**
     * 审计日志切面
     */
    @AfterReturning(value = "@annotation(auditLog)", returning = "result")
    public void logAuditOperation(JoinPoint joinPoint, AuditLog auditLog, Object result) {
        String userId = getCurrentUserId();
        String operation = auditLog.operation();
        String resource = auditLog.resource();
        String action = auditLog.action();
        
        Map<String, Object> details = new HashMap<>();
        details.put("method", joinPoint.getSignature().getName());
        details.put("args", joinPoint.getArgs());
        details.put("result", result);
        
        structuredLogger.logAudit(userId, operation, resource, action, details);
    }
    
    private String getCurrentUserId() {
        // 从安全上下文获取当前用户ID
        return SecurityContextHolder.getContext()
            .getAuthentication()
            .getName();
    }
}

/**
 * 日志操作注解
 */
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
public @interface LogOperation {
    String module() default "";
    String operation() default "";
}

/**
 * 审计日志注解
 */
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
public @interface AuditLog {
    String operation();
    String resource();
    String action();
}
```

## 业务场景应用

### 1. IMSI资源分配日志

```java
/**
 * IMSI资源分配服务
 */
@Service
@Slf4j
public class ImsiAllocationService {
    
    @Autowired
    private StructuredLogger structuredLogger;
    
    /**
     * 分配IMSI资源
     */
    @LogOperation(module = "IMSI_MANAGEMENT", operation = "ALLOCATE_IMSI")
    @AuditLog(operation = "IMSI分配", resource = "IMSI资源", action = "分配")
    public ImsiAllocationResult allocateImsi(ImsiAllocationRequest request) {
        String userId = getCurrentUserId();
        
        // 记录业务开始日志
        structuredLogger.logBusiness(
            "IMSI_MANAGEMENT",
            "ALLOCATE_IMSI",
            "开始分配IMSI资源: 供应商=%s, 数量=%d, 用户=%s",
            request.getSupplierId(),
            request.getQuantity(),
            userId
        );
        
        try {
            // 1. 验证请求参数
            validateAllocationRequest(request);
            
            // 2. 检查供应商资源
            checkSupplierResource(request.getSupplierId(), request.getQuantity());
            
            // 3. 执行分配逻辑
            List<String> allocatedImsis = performAllocation(request);
            
            // 4. 更新资源状态
            updateResourceStatus(allocatedImsis, request.getSupplierId());
            
            ImsiAllocationResult result = ImsiAllocationResult.builder()
                .allocatedImsis(allocatedImsis)
                .quantity(allocatedImsis.size())
                .supplierId(request.getSupplierId())
                .build();
            
            // 记录成功日志
            Map<String, Object> details = new HashMap<>();
            details.put("supplierId", request.getSupplierId());
            details.put("requestQuantity", request.getQuantity());
            details.put("allocatedQuantity", allocatedImsis.size());
            details.put("allocatedImsis", allocatedImsis);
            
            structuredLogger.logAudit(
                userId,
                "IMSI分配",
                "IMSI资源",
                "分配成功",
                details
            );
            
            structuredLogger.logBusiness(
                "IMSI_MANAGEMENT",
                "ALLOCATE_IMSI",
                "IMSI资源分配成功: 供应商=%s, 请求数量=%d, 实际分配=%d",
                request.getSupplierId(),
                request.getQuantity(),
                allocatedImsis.size()
            );
            
            return result;
            
        } catch (Exception e) {
            // 记录错误日志
            structuredLogger.logError(
                "IMSI_MANAGEMENT",
                "ALLOCATE_IMSI",
                String.format("IMSI资源分配失败: 供应商=%s, 数量=%d, 错误=%s",
                    request.getSupplierId(),
                    request.getQuantity(),
                    e.getMessage()),
                e
            );
            
            throw new ImsiAllocationException("IMSI资源分配失败: " + e.getMessage(), e);
        }
    }
    
    private void validateAllocationRequest(ImsiAllocationRequest request) {
        structuredLogger.logBusiness(
            "IMSI_MANAGEMENT",
            "VALIDATE_REQUEST",
            "验证分配请求: %s",
            request.toString()
        );
        
        // 验证逻辑...
    }
    
    private void checkSupplierResource(String supplierId, Integer quantity) {
        structuredLogger.logBusiness(
            "IMSI_MANAGEMENT",
            "CHECK_RESOURCE",
            "检查供应商资源: 供应商=%s, 需求数量=%d",
            supplierId,
            quantity
        );
        
        // 检查逻辑...
    }
    
    private List<String> performAllocation(ImsiAllocationRequest request) {
        structuredLogger.logBusiness(
            "IMSI_MANAGEMENT",
            "PERFORM_ALLOCATION",
            "执行IMSI分配: %s",
            request.toString()
        );
        
        // 分配逻辑...
        return new ArrayList<>();
    }
    
    private void updateResourceStatus(List<String> imsis, String supplierId) {
        structuredLogger.logBusiness(
            "IMSI_MANAGEMENT",
            "UPDATE_STATUS",
            "更新资源状态: 供应商=%s, IMSI数量=%d",
            supplierId,
            imsis.size()
        );
        
        // 更新逻辑...
    }
    
    private String getCurrentUserId() {
        return SecurityContextHolder.getContext()
            .getAuthentication()
            .getName();
    }
}
```

### 2. 日志查询服务

```java
/**
 * 日志查询服务
 */
@Service
@Slf4j
public class LogQueryService {
    
    @Autowired
    private ElasticsearchRestTemplate elasticsearchTemplate;
    
    /**
     * 查询业务日志
     */
    public Page<StructuredLog> queryBusinessLogs(LogQueryRequest request) {
        BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery();
        
        // 基础过滤条件
        queryBuilder.must(QueryBuilders.termQuery("type", LogType.BUSINESS.getCode()));
        
        // 时间范围
        if (request.getStartTime() != null && request.getEndTime() != null) {
            queryBuilder.must(QueryBuilders.rangeQuery("timestamp")
                .gte(request.getStartTime())
                .lte(request.getEndTime()));
        }
        
        // 服务名称
        if (StringUtils.hasText(request.getServiceName())) {
            queryBuilder.must(QueryBuilders.termQuery("serviceName", request.getServiceName()));
        }
        
        // 模块
        if (StringUtils.hasText(request.getModule())) {
            queryBuilder.must(QueryBuilders.termQuery("module", request.getModule()));
        }
        
        // 关键词搜索
        if (StringUtils.hasText(request.getKeyword())) {
            queryBuilder.must(QueryBuilders.multiMatchQuery(request.getKeyword())
                .field("message")
                .field("operation")
                .type(MultiMatchQueryBuilder.Type.BEST_FIELDS));
        }
        
        // 构建搜索请求
        NativeSearchQuery searchQuery = new NativeSearchQueryBuilder()
            .withQuery(queryBuilder)
            .withSort(SortBuilders.fieldSort("timestamp").order(SortOrder.DESC))
            .withPageable(PageRequest.of(request.getPage(), request.getSize()))
            .build();
        
        SearchHits<StructuredLog> searchHits = elasticsearchTemplate
            .search(searchQuery, StructuredLog.class);
        
        List<StructuredLog> logs = searchHits.getSearchHits().stream()
            .map(SearchHit::getContent)
            .collect(Collectors.toList());
        
        return new PageImpl<>(logs, searchQuery.getPageable(), searchHits.getTotalHits());
    }
    
    /**
     * 查询错误日志统计
     */
    public Map<String, Long> getErrorLogStats(LocalDateTime startTime, LocalDateTime endTime) {
        BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery()
            .must(QueryBuilders.termQuery("type", LogType.ERROR.getCode()))
            .must(QueryBuilders.rangeQuery("timestamp")
                .gte(startTime)
                .lte(endTime));
        
        TermsAggregationBuilder aggregation = AggregationBuilders
            .terms("error_by_service")
            .field("serviceName")
            .size(100);
        
        NativeSearchQuery searchQuery = new NativeSearchQueryBuilder()
            .withQuery(queryBuilder)
            .addAggregation(aggregation)
            .withMaxResults(0)
            .build();
        
        SearchHits<StructuredLog> searchHits = elasticsearchTemplate
            .search(searchQuery, StructuredLog.class);
        
        Terms terms = searchHits.getAggregations().get("error_by_service");
        
        return terms.getBuckets().stream()
            .collect(Collectors.toMap(
                Terms.Bucket::getKeyAsString,
                Terms.Bucket::getDocCount
            ));
    }
}
```

## 监控告警

### 1. 日志告警规则

```yaml
# elastalert/rules/nsrs-error-alert.yml
name: NSRS错误日志告警
type: frequency
index: nsrs-logs-*
num_events: 10
timeframe:
  minutes: 5

filter:
- term:
    level: "ERROR"
- range:
    "@timestamp":
      gte: "now-5m"

alert:
- "email"
- "slack"

email:
- "ops@nsrs.com"

slack:
slack_webhook_url: "https://hooks.slack.com/services/xxx"
slack_channel_override: "#alerts"

alert_text: |
  NSRS系统在过去5分钟内出现了{0}个错误日志
  
  服务: {1}
  时间: {2}
  错误信息: {3}

alert_text_args:
  - num_matches
  - serviceName
  - "@timestamp"
  - message
```

### 2. 日志监控指标

```java
/**
 * 日志监控指标收集
 */
@Component
@Slf4j
public class LogMetricsCollector {
    
    @Autowired
    private MeterRegistry meterRegistry;
    
    private final Counter errorLogCounter;
    private final Counter businessLogCounter;
    private final Timer logProcessingTimer;
    
    public LogMetricsCollector(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
        this.errorLogCounter = Counter.builder("log_errors_total")
            .description("错误日志总数")
            .register(meterRegistry);
        this.businessLogCounter = Counter.builder("log_business_total")
            .description("业务日志总数")
            .register(meterRegistry);
        this.logProcessingTimer = Timer.builder("log_processing_duration")
            .description("日志处理耗时")
            .register(meterRegistry);
    }
    
    /**
     * 记录错误日志指标
     */
    public void recordErrorLog(String serviceName, String module) {
        errorLogCounter.increment(
            Tags.of(
                "service", serviceName,
                "module", module
            )
        );
    }
    
    /**
     * 记录业务日志指标
     */
    public void recordBusinessLog(String serviceName, String module) {
        businessLogCounter.increment(
            Tags.of(
                "service", serviceName,
                "module", module
            )
        );
    }
    
    /**
     * 记录日志处理耗时
     */
    public void recordLogProcessingTime(long duration, String logType) {
        logProcessingTimer.record(duration, TimeUnit.MILLISECONDS,
            Tags.of("log_type", logType));
    }
}
```

## 最佳实践

### 1. 日志性能优化

- **异步日志**: 使用异步Appender避免阻塞业务线程
- **批量发送**: 批量发送日志到Logstash减少网络开销
- **本地缓存**: 使用本地缓存减少重复日志
- **压缩传输**: 启用日志压缩减少网络带宽

### 2. 存储优化

- **索引策略**: 按时间分片，定期删除过期索引
- **字段映射**: 优化字段映射，减少存储空间
- **冷热分离**: 热数据SSD存储，冷数据机械硬盘
- **数据压缩**: 启用索引压缩节省存储空间

### 3. 查询优化

- **索引模板**: 使用索引模板统一配置
- **查询缓存**: 启用查询缓存提升查询性能
- **分页限制**: 限制查询结果数量避免内存溢出
- **聚合优化**: 优化聚合查询提升统计性能

通过以上日志聚合架构设计，NSRS系统实现了完整的日志收集、处理、存储和分析能力，为系统运维和问题排查提供了强有力的支持。