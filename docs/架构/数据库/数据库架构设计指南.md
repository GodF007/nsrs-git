# 数据库架构设计指南

## 概述

数据库架构是NSRS号卡资源管理系统的核心基础设施。本文档详细阐述了基于MySQL的分库分表策略、读写分离、数据同步方案，以及数据库高可用和性能优化的完整解决方案。

### 数据库架构目标

- **高性能**: 通过分库分表和读写分离提升数据库性能
- **高可用**: 主从复制和故障自动切换保证系统可用性
- **可扩展**: 支持水平扩展，应对业务增长
- **数据一致性**: 保证分布式环境下的数据一致性
- **运维友好**: 简化数据库运维和监控管理

### 数据库架构图

```
┌─────────────────────────────────────────────────────────────────┐
│                        Application Layer                        │
└─────────────────────┬───────────────────────────────────────────┘
                      │
┌─────────────────────┴───────────────────────────────────────────┐
│                    Sharding-JDBC                               │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              Sharding Strategy                          │   │
│  │  - Database Sharding: user_id % 4                      │   │
│  │  - Table Sharding: imsi_id % 16                        │   │
│  │  - Read/Write Splitting                                 │   │
│  └─────────────────────────────────────────────────────────┘   │
└─────────────────────┬───────────────────────────────────────────┘
                      │
┌─────────────────────┴───────────────────────────────────────────┐
│                    Database Cluster                            │
│                                                                 │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │
│  │   DB_0      │  │   DB_1      │  │   DB_2      │            │
│  │             │  │             │  │             │            │
│  │┌───────────┐│  │┌───────────┐│  │┌───────────┐│            │
│  ││Master(W) ││  ││Master(W) ││  ││Master(W) ││            │
│  │└───────────┘│  │└───────────┘│  │└───────────┘│            │
│  │┌───────────┐│  │┌───────────┐│  │┌───────────┐│            │
│  ││Slave1(R) ││  ││Slave1(R) ││  ││Slave1(R) ││            │
│  │└───────────┘│  │└───────────┘│  │└───────────┘│            │
│  │┌───────────┐│  │┌───────────┐│  │┌───────────┐│            │
│  ││Slave2(R) ││  ││Slave2(R) ││  ││Slave2(R) ││            │
│  │└───────────┘│  │└───────────┘│  │└───────────┘│            │
│  └─────────────┘  └─────────────┘  └─────────────┘            │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                  MHA Manager                            │   │
│  │  - Master Failover                                      │   │
│  │  - VIP Management                                       │   │
│  │  - Health Check                                         │   │
│  └─────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
```

## 分库分表策略

### 1. 分库策略

基于业务特点，采用垂直分库和水平分库相结合的策略：

#### 垂直分库

```sql
-- 用户中心数据库
CREATE DATABASE nsrs_user_center;

-- 号卡资源数据库
CREATE DATABASE nsrs_sim_resource;

-- 订单管理数据库
CREATE DATABASE nsrs_order_mgnt;

-- 计费系统数据库
CREATE DATABASE nsrs_billing;

-- 系统管理数据库
CREATE DATABASE nsrs_system_mgnt;
```

#### 水平分库

```yaml
# sharding-jdbc配置
spring:
  shardingsphere:
    datasource:
      names: ds0,ds1,ds2,ds3
      
      # 数据源0
      ds0:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://10.1.7.10:3306/nsrs_sim_resource_0?useUnicode=true&characterEncoding=utf8&serverTimezone=Asia/Shanghai
        username: nsrs_user
        password: nsrs_password_2024
        hikari:
          maximum-pool-size: 20
          minimum-idle: 5
          connection-timeout: 30000
          idle-timeout: 600000
          max-lifetime: 1800000
          
      # 数据源1
      ds1:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://10.1.7.11:3306/nsrs_sim_resource_1?useUnicode=true&characterEncoding=utf8&serverTimezone=Asia/Shanghai
        username: nsrs_user
        password: nsrs_password_2024
        hikari:
          maximum-pool-size: 20
          minimum-idle: 5
          connection-timeout: 30000
          idle-timeout: 600000
          max-lifetime: 1800000
          
      # 数据源2
      ds2:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://10.1.7.12:3306/nsrs_sim_resource_2?useUnicode=true&characterEncoding=utf8&serverTimezone=Asia/Shanghai
        username: nsrs_user
        password: nsrs_password_2024
        hikari:
          maximum-pool-size: 20
          minimum-idle: 5
          connection-timeout: 30000
          idle-timeout: 600000
          max-lifetime: 1800000
          
      # 数据源3
      ds3:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://10.1.7.13:3306/nsrs_sim_resource_3?useUnicode=true&characterEncoding=utf8&serverTimezone=Asia/Shanghai
        username: nsrs_user
        password: nsrs_password_2024
        hikari:
          maximum-pool-size: 20
          minimum-idle: 5
          connection-timeout: 30000
          idle-timeout: 600000
          max-lifetime: 1800000
    
    # 分片规则
    rules:
      sharding:
        # 分库策略
        default-database-strategy:
          standard:
            sharding-column: user_id
            sharding-algorithm-name: database-inline
        
        # 分表策略
        tables:
          imsi_resource:
            actual-data-nodes: ds$->{0..3}.imsi_resource_$->{0..15}
            table-strategy:
              standard:
                sharding-column: imsi_id
                sharding-algorithm-name: table-inline
            key-generate-strategy:
              column: imsi_id
              key-generator-name: snowflake
              
          imsi_usage_log:
            actual-data-nodes: ds$->{0..3}.imsi_usage_log_$->{0..15}
            table-strategy:
              standard:
                sharding-column: imsi_id
                sharding-algorithm-name: table-inline
            key-generate-strategy:
              column: log_id
              key-generator-name: snowflake
              
          user_order:
            actual-data-nodes: ds$->{0..3}.user_order_$->{0..7}
            table-strategy:
              standard:
                sharding-column: user_id
                sharding-algorithm-name: order-table-inline
            key-generate-strategy:
              column: order_id
              key-generator-name: snowflake
        
        # 分片算法
        sharding-algorithms:
          database-inline:
            type: INLINE
            props:
              algorithm-expression: ds$->{user_id % 4}
              
          table-inline:
            type: INLINE
            props:
              algorithm-expression: imsi_resource_$->{imsi_id % 16}
              
          order-table-inline:
            type: INLINE
            props:
              algorithm-expression: user_order_$->{user_id % 8}
        
        # 主键生成策略
        key-generators:
          snowflake:
            type: SNOWFLAKE
            props:
              worker-id: 1
              max-vibration-offset: 1
    
    # 属性配置
    props:
      sql-show: true
      sql-simple: false
      executor-size: 20
      max-connections-size-per-query: 1
```

### 2. 分表策略

#### IMSI资源表分表

```sql
-- 分表脚本生成
DELIMITER //
CREATE PROCEDURE CreateImsiResourceTables()
BEGIN
    DECLARE i INT DEFAULT 0;
    DECLARE sql_text TEXT;
    
    WHILE i < 16 DO
        SET sql_text = CONCAT('
            CREATE TABLE IF NOT EXISTS imsi_resource_', i, ' (
                imsi_id BIGINT NOT NULL AUTO_INCREMENT COMMENT "IMSI资源ID",
                imsi VARCHAR(20) NOT NULL COMMENT "IMSI号码",
                iccid VARCHAR(25) NOT NULL COMMENT "ICCID号码",
                msisdn VARCHAR(15) COMMENT "MSISDN号码",
                group_id BIGINT COMMENT "IMSI组ID",
                supplier_id BIGINT NOT NULL COMMENT "供应商ID",
                status VARCHAR(20) NOT NULL DEFAULT "AVAILABLE" COMMENT "状态",
                network_type VARCHAR(10) COMMENT "网络类型",
                region_code VARCHAR(10) COMMENT "地区代码",
                activation_date DATETIME COMMENT "激活时间",
                expiry_date DATETIME COMMENT "过期时间",
                user_id BIGINT COMMENT "使用用户ID",
                allocated_time DATETIME COMMENT "分配时间",
                create_time DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT "创建时间",
                update_time DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT "更新时间",
                create_by VARCHAR(50) COMMENT "创建人",
                update_by VARCHAR(50) COMMENT "更新人",
                remark TEXT COMMENT "备注",
                PRIMARY KEY (imsi_id),
                UNIQUE KEY uk_imsi (imsi),
                UNIQUE KEY uk_iccid (iccid),
                KEY idx_group_id (group_id),
                KEY idx_supplier_id (supplier_id),
                KEY idx_status (status),
                KEY idx_user_id (user_id),
                KEY idx_create_time (create_time)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT="IMSI资源表_', i, '";
        ');
        
        SET @sql = sql_text;
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;
        
        SET i = i + 1;
    END WHILE;
END //
DELIMITER ;

-- 执行分表创建
CALL CreateImsiResourceTables();
```

#### 使用日志表分表

```sql
-- 按月分表的使用日志
DELIMITER //
CREATE PROCEDURE CreateUsageLogTables()
BEGIN
    DECLARE i INT DEFAULT 0;
    DECLARE sql_text TEXT;
    
    WHILE i < 16 DO
        SET sql_text = CONCAT('
            CREATE TABLE IF NOT EXISTS imsi_usage_log_', i, ' (
                log_id BIGINT NOT NULL AUTO_INCREMENT COMMENT "日志ID",
                imsi_id BIGINT NOT NULL COMMENT "IMSI资源ID",
                imsi VARCHAR(20) NOT NULL COMMENT "IMSI号码",
                user_id BIGINT COMMENT "用户ID",
                action_type VARCHAR(20) NOT NULL COMMENT "操作类型",
                data_usage BIGINT DEFAULT 0 COMMENT "数据使用量(KB)",
                voice_usage INT DEFAULT 0 COMMENT "语音使用量(秒)",
                sms_usage INT DEFAULT 0 COMMENT "短信使用量(条)",
                cost_amount DECIMAL(10,4) DEFAULT 0.0000 COMMENT "费用金额",
                location_info VARCHAR(100) COMMENT "位置信息",
                device_info VARCHAR(200) COMMENT "设备信息",
                session_start_time DATETIME COMMENT "会话开始时间",
                session_end_time DATETIME COMMENT "会话结束时间",
                create_time DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT "创建时间",
                remark TEXT COMMENT "备注",
                PRIMARY KEY (log_id),
                KEY idx_imsi_id (imsi_id),
                KEY idx_imsi (imsi),
                KEY idx_user_id (user_id),
                KEY idx_action_type (action_type),
                KEY idx_create_time (create_time),
                KEY idx_session_time (session_start_time, session_end_time)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT="IMSI使用日志表_', i, '";
        ');
        
        SET @sql = sql_text;
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;
        
        SET i = i + 1;
    END WHILE;
END //
DELIMITER ;

-- 执行分表创建
CALL CreateUsageLogTables();
```

### 3. 分片键选择策略

```java
/**
 * 分片键选择策略
 */
@Component
public class ShardingKeyStrategy {
    
    /**
     * IMSI资源分片键计算
     */
    public String calculateImsiResourceShardingKey(String imsi) {
        // 使用IMSI的后4位进行哈希
        String suffix = imsi.substring(imsi.length() - 4);
        int hash = suffix.hashCode();
        return String.valueOf(Math.abs(hash) % 16);
    }
    
    /**
     * 用户订单分片键计算
     */
    public String calculateUserOrderShardingKey(Long userId) {
        return String.valueOf(userId % 8);
    }
    
    /**
     * 数据库分片键计算
     */
    public String calculateDatabaseShardingKey(Long userId) {
        return String.valueOf(userId % 4);
    }
    
    /**
     * 时间范围分片键计算（按月）
     */
    public String calculateTimeRangeShardingKey(Date date) {
        Calendar cal = Calendar.getInstance();
        cal.setTime(date);
        int year = cal.get(Calendar.YEAR);
        int month = cal.get(Calendar.MONTH) + 1;
        return String.format("%d%02d", year, month);
    }
}
```

## 读写分离配置

### 1. 主从复制配置

#### 主库配置（Master）

```ini
# /etc/mysql/mysql.conf.d/mysqld.cnf
[mysqld]
# 服务器ID（每个服务器唯一）
server-id = 1

# 启用二进制日志
log-bin = mysql-bin
binlog-format = ROW
binlog-do-db = nsrs_sim_resource_0
binlog-do-db = nsrs_sim_resource_1
binlog-do-db = nsrs_sim_resource_2
binlog-do-db = nsrs_sim_resource_3

# 二进制日志过期时间（天）
expire_logs_days = 7

# 最大二进制日志大小
max_binlog_size = 100M

# 同步设置
sync_binlog = 1
innodb_flush_log_at_trx_commit = 1

# 字符集
character-set-server = utf8mb4
collation-server = utf8mb4_unicode_ci

# 性能优化
innodb_buffer_pool_size = 2G
innodb_log_file_size = 256M
innodb_log_buffer_size = 16M
innodb_flush_method = O_DIRECT
innodb_file_per_table = 1

# 连接设置
max_connections = 1000
max_connect_errors = 100000
wait_timeout = 28800
interactive_timeout = 28800

# 查询缓存
query_cache_type = 1
query_cache_size = 128M
query_cache_limit = 2M

# 慢查询日志
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 2
log_queries_not_using_indexes = 1
```

#### 从库配置（Slave）

```ini
# /etc/mysql/mysql.conf.d/mysqld.cnf
[mysqld]
# 服务器ID（每个从库唯一）
server-id = 2

# 启用中继日志
relay-log = mysql-relay-bin
relay-log-index = mysql-relay-bin.index

# 从库只读
read_only = 1
super_read_only = 1

# 复制设置
replicate-do-db = nsrs_sim_resource_0
replicate-do-db = nsrs_sim_resource_1
replicate-do-db = nsrs_sim_resource_2
replicate-do-db = nsrs_sim_resource_3

# 跳过复制错误（谨慎使用）
# slave-skip-errors = 1062,1053,1146

# 并行复制
slave_parallel_type = LOGICAL_CLOCK
slave_parallel_workers = 4
slave_preserve_commit_order = 1

# 字符集
character-set-server = utf8mb4
collation-server = utf8mb4_unicode_ci

# 性能优化
innodb_buffer_pool_size = 2G
innodb_log_file_size = 256M
innodb_log_buffer_size = 16M
innodb_flush_method = O_DIRECT
innodb_file_per_table = 1

# 连接设置
max_connections = 1000
max_connect_errors = 100000
wait_timeout = 28800
interactive_timeout = 28800

# 查询缓存
query_cache_type = 1
query_cache_size = 128M
query_cache_limit = 2M
```

#### 主从复制设置

```sql
-- 主库创建复制用户
CREATE USER 'replication'@'%' IDENTIFIED BY 'replication_password_2024';
GRANT REPLICATION SLAVE ON *.* TO 'replication'@'%';
FLUSH PRIVILEGES;

-- 查看主库状态
SHOW MASTER STATUS;

-- 从库配置复制
CHANGE MASTER TO
    MASTER_HOST='10.1.7.10',
    MASTER_PORT=3306,
    MASTER_USER='replication',
    MASTER_PASSWORD='replication_password_2024',
    MASTER_LOG_FILE='mysql-bin.000001',
    MASTER_LOG_POS=154,
    MASTER_CONNECT_RETRY=10;

-- 启动从库复制
START SLAVE;

-- 查看从库状态
SHOW SLAVE STATUS\G
```

### 2. 读写分离配置

```yaml
# application.yml
spring:
  shardingsphere:
    datasource:
      names: master-ds0,slave-ds0-0,slave-ds0-1,master-ds1,slave-ds1-0,slave-ds1-1
      
      # 主库数据源
      master-ds0:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://10.1.7.10:3306/nsrs_sim_resource_0?useUnicode=true&characterEncoding=utf8&serverTimezone=Asia/Shanghai
        username: nsrs_user
        password: nsrs_password_2024
        hikari:
          maximum-pool-size: 20
          minimum-idle: 5
          connection-timeout: 30000
          idle-timeout: 600000
          max-lifetime: 1800000
          
      # 从库数据源1
      slave-ds0-0:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://10.1.7.20:3306/nsrs_sim_resource_0?useUnicode=true&characterEncoding=utf8&serverTimezone=Asia/Shanghai
        username: nsrs_user
        password: nsrs_password_2024
        hikari:
          maximum-pool-size: 15
          minimum-idle: 3
          connection-timeout: 30000
          idle-timeout: 600000
          max-lifetime: 1800000
          
      # 从库数据源2
      slave-ds0-1:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://10.1.7.21:3306/nsrs_sim_resource_0?useUnicode=true&characterEncoding=utf8&serverTimezone=Asia/Shanghai
        username: nsrs_user
        password: nsrs_password_2024
        hikari:
          maximum-pool-size: 15
          minimum-idle: 3
          connection-timeout: 30000
          idle-timeout: 600000
          max-lifetime: 1800000
    
    rules:
      # 读写分离规则
      readwrite-splitting:
        data-sources:
          ds0:
            static-strategy:
              write-data-source-name: master-ds0
              read-data-source-names:
                - slave-ds0-0
                - slave-ds0-1
            load-balancer-name: round-robin
          ds1:
            static-strategy:
              write-data-source-name: master-ds1
              read-data-source-names:
                - slave-ds1-0
                - slave-ds1-1
            load-balancer-name: round-robin
        
        # 负载均衡算法
        load-balancers:
          round-robin:
            type: ROUND_ROBIN
          random:
            type: RANDOM
          weight:
            type: WEIGHT
            props:
              slave-ds0-0: 1
              slave-ds0-1: 2
      
      # 分片规则（与读写分离结合）
      sharding:
        tables:
          imsi_resource:
            actual-data-nodes: ds$->{0..1}.imsi_resource_$->{0..15}
            table-strategy:
              standard:
                sharding-column: imsi_id
                sharding-algorithm-name: table-inline
            key-generate-strategy:
              column: imsi_id
              key-generator-name: snowflake
        
        default-database-strategy:
          standard:
            sharding-column: user_id
            sharding-algorithm-name: database-inline
        
        sharding-algorithms:
          database-inline:
            type: INLINE
            props:
              algorithm-expression: ds$->{user_id % 2}
          table-inline:
            type: INLINE
            props:
              algorithm-expression: imsi_resource_$->{imsi_id % 16}
        
        key-generators:
          snowflake:
            type: SNOWFLAKE
            props:
              worker-id: 1
    
    props:
      sql-show: true
      sql-simple: false
```

### 3. 读写分离服务层

```java
/**
 * 读写分离服务
 */
@Service
public class ReadWriteSplittingService {
    
    @Autowired
    private ImsiResourceMapper imsiResourceMapper;
    
    /**
     * 强制读主库注解
     */
    @Target({ElementType.METHOD, ElementType.TYPE})
    @Retention(RetentionPolicy.RUNTIME)
    public @interface ReadFromMaster {
    }
    
    /**
     * 强制读从库注解
     */
    @Target({ElementType.METHOD, ElementType.TYPE})
    @Retention(RetentionPolicy.RUNTIME)
    public @interface ReadFromSlave {
    }
    
    /**
     * 查询IMSI资源（默认从从库读取）
     */
    @ReadFromSlave
    @Transactional(readOnly = true)
    public ImsiResource getImsiResource(String imsi) {
        return imsiResourceMapper.selectByImsi(imsi);
    }
    
    /**
     * 查询IMSI资源（强制从主库读取，保证数据一致性）
     */
    @ReadFromMaster
    @Transactional(readOnly = true)
    public ImsiResource getImsiResourceFromMaster(String imsi) {
        return imsiResourceMapper.selectByImsi(imsi);
    }
    
    /**
     * 更新IMSI资源（写主库）
     */
    @Transactional
    public void updateImsiResource(ImsiResource resource) {
        imsiResourceMapper.updateByImsi(resource);
    }
    
    /**
     * 批量查询（从从库读取）
     */
    @ReadFromSlave
    @Transactional(readOnly = true)
    public List<ImsiResource> getImsiResourceList(List<String> imsiList) {
        return imsiResourceMapper.selectByImsiList(imsiList);
    }
    
    /**
     * 统计查询（从从库读取）
     */
    @ReadFromSlave
    @Transactional(readOnly = true)
    public Map<String, Object> getImsiResourceStatistics(Long supplierId) {
        Map<String, Object> stats = new HashMap<>();
        stats.put("totalCount", imsiResourceMapper.countBySupplier(supplierId));
        stats.put("availableCount", imsiResourceMapper.countBySupplierAndStatus(supplierId, "AVAILABLE"));
        stats.put("usedCount", imsiResourceMapper.countBySupplierAndStatus(supplierId, "USED"));
        stats.put("expiredCount", imsiResourceMapper.countBySupplierAndStatus(supplierId, "EXPIRED"));
        return stats;
    }
}
```

### 4. 读写分离AOP实现

```java
/**
 * 读写分离AOP切面
 */
@Aspect
@Component
public class ReadWriteSplittingAspect {
    
    private static final String MASTER_KEY = "master";
    private static final String SLAVE_KEY = "slave";
    
    /**
     * 数据源上下文
     */
    public static class DataSourceContext {
        private static final ThreadLocal<String> CONTEXT = new ThreadLocal<>();
        
        public static void setDataSource(String dataSource) {
            CONTEXT.set(dataSource);
        }
        
        public static String getDataSource() {
            return CONTEXT.get();
        }
        
        public static void clear() {
            CONTEXT.remove();
        }
    }
    
    /**
     * 拦截ReadFromMaster注解
     */
    @Around("@annotation(readFromMaster)")
    public Object aroundReadFromMaster(ProceedingJoinPoint joinPoint, ReadFromMaster readFromMaster) throws Throwable {
        DataSourceContext.setDataSource(MASTER_KEY);
        try {
            return joinPoint.proceed();
        } finally {
            DataSourceContext.clear();
        }
    }
    
    /**
     * 拦截ReadFromSlave注解
     */
    @Around("@annotation(readFromSlave)")
    public Object aroundReadFromSlave(ProceedingJoinPoint joinPoint, ReadFromSlave readFromSlave) throws Throwable {
        DataSourceContext.setDataSource(SLAVE_KEY);
        try {
            return joinPoint.proceed();
        } finally {
            DataSourceContext.clear();
        }
    }
    
    /**
     * 拦截事务方法，根据只读属性选择数据源
     */
    @Around("@annotation(org.springframework.transaction.annotation.Transactional)")
    public Object aroundTransactional(ProceedingJoinPoint joinPoint) throws Throwable {
        MethodSignature signature = (MethodSignature) joinPoint.getSignature();
        Method method = signature.getMethod();
        
        Transactional transactional = method.getAnnotation(Transactional.class);
        if (transactional != null && transactional.readOnly()) {
            DataSourceContext.setDataSource(SLAVE_KEY);
        } else {
            DataSourceContext.setDataSource(MASTER_KEY);
        }
        
        try {
            return joinPoint.proceed();
        } finally {
            DataSourceContext.clear();
        }
    }
}
```

## 数据同步方案

### 1. 实时数据同步

#### Canal配置

```yaml
# canal.properties
canal.id = 1
canal.ip =
canal.port = 11111
canal.metrics.pull.port = 11112
canal.zkServers =

# canal instance配置
canal.destinations = nsrs_sync
canal.conf.dir = ../conf
canal.auto.scan = true
canal.auto.scan.interval = 5

canal.instance.master.address = 10.1.7.10:3306
canal.instance.master.journal.name = mysql-bin.000001
canal.instance.master.position = 154
canal.instance.master.timestamp =
canal.instance.master.gtid =

# username/password
canal.instance.dbUsername = canal
canal.instance.dbPassword = canal_password_2024
canal.instance.connectionCharset = UTF-8
canal.instance.enableDruid = false

# table regex
canal.instance.filter.regex = nsrs_sim_resource_.*\.imsi_resource_.*,nsrs_sim_resource_.*\.imsi_usage_log_.*
canal.instance.filter.black.regex =

# mq config
canal.mq.topic = nsrs_binlog_topic
canal.mq.partition = 0
canal.mq.partitionsNum = 1
canal.mq.partitionHash = .*\..*
canal.mq.dynamicTopic = .*\..*
canal.mq.sendTimeout = 30000
canal.mq.build.timeout = 30000

# kafka
kafka.bootstrap.servers = 10.1.7.30:9092,10.1.7.31:9092,10.1.7.32:9092
kafka.acks = all
kafka.compression.type = none
kafka.batch.size = 16384
kafka.linger.ms = 1
kafka.max.request.size = 1048576
kafka.buffer.memory = 33554432
kafka.retries = 2147483647
kafka.max.in.flight.requests.per.connection = 5
```

#### 数据同步服务

```java
/**
 * 数据同步服务
 */
@Service
public class DataSyncService {
    
    @Autowired
    private RedisTemplate<String, Object> redisTemplate;
    
    @Autowired
    private ElasticsearchTemplate elasticsearchTemplate;
    
    @Autowired
    private KafkaTemplate<String, Object> kafkaTemplate;
    
    /**
     * 处理Canal数据变更事件
     */
    @KafkaListener(topics = "nsrs_binlog_topic")
    public void handleDataChangeEvent(String message) {
        try {
            JSONObject event = JSON.parseObject(message);
            String database = event.getString("database");
            String table = event.getString("table");
            String type = event.getString("type");
            JSONArray data = event.getJSONArray("data");
            JSONArray old = event.getJSONArray("old");
            
            log.info("Received data change event: database={}, table={}, type={}", database, table, type);
            
            // 处理IMSI资源表变更
            if (table.startsWith("imsi_resource_")) {
                handleImsiResourceChange(type, data, old);
            }
            // 处理使用日志表变更
            else if (table.startsWith("imsi_usage_log_")) {
                handleUsageLogChange(type, data, old);
            }
            
        } catch (Exception e) {
            log.error("Failed to handle data change event: {}", message, e);
        }
    }
    
    /**
     * 处理IMSI资源变更
     */
    private void handleImsiResourceChange(String type, JSONArray data, JSONArray old) {
        for (int i = 0; i < data.size(); i++) {
            JSONObject record = data.getJSONObject(i);
            String imsi = record.getString("imsi");
            
            switch (type) {
                case "INSERT":
                case "UPDATE":
                    // 同步到Redis缓存
                    syncToRedisCache(imsi, record);
                    // 同步到Elasticsearch
                    syncToElasticsearch(imsi, record);
                    // 发送业务事件
                    publishBusinessEvent("imsi.resource.changed", record);
                    break;
                    
                case "DELETE":
                    // 删除Redis缓存
                    deleteFromRedisCache(imsi);
                    // 删除Elasticsearch索引
                    deleteFromElasticsearch(imsi);
                    // 发送业务事件
                    publishBusinessEvent("imsi.resource.deleted", record);
                    break;
            }
        }
    }
    
    /**
     * 处理使用日志变更
     */
    private void handleUsageLogChange(String type, JSONArray data, JSONArray old) {
        for (int i = 0; i < data.size(); i++) {
            JSONObject record = data.getJSONObject(i);
            
            if ("INSERT".equals(type)) {
                // 同步到Elasticsearch用于日志分析
                syncUsageLogToElasticsearch(record);
                // 更新实时统计
                updateRealtimeStatistics(record);
                // 发送使用事件
                publishBusinessEvent("imsi.usage.logged", record);
            }
        }
    }
    
    /**
     * 同步到Redis缓存
     */
    private void syncToRedisCache(String imsi, JSONObject record) {
        try {
            String cacheKey = "nsrs:imsi:" + imsi;
            ImsiResource resource = JSON.toJavaObject(record, ImsiResource.class);
            redisTemplate.opsForValue().set(cacheKey, resource, Duration.ofHours(1));
            log.debug("Synced IMSI resource to Redis cache: {}", imsi);
        } catch (Exception e) {
            log.error("Failed to sync IMSI resource to Redis cache: {}", imsi, e);
        }
    }
    
    /**
     * 同步到Elasticsearch
     */
    private void syncToElasticsearch(String imsi, JSONObject record) {
        try {
            String indexName = "nsrs-imsi-resource-" + LocalDate.now().format(DateTimeFormatter.ofPattern("yyyy-MM"));
            elasticsearchTemplate.index(IndexQuery.builder()
                .indexName(indexName)
                .id(imsi)
                .object(record)
                .build());
            log.debug("Synced IMSI resource to Elasticsearch: {}", imsi);
        } catch (Exception e) {
            log.error("Failed to sync IMSI resource to Elasticsearch: {}", imsi, e);
        }
    }
    
    /**
     * 同步使用日志到Elasticsearch
     */
    private void syncUsageLogToElasticsearch(JSONObject record) {
        try {
            String indexName = "nsrs-usage-log-" + LocalDate.now().format(DateTimeFormatter.ofPattern("yyyy-MM"));
            String logId = record.getString("log_id");
            elasticsearchTemplate.index(IndexQuery.builder()
                .indexName(indexName)
                .id(logId)
                .object(record)
                .build());
            log.debug("Synced usage log to Elasticsearch: {}", logId);
        } catch (Exception e) {
            log.error("Failed to sync usage log to Elasticsearch: {}", record.getString("log_id"), e);
        }
    }
    
    /**
     * 更新实时统计
     */
    private void updateRealtimeStatistics(JSONObject record) {
        try {
            String imsi = record.getString("imsi");
            Long dataUsage = record.getLong("data_usage");
            Integer voiceUsage = record.getInteger("voice_usage");
            Integer smsUsage = record.getInteger("sms_usage");
            
            // 更新Redis中的实时统计
            String statsKey = "nsrs:stats:" + imsi;
            redisTemplate.opsForHash().increment(statsKey, "total_data_usage", dataUsage != null ? dataUsage : 0);
            redisTemplate.opsForHash().increment(statsKey, "total_voice_usage", voiceUsage != null ? voiceUsage : 0);
            redisTemplate.opsForHash().increment(statsKey, "total_sms_usage", smsUsage != null ? smsUsage : 0);
            redisTemplate.expire(statsKey, Duration.ofDays(30));
            
            log.debug("Updated realtime statistics for IMSI: {}", imsi);
        } catch (Exception e) {
            log.error("Failed to update realtime statistics", e);
        }
    }
    
    /**
     * 发布业务事件
     */
    private void publishBusinessEvent(String eventType, JSONObject data) {
        try {
            Map<String, Object> event = new HashMap<>();
            event.put("eventType", eventType);
            event.put("timestamp", System.currentTimeMillis());
            event.put("data", data);
            
            kafkaTemplate.send("nsrs_business_events", JSON.toJSONString(event));
            log.debug("Published business event: {}", eventType);
        } catch (Exception e) {
            log.error("Failed to publish business event: {}", eventType, e);
        }
    }
    
    /**
     * 删除Redis缓存
     */
    private void deleteFromRedisCache(String imsi) {
        try {
            String cacheKey = "nsrs:imsi:" + imsi;
            redisTemplate.delete(cacheKey);
            log.debug("Deleted IMSI resource from Redis cache: {}", imsi);
        } catch (Exception e) {
            log.error("Failed to delete IMSI resource from Redis cache: {}", imsi, e);
        }
    }
    
    /**
     * 删除Elasticsearch索引
     */
    private void deleteFromElasticsearch(String imsi) {
        try {
            String indexName = "nsrs-imsi-resource-" + LocalDate.now().format(DateTimeFormatter.ofPattern("yyyy-MM"));
            elasticsearchTemplate.delete(indexName, imsi);
            log.debug("Deleted IMSI resource from Elasticsearch: {}", imsi);
        } catch (Exception e) {
            log.error("Failed to delete IMSI resource from Elasticsearch: {}", imsi, e);
        }
    }
}
```

### 2. 批量数据同步

```java
/**
 * 批量数据同步服务
 */
@Service
public class BatchDataSyncService {
    
    @Autowired
    private ImsiResourceMapper imsiResourceMapper;
    
    @Autowired
    private RedisTemplate<String, Object> redisTemplate;
    
    @Autowired
    private ElasticsearchTemplate elasticsearchTemplate;
    
    /**
     * 全量同步IMSI资源到缓存
     */
    @Scheduled(cron = "0 0 2 * * ?")  // 每天凌晨2点执行
    public void fullSyncImsiResourceToCache() {
        log.info("Starting full sync IMSI resource to cache");
        
        try {
            int pageSize = 1000;
            int offset = 0;
            int totalSynced = 0;
            
            while (true) {
                List<ImsiResource> resources = imsiResourceMapper.selectForSync(offset, pageSize);
                if (resources.isEmpty()) {
                    break;
                }
                
                // 批量同步到Redis
                Map<String, Object> cacheData = new HashMap<>();
                for (ImsiResource resource : resources) {
                    String cacheKey = "nsrs:imsi:" + resource.getImsi();
                    cacheData.put(cacheKey, resource);
                }
                
                if (!cacheData.isEmpty()) {
                    redisTemplate.opsForValue().multiSet(cacheData);
                    
                    // 设置过期时间
                    for (String key : cacheData.keySet()) {
                        redisTemplate.expire(key, Duration.ofHours(1));
                    }
                }
                
                totalSynced += resources.size();
                offset += pageSize;
                
                log.debug("Synced {} IMSI resources to cache, total: {}", resources.size(), totalSynced);
                
                // 避免对数据库造成过大压力
                Thread.sleep(100);
            }
            
            log.info("Full sync IMSI resource to cache completed, total synced: {}", totalSynced);
            
        } catch (Exception e) {
            log.error("Full sync IMSI resource to cache failed", e);
        }
    }
    
    /**
     * 增量同步IMSI资源到Elasticsearch
     */
    @Scheduled(fixedRate = 300000)  // 每5分钟执行一次
    public void incrementalSyncToElasticsearch() {
        log.info("Starting incremental sync to Elasticsearch");
        
        try {
            // 获取上次同步时间
            String lastSyncTimeKey = "nsrs:sync:last_sync_time";
            String lastSyncTimeStr = (String) redisTemplate.opsForValue().get(lastSyncTimeKey);
            
            LocalDateTime lastSyncTime;
            if (lastSyncTimeStr != null) {
                lastSyncTime = LocalDateTime.parse(lastSyncTimeStr);
            } else {
                lastSyncTime = LocalDateTime.now().minusHours(1);  // 默认同步最近1小时的数据
            }
            
            LocalDateTime currentTime = LocalDateTime.now();
            
            // 查询增量数据
            List<ImsiResource> incrementalData = imsiResourceMapper.selectByUpdateTimeRange(
                Timestamp.valueOf(lastSyncTime), 
                Timestamp.valueOf(currentTime)
            );
            
            if (!incrementalData.isEmpty()) {
                // 批量索引到Elasticsearch
                String indexName = "nsrs-imsi-resource-" + LocalDate.now().format(DateTimeFormatter.ofPattern("yyyy-MM"));
                
                List<IndexQuery> indexQueries = incrementalData.stream()
                    .map(resource -> IndexQuery.builder()
                        .indexName(indexName)
                        .id(resource.getImsi())
                        .object(resource)
                        .build())
                    .collect(Collectors.toList());
                
                elasticsearchTemplate.bulkIndex(indexQueries);
                
                log.info("Incremental sync to Elasticsearch completed, synced {} records", incrementalData.size());
            }
            
            // 更新同步时间
            redisTemplate.opsForValue().set(lastSyncTimeKey, currentTime.toString());
            
        } catch (Exception e) {
            log.error("Incremental sync to Elasticsearch failed", e);
        }
    }
    
    /**
     * 数据一致性检查
     */
    @Scheduled(cron = "0 30 3 * * ?")  // 每天凌晨3:30执行
    public void dataConsistencyCheck() {
        log.info("Starting data consistency check");
        
        try {
            int pageSize = 1000;
            int offset = 0;
            int inconsistentCount = 0;
            
            while (true) {
                List<ImsiResource> dbResources = imsiResourceMapper.selectForSync(offset, pageSize);
                if (dbResources.isEmpty()) {
                    break;
                }
                
                for (ImsiResource dbResource : dbResources) {
                    String cacheKey = "nsrs:imsi:" + dbResource.getImsi();
                    ImsiResource cacheResource = (ImsiResource) redisTemplate.opsForValue().get(cacheKey);
                    
                    if (cacheResource == null) {
                        // 缓存中不存在，重新加载
                        redisTemplate.opsForValue().set(cacheKey, dbResource, Duration.ofHours(1));
                        inconsistentCount++;
                        log.debug("Cache miss detected for IMSI: {}, reloaded", dbResource.getImsi());
                    } else if (!isDataConsistent(dbResource, cacheResource)) {
                        // 数据不一致，更新缓存
                        redisTemplate.opsForValue().set(cacheKey, dbResource, Duration.ofHours(1));
                        inconsistentCount++;
                        log.debug("Data inconsistency detected for IMSI: {}, cache updated", dbResource.getImsi());
                    }
                }
                
                offset += pageSize;
                Thread.sleep(50);  // 避免对系统造成过大压力
            }
            
            log.info("Data consistency check completed, fixed {} inconsistencies", inconsistentCount);
            
        } catch (Exception e) {
            log.error("Data consistency check failed", e);
        }
    }
    
    /**
     * 检查数据是否一致
     */
    private boolean isDataConsistent(ImsiResource dbResource, ImsiResource cacheResource) {
        return Objects.equals(dbResource.getStatus(), cacheResource.getStatus()) &&
               Objects.equals(dbResource.getUserId(), cacheResource.getUserId()) &&
               Objects.equals(dbResource.getUpdateTime(), cacheResource.getUpdateTime());
    }
}
```

## 数据库监控和运维

### 1. 数据库监控配置

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "mysql_rules.yml"

scrape_configs:
  - job_name: 'mysql'
    static_configs:
      - targets: ['10.1.7.10:9104', '10.1.7.11:9104', '10.1.7.12:9104']
    scrape_interval: 5s
    metrics_path: /metrics
    
  - job_name: 'mysql-exporter'
    static_configs:
      - targets: ['10.1.7.10:9104']
    params:
      collect[]:
        - info_schema.innodb_metrics
        - info_schema.processlist
        - info_schema.query_response_time
        - slave_status
        - binlog_size
        - perf_schema.tableiowaits
        - perf_schema.indexiowaits
```

```yaml
# mysql_rules.yml
groups:
  - name: mysql.rules
    rules:
      - alert: MySQLDown
        expr: mysql_up == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "MySQL instance is down"
          description: "MySQL instance {{ $labels.instance }} is down"
          
      - alert: MySQLSlowQueries
        expr: rate(mysql_global_status_slow_queries[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "MySQL slow queries detected"
          description: "MySQL instance {{ $labels.instance }} has {{ $value }} slow queries per second"
          
      - alert: MySQLReplicationLag
        expr: mysql_slave_lag_seconds > 30
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "MySQL replication lag is high"
          description: "MySQL slave {{ $labels.instance }} is lagging {{ $value }} seconds behind master"
          
      - alert: MySQLConnectionsHigh
        expr: mysql_global_status_threads_connected / mysql_global_variables_max_connections > 0.8
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "MySQL connections usage is high"
          description: "MySQL instance {{ $labels.instance }} connection usage is {{ $value | humanizePercentage }}"
          
      - alert: MySQLInnoDBBufferPoolHitRatio
        expr: (mysql_global_status_innodb_buffer_pool_read_requests - mysql_global_status_innodb_buffer_pool_reads) / mysql_global_status_innodb_buffer_pool_read_requests < 0.95
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "MySQL InnoDB buffer pool hit ratio is low"
          description: "MySQL instance {{ $labels.instance }} InnoDB buffer pool hit ratio is {{ $value | humanizePercentage }}"
```

### 2. 数据库运维工具

```java
/**
 * 数据库运维服务
 */
@Service
public class DatabaseOpsService {
    
    @Autowired
    private JdbcTemplate jdbcTemplate;
    
    @Autowired
    private DataSource dataSource;
    
    /**
     * 获取数据库状态
     */
    public Map<String, Object> getDatabaseStatus() {
        Map<String, Object> status = new HashMap<>();
        
        try {
            // 基本状态信息
            List<Map<String, Object>> variables = jdbcTemplate.queryForList("SHOW GLOBAL STATUS");
            Map<String, Object> statusMap = variables.stream()
                .collect(Collectors.toMap(
                    row -> (String) row.get("Variable_name"),
                    row -> row.get("Value")
                ));
            
            status.put("connections", statusMap.get("Threads_connected"));
            status.put("queries", statusMap.get("Queries"));
            status.put("slow_queries", statusMap.get("Slow_queries"));
            status.put("uptime", statusMap.get("Uptime"));
            
            // InnoDB状态
            status.put("innodb_buffer_pool_pages_total", statusMap.get("Innodb_buffer_pool_pages_total"));
            status.put("innodb_buffer_pool_pages_free", statusMap.get("Innodb_buffer_pool_pages_free"));
            status.put("innodb_buffer_pool_read_requests", statusMap.get("Innodb_buffer_pool_read_requests"));
            status.put("innodb_buffer_pool_reads", statusMap.get("Innodb_buffer_pool_reads"));
            
            // 主从复制状态
            try {
                Map<String, Object> slaveStatus = jdbcTemplate.queryForMap("SHOW SLAVE STATUS");
                status.put("slave_io_running", slaveStatus.get("Slave_IO_Running"));
                status.put("slave_sql_running", slaveStatus.get("Slave_SQL_Running"));
                status.put("seconds_behind_master", slaveStatus.get("Seconds_Behind_Master"));
            } catch (Exception e) {
                // 主库没有从库状态
                status.put("is_master", true);
            }
            
        } catch (Exception e) {
            log.error("Failed to get database status", e);
            status.put("error", e.getMessage());
        }
        
        return status;
    }
    
    /**
     * 获取慢查询日志
     */
    public List<Map<String, Object>> getSlowQueries(int limit) {
        try {
            String sql = "SELECT * FROM mysql.slow_log ORDER BY start_time DESC LIMIT ?";
            return jdbcTemplate.queryForList(sql, limit);
        } catch (Exception e) {
            log.error("Failed to get slow queries", e);
            return Collections.emptyList();
        }
    }
    
    /**
     * 获取当前运行的查询
     */
    public List<Map<String, Object>> getCurrentQueries() {
        try {
            String sql = "SELECT ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO " +
                        "FROM INFORMATION_SCHEMA.PROCESSLIST " +
                        "WHERE COMMAND != 'Sleep' AND TIME > 1 " +
                        "ORDER BY TIME DESC";
            return jdbcTemplate.queryForList(sql);
        } catch (Exception e) {
            log.error("Failed to get current queries", e);
            return Collections.emptyList();
        }
    }
    
    /**
     * 获取表空间使用情况
     */
    public List<Map<String, Object>> getTableSpaceUsage() {
        try {
            String sql = "SELECT " +
                        "    table_schema AS 'Database', " +
                        "    table_name AS 'Table', " +
                        "    ROUND(((data_length + index_length) / 1024 / 1024), 2) AS 'Size_MB', " +
                        "    table_rows AS 'Rows', " +
                        "    ROUND(((data_length) / 1024 / 1024), 2) AS 'Data_MB', " +
                        "    ROUND(((index_length) / 1024 / 1024), 2) AS 'Index_MB' " +
                        "FROM information_schema.TABLES " +
                        "WHERE table_schema NOT IN ('information_schema', 'mysql', 'performance_schema', 'sys') " +
                        "ORDER BY (data_length + index_length) DESC";
            return jdbcTemplate.queryForList(sql);
        } catch (Exception e) {
            log.error("Failed to get table space usage", e);
            return Collections.emptyList();
        }
    }
    
    /**
     * 获取索引使用情况
     */
    public List<Map<String, Object>> getIndexUsage(String database) {
        try {
            String sql = "SELECT " +
                        "    t.TABLE_SCHEMA AS 'Database', " +
                        "    t.TABLE_NAME AS 'Table', " +
                        "    s.INDEX_NAME AS 'Index', " +
                        "    s.COLUMN_NAME AS 'Column', " +
                        "    s.CARDINALITY AS 'Cardinality', " +
                        "    ROUND(((SELECT SUM(stat_value) FROM mysql.innodb_index_stats " +
                        "            WHERE database_name = t.TABLE_SCHEMA " +
                        "            AND table_name = t.TABLE_NAME " +
                        "            AND index_name = s.INDEX_NAME " +
                        "            AND stat_name = 'size') * @@innodb_page_size / 1024 / 1024), 2) AS 'Size_MB' " +
                        "FROM information_schema.TABLES t " +
                        "JOIN information_schema.STATISTICS s ON t.TABLE_SCHEMA = s.TABLE_SCHEMA " +
                        "    AND t.TABLE_NAME = s.TABLE_NAME " +
                        "WHERE t.TABLE_SCHEMA = ? " +
                        "ORDER BY Size_MB DESC";
            return jdbcTemplate.queryForList(sql, database);
        } catch (Exception e) {
            log.error("Failed to get index usage for database: {}", database, e);
            return Collections.emptyList();
        }
    }
    
    /**
     * 执行数据库优化
     */
    public Map<String, Object> optimizeDatabase(String database) {
        Map<String, Object> result = new HashMap<>();
        List<String> optimizedTables = new ArrayList<>();
        
        try {
            // 获取需要优化的表
            String sql = "SELECT TABLE_NAME FROM information_schema.TABLES " +
                        "WHERE TABLE_SCHEMA = ? AND ENGINE = 'InnoDB'";
            List<String> tables = jdbcTemplate.queryForList(sql, String.class, database);
            
            for (String table : tables) {
                try {
                    // 优化表
                    jdbcTemplate.execute("OPTIMIZE TABLE " + database + "." + table);
                    optimizedTables.add(table);
                    log.debug("Optimized table: {}.{}", database, table);
                } catch (Exception e) {
                    log.warn("Failed to optimize table: {}.{}", database, table, e);
                }
            }
            
            result.put("success", true);
            result.put("optimizedTables", optimizedTables);
            result.put("totalTables", tables.size());
            
        } catch (Exception e) {
            log.error("Failed to optimize database: {}", database, e);
            result.put("success", false);
            result.put("error", e.getMessage());
        }
        
        return result;
    }
    
    /**
     * 检查数据库连接池状态
     */
    public Map<String, Object> getConnectionPoolStatus() {
        Map<String, Object> status = new HashMap<>();
        
        try {
            if (dataSource instanceof HikariDataSource) {
                HikariDataSource hikariDS = (HikariDataSource) dataSource;
                HikariPoolMXBean poolBean = hikariDS.getHikariPoolMXBean();
                
                status.put("activeConnections", poolBean.getActiveConnections());
                status.put("idleConnections", poolBean.getIdleConnections());
                status.put("totalConnections", poolBean.getTotalConnections());
                status.put("threadsAwaitingConnection", poolBean.getThreadsAwaitingConnection());
                status.put("maximumPoolSize", hikariDS.getMaximumPoolSize());
                status.put("minimumIdle", hikariDS.getMinimumIdle());
            }
        } catch (Exception e) {
            log.error("Failed to get connection pool status", e);
            status.put("error", e.getMessage());
        }
        
        return status;
    }
}
```

### 3. 数据库运维REST接口

```java
/**
 * 数据库运维控制器
 */
@RestController
@RequestMapping("/admin/database")
public class DatabaseOpsController {
    
    @Autowired
    private DatabaseOpsService databaseOpsService;
    
    /**
     * 获取数据库状态
     */
    @GetMapping("/status")
    public ResponseEntity<Map<String, Object>> getDatabaseStatus() {
        Map<String, Object> status = databaseOpsService.getDatabaseStatus();
        return ResponseEntity.ok(status);
    }
    
    /**
     * 获取慢查询
     */
    @GetMapping("/slow-queries")
    public ResponseEntity<List<Map<String, Object>>> getSlowQueries(
            @RequestParam(defaultValue = "100") int limit) {
        List<Map<String, Object>> slowQueries = databaseOpsService.getSlowQueries(limit);
        return ResponseEntity.ok(slowQueries);
    }
    
    /**
     * 获取当前查询
     */
    @GetMapping("/current-queries")
    public ResponseEntity<List<Map<String, Object>>> getCurrentQueries() {
        List<Map<String, Object>> currentQueries = databaseOpsService.getCurrentQueries();
        return ResponseEntity.ok(currentQueries);
    }
    
    /**
     * 获取表空间使用情况
     */
    @GetMapping("/table-space")
    public ResponseEntity<List<Map<String, Object>>> getTableSpaceUsage() {
        List<Map<String, Object>> tableSpace = databaseOpsService.getTableSpaceUsage();
        return ResponseEntity.ok(tableSpace);
    }
    
    /**
     * 获取索引使用情况
     */
    @GetMapping("/index-usage")
    public ResponseEntity<List<Map<String, Object>>> getIndexUsage(
            @RequestParam String database) {
        List<Map<String, Object>> indexUsage = databaseOpsService.getIndexUsage(database);
        return ResponseEntity.ok(indexUsage);
    }
    
    /**
     * 优化数据库
     */
    @PostMapping("/optimize")
    public ResponseEntity<Map<String, Object>> optimizeDatabase(
            @RequestParam String database) {
        Map<String, Object> result = databaseOpsService.optimizeDatabase(database);
        return ResponseEntity.ok(result);
    }
    
    /**
     * 获取连接池状态
     */
    @GetMapping("/connection-pool")
    public ResponseEntity<Map<String, Object>> getConnectionPoolStatus() {
        Map<String, Object> status = databaseOpsService.getConnectionPoolStatus();
        return ResponseEntity.ok(status);
    }
}
```

## 数据库性能优化

### 1. MySQL配置优化

```ini
# /etc/mysql/mysql.conf.d/mysqld.cnf - 生产环境优化配置
[mysqld]
# 基本设置
port = 3306
socket = /var/run/mysqld/mysqld.sock
pid-file = /var/run/mysqld/mysqld.pid
basedir = /usr
datadir = /var/lib/mysql
tmpdir = /tmp
lc-messages-dir = /usr/share/mysql

# 字符集设置
character-set-server = utf8mb4
collation-server = utf8mb4_unicode_ci
init_connect = 'SET NAMES utf8mb4'

# 网络设置
bind-address = 0.0.0.0
max_connections = 2000
max_connect_errors = 100000
max_allowed_packet = 64M
wait_timeout = 28800
interactive_timeout = 28800
net_read_timeout = 30
net_write_timeout = 60

# InnoDB设置
default-storage-engine = InnoDB
innodb_buffer_pool_size = 8G  # 设置为物理内存的70-80%
innodb_buffer_pool_instances = 8
innodb_log_file_size = 512M
innodb_log_buffer_size = 64M
innodb_flush_log_at_trx_commit = 1
innodb_flush_method = O_DIRECT
innodb_file_per_table = 1
innodb_open_files = 65535
innodb_io_capacity = 2000
innodb_io_capacity_max = 4000
innodb_read_io_threads = 8
innodb_write_io_threads = 8
innodb_thread_concurrency = 0
innodb_lock_wait_timeout = 120
innodb_rollback_on_timeout = 1

# 查询缓存
query_cache_type = 1
query_cache_size = 256M
query_cache_limit = 4M
query_cache_min_res_unit = 2K

# 临时表设置
tmp_table_size = 256M
max_heap_table_size = 256M

# 排序和分组
sort_buffer_size = 4M
read_buffer_size = 2M
read_rnd_buffer_size = 8M
join_buffer_size = 4M

# 线程设置
thread_cache_size = 64
thread_stack = 256K

# 表设置
table_open_cache = 4096
table_definition_cache = 2048
open_files_limit = 65535

# 二进制日志
log-bin = mysql-bin
binlog_format = ROW
binlog_cache_size = 2M
max_binlog_cache_size = 8M
max_binlog_size = 512M
expire_logs_days = 7
sync_binlog = 1

# 慢查询日志
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 1
log_queries_not_using_indexes = 1
log_slow_admin_statements = 1
log_slow_slave_statements = 1
min_examined_row_limit = 1000

# 错误日志
log-error = /var/log/mysql/error.log

# 通用日志（生产环境建议关闭）
# general_log = 1
# general_log_file = /var/log/mysql/general.log

# 性能模式
performance_schema = ON
performance_schema_max_table_instances = 12500
performance_schema_max_table_handles = 4000

# 其他优化
skip-name-resolve = 1
back_log = 1024
max_user_connections = 800
max_connections_per_hour = 0
max_queries_per_hour = 0
max_updates_per_hour = 0
```

### 2. 索引优化策略

```sql
-- IMSI资源表索引优化
ALTER TABLE imsi_resource_0 ADD INDEX idx_composite_status_supplier (status, supplier_id, create_time);
ALTER TABLE imsi_resource_0 ADD INDEX idx_composite_user_status (user_id, status, allocated_time);
ALTER TABLE imsi_resource_0 ADD INDEX idx_composite_group_status (group_id, status, activation_date);
ALTER TABLE imsi_resource_0 ADD INDEX idx_expiry_status (expiry_date, status);
ALTER TABLE imsi_resource_0 ADD INDEX idx_region_network (region_code, network_type, status);

-- 使用日志表索引优化
ALTER TABLE imsi_usage_log_0 ADD INDEX idx_composite_imsi_time (imsi_id, create_time, action_type);
ALTER TABLE imsi_usage_log_0 ADD INDEX idx_composite_user_time (user_id, create_time, action_type);
ALTER TABLE imsi_usage_log_0 ADD INDEX idx_session_time_range (session_start_time, session_end_time);
ALTER TABLE imsi_usage_log_0 ADD INDEX idx_cost_time (cost_amount, create_time);

-- 分析表统计信息
ANALYZE TABLE imsi_resource_0, imsi_resource_1, imsi_resource_2, imsi_resource_3;
ANALYZE TABLE imsi_usage_log_0, imsi_usage_log_1, imsi_usage_log_2, imsi_usage_log_3;

-- 检查未使用的索引
SELECT 
    s.TABLE_SCHEMA,
    s.TABLE_NAME,
    s.INDEX_NAME,
    s.COLUMN_NAME,
    s.CARDINALITY
FROM information_schema.STATISTICS s
LEFT JOIN performance_schema.table_io_waits_summary_by_index_usage p 
    ON s.TABLE_SCHEMA = p.OBJECT_SCHEMA 
    AND s.TABLE_NAME = p.OBJECT_NAME 
    AND s.INDEX_NAME = p.INDEX_NAME
WHERE s.TABLE_SCHEMA = 'nsrs_sim_resource_0'
    AND p.INDEX_NAME IS NULL
    AND s.INDEX_NAME != 'PRIMARY'
ORDER BY s.TABLE_NAME, s.INDEX_NAME;
```

### 3. 查询优化

```java
/**
 * 查询优化服务
 */
@Service
public class QueryOptimizationService {
    
    @Autowired
    private ImsiResourceMapper imsiResourceMapper;
    
    /**
     * 优化的IMSI资源查询（使用覆盖索引）
     */
    public List<ImsiResourceDTO> getOptimizedImsiResources(ImsiResourceQuery query) {
        // 使用覆盖索引，避免回表查询
        return imsiResourceMapper.selectOptimizedByQuery(query);
    }
    
    /**
     * 批量查询优化（使用IN查询代替多次单查询）
     */
    public Map<String, ImsiResource> batchGetImsiResources(List<String> imsiList) {
        if (imsiList.size() <= 100) {
            // 小批量直接查询
            List<ImsiResource> resources = imsiResourceMapper.selectByImsiList(imsiList);
            return resources.stream().collect(Collectors.toMap(
                ImsiResource::getImsi, 
                Function.identity()
            ));
        } else {
            // 大批量分页查询
            Map<String, ImsiResource> result = new HashMap<>();
            List<List<String>> partitions = Lists.partition(imsiList, 100);
            
            for (List<String> partition : partitions) {
                List<ImsiResource> resources = imsiResourceMapper.selectByImsiList(partition);
                resources.forEach(resource -> result.put(resource.getImsi(), resource));
            }
            
            return result;
        }
    }
    
    /**
     * 分页查询优化（使用延迟关联）
     */
    public PageResult<ImsiResource> getImsiResourcesWithDelayedJoin(ImsiResourceQuery query, int page, int size) {
        // 先查询主键ID
        int offset = (page - 1) * size;
        List<Long> ids = imsiResourceMapper.selectIdsByQuery(query, offset, size);
        
        if (ids.isEmpty()) {
            return new PageResult<>(Collections.emptyList(), 0, page, size);
        }
        
        // 再根据ID查询完整数据
        List<ImsiResource> resources = imsiResourceMapper.selectByIds(ids);
        
        // 获取总数
        long total = imsiResourceMapper.countByQuery(query);
        
        return new PageResult<>(resources, total, page, size);
    }
    
    /**
     * 统计查询优化（使用预计算）
     */
    @Cacheable(value = "imsiStats", key = "#supplierId + ':' + #date")
    public ImsiResourceStats getImsiResourceStats(Long supplierId, LocalDate date) {
        // 优先从预计算表查询
        ImsiResourceStats stats = imsiResourceMapper.selectPrecomputedStats(supplierId, date);
        
        if (stats == null) {
            // 实时计算并缓存
            stats = imsiResourceMapper.selectRealtimeStats(supplierId, date);
            
            // 异步更新预计算表
            CompletableFuture.runAsync(() -> {
                imsiResourceMapper.insertOrUpdatePrecomputedStats(stats);
            });
        }
        
        return stats;
    }
    
    /**
     * 范围查询优化（使用分区裁剪）
     */
    public List<ImsiUsageLog> getUsageLogsByTimeRange(String imsi, LocalDateTime startTime, LocalDateTime endTime) {
        // 计算需要查询的分区
        List<String> partitions = calculatePartitions(startTime, endTime);
        
        List<ImsiUsageLog> result = new ArrayList<>();
        for (String partition : partitions) {
            List<ImsiUsageLog> logs = imsiResourceMapper.selectUsageLogsByTimeRangeFromPartition(
                imsi, startTime, endTime, partition
            );
            result.addAll(logs);
        }
        
        // 按时间排序
        result.sort(Comparator.comparing(ImsiUsageLog::getCreateTime));
        
        return result;
    }
    
    /**
     * 计算时间范围对应的分区
     */
    private List<String> calculatePartitions(LocalDateTime startTime, LocalDateTime endTime) {
        List<String> partitions = new ArrayList<>();
        
        LocalDateTime current = startTime.withDayOfMonth(1).withHour(0).withMinute(0).withSecond(0);
        while (!current.isAfter(endTime)) {
            String partition = current.format(DateTimeFormatter.ofPattern("yyyyMM"));
            partitions.add(partition);
            current = current.plusMonths(1);
        }
        
        return partitions;
    }
}
```

## 数据库备份和恢复

### 1. 自动备份脚本

```bash
#!/bin/bash
# mysql_backup.sh - MySQL自动备份脚本

# 配置参数
DB_HOST="10.1.7.10"
DB_PORT="3306"
DB_USER="backup_user"
DB_PASS="backup_password_2024"
BACKUP_DIR="/data/mysql_backup"
LOG_FILE="/var/log/mysql_backup.log"
RETENTION_DAYS=7

# 数据库列表
DATABASES=("nsrs_sim_resource_0" "nsrs_sim_resource_1" "nsrs_sim_resource_2" "nsrs_sim_resource_3")

# 创建备份目录
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_PATH="$BACKUP_DIR/$DATE"
mkdir -p "$BACKUP_PATH"

# 日志函数
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

log "Starting MySQL backup process"

# 备份每个数据库
for db in "${DATABASES[@]}"; do
    log "Backing up database: $db"
    
    # 全量备份
    mysqldump -h"$DB_HOST" -P"$DB_PORT" -u"$DB_USER" -p"$DB_PASS" \
        --single-transaction \
        --routines \
        --triggers \
        --events \
        --hex-blob \
        --master-data=2 \
        --flush-logs \
        "$db" | gzip > "$BACKUP_PATH/${db}_full.sql.gz"
    
    if [ $? -eq 0 ]; then
        log "Successfully backed up database: $db"
    else
        log "ERROR: Failed to backup database: $db"
        exit 1
    fi
done

# 备份二进制日志
log "Backing up binary logs"
mysql -h"$DB_HOST" -P"$DB_PORT" -u"$DB_USER" -p"$DB_PASS" \
    -e "FLUSH LOGS; SHOW MASTER STATUS;" > "$BACKUP_PATH/master_status.txt"

# 复制二进制日志文件
cp /var/lib/mysql/mysql-bin.* "$BACKUP_PATH/" 2>/dev/null || true

# 压缩备份目录
log "Compressing backup directory"
tar -czf "$BACKUP_DIR/mysql_backup_$DATE.tar.gz" -C "$BACKUP_DIR" "$DATE"
rm -rf "$BACKUP_PATH"

# 清理过期备份
log "Cleaning up old backups (retention: $RETENTION_DAYS days)"
find "$BACKUP_DIR" -name "mysql_backup_*.tar.gz" -mtime +$RETENTION_DAYS -delete

# 上传到远程存储（可选）
if [ -n "$REMOTE_BACKUP_PATH" ]; then
    log "Uploading backup to remote storage"
    rsync -avz "$BACKUP_DIR/mysql_backup_$DATE.tar.gz" "$REMOTE_BACKUP_PATH/"
fi

log "MySQL backup process completed successfully"

# 发送通知（可选）
if [ -n "$NOTIFICATION_EMAIL" ]; then
    echo "MySQL backup completed successfully at $(date)" | \
        mail -s "MySQL Backup Success - $DATE" "$NOTIFICATION_EMAIL"
fi
```

### 2. 增量备份脚本

```bash
#!/bin/bash
# mysql_incremental_backup.sh - MySQL增量备份脚本

# 配置参数
DB_HOST="10.1.7.10"
DB_PORT="3306"
DB_USER="backup_user"
DB_PASS="backup_password_2024"
BACKUP_DIR="/data/mysql_incremental_backup"
LOG_FILE="/var/log/mysql_incremental_backup.log"
LAST_BACKUP_INFO="/data/mysql_backup/last_backup_info.txt"

# 日志函数
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

log "Starting MySQL incremental backup process"

# 创建备份目录
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_PATH="$BACKUP_DIR/$DATE"
mkdir -p "$BACKUP_PATH"

# 获取当前二进制日志位置
CURRENT_STATUS=$(mysql -h"$DB_HOST" -P"$DB_PORT" -u"$DB_USER" -p"$DB_PASS" \
    -e "SHOW MASTER STATUS\G" | grep -E "File|Position")

CURRENT_FILE=$(echo "$CURRENT_STATUS" | grep "File:" | awk '{print $2}')
CURRENT_POS=$(echo "$CURRENT_STATUS" | grep "Position:" | awk '{print $2}')

log "Current binary log: $CURRENT_FILE, Position: $CURRENT_POS"

# 读取上次备份信息
if [ -f "$LAST_BACKUP_INFO" ]; then
    LAST_FILE=$(grep "File:" "$LAST_BACKUP_INFO" | awk '{print $2}')
    LAST_POS=$(grep "Position:" "$LAST_BACKUP_INFO" | awk '{print $2}')
    log "Last backup: $LAST_FILE, Position: $LAST_POS"
else
    log "No previous backup info found, performing full binary log backup"
    LAST_FILE=""
    LAST_POS="4"
fi

# 备份二进制日志
if [ -n "$LAST_FILE" ] && [ "$LAST_FILE" = "$CURRENT_FILE" ]; then
    # 同一个文件，备份增量部分
    log "Backing up incremental changes from $LAST_FILE position $LAST_POS to $CURRENT_POS"
    mysqlbinlog --start-position="$LAST_POS" --stop-position="$CURRENT_POS" \
        "/var/lib/mysql/$CURRENT_FILE" > "$BACKUP_PATH/incremental.sql"
else
    # 不同文件，备份所有新的二进制日志
    log "Backing up all binary logs since $LAST_FILE"
    
    # 获取所有二进制日志文件
    BINLOG_FILES=$(mysql -h"$DB_HOST" -P"$DB_PORT" -u"$DB_USER" -p"$DB_PASS" \
        -e "SHOW BINARY LOGS" | awk 'NR>1 {print $1}')
    
    START_BACKUP=false
    for file in $BINLOG_FILES; do
        if [ "$file" = "$LAST_FILE" ]; then
            START_BACKUP=true
            # 备份最后一个文件的剩余部分
            mysqlbinlog --start-position="$LAST_POS" "/var/lib/mysql/$file" \
                >> "$BACKUP_PATH/incremental.sql"
        elif [ "$START_BACKUP" = true ] && [ "$file" != "$CURRENT_FILE" ]; then
            # 备份中间的完整文件
            mysqlbinlog "/var/lib/mysql/$file" >> "$BACKUP_PATH/incremental.sql"
        elif [ "$file" = "$CURRENT_FILE" ]; then
            # 备份当前文件到指定位置
            mysqlbinlog --stop-position="$CURRENT_POS" "/var/lib/mysql/$file" \
                >> "$BACKUP_PATH/incremental.sql"
            break
        fi
    done
fi

# 压缩增量备份
if [ -f "$BACKUP_PATH/incremental.sql" ]; then
    gzip "$BACKUP_PATH/incremental.sql"
    log "Incremental backup completed and compressed"
else
    log "No incremental changes to backup"
    rmdir "$BACKUP_PATH"
fi

# 更新备份信息
echo "File: $CURRENT_FILE" > "$LAST_BACKUP_INFO"
echo "Position: $CURRENT_POS" >> "$LAST_BACKUP_INFO"
echo "Date: $(date)" >> "$LAST_BACKUP_INFO"

log "MySQL incremental backup process completed"
```

### 3. 数据恢复脚本

```bash
#!/bin/bash
# mysql_restore.sh - MySQL数据恢复脚本

# 配置参数
DB_HOST="10.1.7.10"
DB_PORT="3306"
DB_USER="root"
DB_PASS="root_password_2024"
BACKUP_DIR="/data/mysql_backup"
LOG_FILE="/var/log/mysql_restore.log"

# 日志函数
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

# 使用说明
usage() {
    echo "Usage: $0 [OPTIONS]"
    echo "Options:"
    echo "  -f, --full-backup FILE     Full backup file to restore"
    echo "  -i, --incremental-dir DIR  Directory containing incremental backups"
    echo "  -d, --database NAME        Database name to restore"
    echo "  -t, --target-time TIME     Point-in-time recovery target (YYYY-MM-DD HH:MM:SS)"
    echo "  -h, --help                 Show this help message"
    exit 1
}

# 解析命令行参数
while [[ $# -gt 0 ]]; do
    case $1 in
        -f|--full-backup)
            FULL_BACKUP="$2"
            shift 2
            ;;
        -i|--incremental-dir)
            INCREMENTAL_DIR="$2"
            shift 2
            ;;
        -d|--database)
            DATABASE="$2"
            shift 2
            ;;
        -t|--target-time)
            TARGET_TIME="$2"
            shift 2
            ;;
        -h|--help)
            usage
            ;;
        *)
            echo "Unknown option: $1"
            usage
            ;;
    esac
done

# 检查必需参数
if [ -z "$FULL_BACKUP" ] || [ -z "$DATABASE" ]; then
    echo "Error: Full backup file and database name are required"
    usage
fi

log "Starting MySQL restore process"
log "Full backup: $FULL_BACKUP"
log "Database: $DATABASE"
log "Target time: ${TARGET_TIME:-'Latest'}"

# 检查备份文件
if [ ! -f "$FULL_BACKUP" ]; then
    log "ERROR: Full backup file not found: $FULL_BACKUP"
    exit 1
fi

# 停止应用服务（可选）
log "Stopping application services"
# systemctl stop nsrs-application

# 创建数据库（如果不存在）
log "Creating database if not exists: $DATABASE"
mysql -h"$DB_HOST" -P"$DB_PORT" -u"$DB_USER" -p"$DB_PASS" \
    -e "CREATE DATABASE IF NOT EXISTS $DATABASE CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;"

# 恢复全量备份
log "Restoring full backup"
if [[ "$FULL_BACKUP" == *.gz ]]; then
    gunzip -c "$FULL_BACKUP" | mysql -h"$DB_HOST" -P"$DB_PORT" -u"$DB_USER" -p"$DB_PASS" "$DATABASE"
else
    mysql -h"$DB_HOST" -P"$DB_PORT" -u"$DB_USER" -p"$DB_PASS" "$DATABASE" < "$FULL_BACKUP"
fi

if [ $? -eq 0 ]; then
    log "Full backup restored successfully"
else
    log "ERROR: Failed to restore full backup"
    exit 1
fi

# 恢复增量备份
if [ -n "$INCREMENTAL_DIR" ] && [ -d "$INCREMENTAL_DIR" ]; then
    log "Restoring incremental backups from: $INCREMENTAL_DIR"
    
    # 按时间顺序处理增量备份
    for incremental_file in $(ls "$INCREMENTAL_DIR"/*/incremental.sql.gz 2>/dev/null | sort); do
        log "Applying incremental backup: $incremental_file"
        
        if [ -n "$TARGET_TIME" ]; then
            # 点对点恢复
            gunzip -c "$incremental_file" | mysql -h"$DB_HOST" -P"$DB_PORT" -u"$DB_USER" -p"$DB_PASS" \
                --stop-datetime="$TARGET_TIME" "$DATABASE"
        else
            # 恢复到最新
            gunzip -c "$incremental_file" | mysql -h"$DB_HOST" -P"$DB_PORT" -u"$DB_USER" -p"$DB_PASS" "$DATABASE"
        fi
        
        if [ $? -eq 0 ]; then
            log "Incremental backup applied successfully: $incremental_file"
        else
            log "ERROR: Failed to apply incremental backup: $incremental_file"
            exit 1
        fi
    done
fi

# 验证恢复结果
log "Verifying restore results"
TABLE_COUNT=$(mysql -h"$DB_HOST" -P"$DB_PORT" -u"$DB_USER" -p"$DB_PASS" "$DATABASE" \
    -e "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='$DATABASE'" -s -N)

log "Restored database contains $TABLE_COUNT tables"

# 重建索引和统计信息
log "Rebuilding indexes and updating statistics"
mysql -h"$DB_HOST" -P"$DB_PORT" -u"$DB_USER" -p"$DB_PASS" "$DATABASE" \
    -e "ANALYZE TABLE $(mysql -h"$DB_HOST" -P"$DB_PORT" -u"$DB_USER" -p"$DB_PASS" "$DATABASE" \
        -e "SELECT GROUP_CONCAT(table_name) FROM information_schema.tables WHERE table_schema='$DATABASE'" -s -N);"

# 启动应用服务
log "Starting application services"
# systemctl start nsrs-application

log "MySQL restore process completed successfully"

# 发送通知
if [ -n "$NOTIFICATION_EMAIL" ]; then
    echo "MySQL restore completed successfully for database $DATABASE at $(date)" | \
        mail -s "MySQL Restore Success" "$NOTIFICATION_EMAIL"
fi
```

## 最佳实践总结

### 1. 分库分表最佳实践

- **合理选择分片键**: 选择数据分布均匀、查询频繁的字段作为分片键
- **避免跨库事务**: 尽量将相关数据分配到同一个库中
- **预留扩展空间**: 分表数量设置为2的幂次方，便于后续扩展
- **统一分片规则**: 所有相关表使用相同的分片策略
- **监控数据倾斜**: 定期检查各分片的数据分布情况

### 2. 读写分离最佳实践

- **延迟监控**: 实时监控主从复制延迟，超过阈值时切换到主库读取
- **负载均衡**: 合理配置从库权重，避免单个从库压力过大
- **故障转移**: 从库故障时自动切换到其他可用从库
- **数据一致性**: 对一致性要求高的查询强制从主库读取
- **连接池优化**: 主库和从库使用不同的连接池配置

### 3. 性能优化最佳实践

- **索引优化**: 定期分析慢查询，优化索引设计
- **查询优化**: 避免全表扫描，使用覆盖索引
- **配置调优**: 根据硬件资源和业务特点调整MySQL配置
- **监控告警**: 建立完善的数据库监控和告警体系
- **容量规划**: 定期评估数据增长趋势，提前进行容量规划

### 4. 运维管理最佳实践

- **自动化备份**: 建立完善的备份策略，包括全量备份和增量备份
- **定期演练**: 定期进行数据恢复演练，验证备份的有效性
- **版本管理**: 使用数据库版本管理工具，规范数据库变更流程
- **安全加固**: 定期更新数据库版本，加强访问控制和权限管理
- **文档维护**: 维护详细的数据库架构文档和运维手册

通过以上数据库架构设计和实施方案，NSRS号卡资源管理系统可以实现高性能、高可用、可扩展的数据存储和访问能力，为业务发展提供坚实的数据基础。