# NSRS号卡资源管理系统 - 大数据处理详解

## 概述

大数据处理是NSRS号卡资源管理系统的重要组成部分，通过实时计算、数据湖、流式处理等技术，实现海量号卡数据的高效处理、实时分析和智能决策，为电信运营商提供数据驱动的业务洞察和优化建议。

## 大数据核心概念

### 数据处理类型枚举

```java
/**
 * 数据处理类型枚举
 * 定义不同的数据处理模式和策略
 */
public enum DataProcessingType {
    /**
     * 批处理
     */
    BATCH_PROCESSING("BATCH_PROCESSING", "批处理", "离线批量数据处理"),
    
    /**
     * 流处理
     */
    STREAM_PROCESSING("STREAM_PROCESSING", "流处理", "实时流式数据处理"),
    
    /**
     * 微批处理
     */
    MICRO_BATCH_PROCESSING("MICRO_BATCH_PROCESSING", "微批处理", "小批量准实时处理"),
    
    /**
     * 混合处理
     */
    HYBRID_PROCESSING("HYBRID_PROCESSING", "混合处理", "批流一体化处理"),
    
    /**
     * 交互式查询
     */
    INTERACTIVE_QUERY("INTERACTIVE_QUERY", "交互式查询", "即席查询分析"),
    
    /**
     * 机器学习
     */
    MACHINE_LEARNING("MACHINE_LEARNING", "机器学习", "ML模型训练和推理");
    
    private final String code;
    private final String description;
    private final String detail;
    
    DataProcessingType(String code, String description, String detail) {
        this.code = code;
        this.description = description;
        this.detail = detail;
    }
    
    // getters...
}
```

### 数据存储层级枚举

```java
/**
 * 数据存储层级枚举
 * 定义数据湖的分层存储架构
 */
public enum DataStorageLayer {
    /**
     * 原始数据层（Bronze）
     */
    RAW_DATA_LAYER("RAW_DATA_LAYER", "原始数据层", "Bronze", "未经处理的原始数据"),
    
    /**
     * 清洗数据层（Silver）
     */
    CLEANED_DATA_LAYER("CLEANED_DATA_LAYER", "清洗数据层", "Silver", "清洗和标准化后的数据"),
    
    /**
     * 聚合数据层（Gold）
     */
    AGGREGATED_DATA_LAYER("AGGREGATED_DATA_LAYER", "聚合数据层", "Gold", "业务聚合和分析就绪的数据"),
    
    /**
     * 特征数据层（Platinum）
     */
    FEATURE_DATA_LAYER("FEATURE_DATA_LAYER", "特征数据层", "Platinum", "机器学习特征工程数据"),
    
    /**
     * 归档数据层（Archive）
     */
    ARCHIVE_DATA_LAYER("ARCHIVE_DATA_LAYER", "归档数据层", "Archive", "长期归档存储数据");
    
    private final String code;
    private final String description;
    private final String layer;
    private final String detail;
    
    DataStorageLayer(String code, String description, String layer, String detail) {
        this.code = code;
        this.description = description;
        this.layer = layer;
        this.detail = detail;
    }
    
    // getters...
}
```

### 实时计算引擎枚举

```java
/**
 * 实时计算引擎枚举
 * 定义支持的流处理引擎
 */
public enum StreamProcessingEngine {
    /**
     * Apache Flink
     */
    FLINK("FLINK", "Apache Flink", "低延迟流处理引擎"),
    
    /**
     * Apache Kafka Streams
     */
    KAFKA_STREAMS("KAFKA_STREAMS", "Kafka Streams", "轻量级流处理库"),
    
    /**
     * Apache Storm
     */
    STORM("STORM", "Apache Storm", "分布式实时计算系统"),
    
    /**
     * Apache Spark Streaming
     */
    SPARK_STREAMING("SPARK_STREAMING", "Spark Streaming", "微批流处理引擎"),
    
    /**
     * Apache Pulsar Functions
     */
    PULSAR_FUNCTIONS("PULSAR_FUNCTIONS", "Pulsar Functions", "轻量级计算框架");
    
    private final String code;
    private final String description;
    private final String detail;
    
    StreamProcessingEngine(String code, String description, String detail) {
        this.code = code;
        this.description = description;
        this.detail = detail;
    }
    
    // getters...
}
```

## 数据湖架构设计

### 数据湖配置实体

```java
/**
 * 数据湖配置实体
 * 定义数据湖的存储和处理配置
 */
@Entity
@Table(name = "data_lake_config")
public class DataLakeConfig {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    /**
     * 配置名称
     */
    @Column(name = "config_name", nullable = false, length = 100)
    private String configName;
    
    /**
     * 数据存储层级
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "storage_layer", nullable = false)
    private DataStorageLayer storageLayer;
    
    /**
     * 存储路径
     */
    @Column(name = "storage_path", nullable = false, length = 500)
    private String storagePath;
    
    /**
     * 数据格式
     */
    @Column(name = "data_format", nullable = false, length = 50)
    private String dataFormat;
    
    /**
     * 分区策略
     */
    @Column(name = "partition_strategy", length = 200)
    private String partitionStrategy;
    
    /**
     * 压缩算法
     */
    @Column(name = "compression_codec", length = 50)
    private String compressionCodec;
    
    /**
     * 保留策略（天）
     */
    @Column(name = "retention_days")
    private Integer retentionDays;
    
    /**
     * 访问权限配置
     */
    @Column(name = "access_policy", columnDefinition = "TEXT")
    private String accessPolicy;
    
    /**
     * 元数据配置
     */
    @Column(name = "metadata_config", columnDefinition = "TEXT")
    private String metadataConfig;
    
    /**
     * 是否启用
     */
    @Column(name = "enabled", nullable = false)
    private Boolean enabled;
    
    /**
     * 创建时间
     */
    @Column(name = "created_at")
    private LocalDateTime createdAt;
    
    /**
     * 更新时间
     */
    @Column(name = "updated_at")
    private LocalDateTime updatedAt;
    
    // constructors, getters, setters...
}
```

### 流处理作业实体

```java
/**
 * 流处理作业实体
 * 定义实时计算作业的配置和状态
 */
@Entity
@Table(name = "stream_processing_job")
public class StreamProcessingJob {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    /**
     * 作业名称
     */
    @Column(name = "job_name", nullable = false, length = 100)
    private String jobName;
    
    /**
     * 作业描述
     */
    @Column(name = "job_description", length = 500)
    private String jobDescription;
    
    /**
     * 处理引擎
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "processing_engine", nullable = false)
    private StreamProcessingEngine processingEngine;
    
    /**
     * 作业配置（JSON格式）
     */
    @Column(name = "job_config", columnDefinition = "TEXT")
    private String jobConfig;
    
    /**
     * 输入数据源
     */
    @Column(name = "input_sources", columnDefinition = "TEXT")
    private String inputSources;
    
    /**
     * 输出目标
     */
    @Column(name = "output_targets", columnDefinition = "TEXT")
    private String outputTargets;
    
    /**
     * 作业状态
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "job_status", nullable = false)
    private StreamJobStatus jobStatus;
    
    /**
     * 并行度
     */
    @Column(name = "parallelism")
    private Integer parallelism;
    
    /**
     * 检查点间隔（毫秒）
     */
    @Column(name = "checkpoint_interval")
    private Long checkpointInterval;
    
    /**
     * 重启策略
     */
    @Column(name = "restart_strategy", length = 200)
    private String restartStrategy;
    
    /**
     * 资源配置
     */
    @Column(name = "resource_config", columnDefinition = "TEXT")
    private String resourceConfig;
    
    /**
     * 作业启动时间
     */
    @Column(name = "started_at")
    private LocalDateTime startedAt;
    
    /**
     * 作业停止时间
     */
    @Column(name = "stopped_at")
    private LocalDateTime stoppedAt;
    
    /**
     * 创建时间
     */
    @Column(name = "created_at")
    private LocalDateTime createdAt;
    
    /**
     * 更新时间
     */
    @Column(name = "updated_at")
    private LocalDateTime updatedAt;
    
    // constructors, getters, setters...
}
```

### 流处理作业状态枚举

```java
/**
 * 流处理作业状态枚举
 */
public enum StreamJobStatus {
    /**
     * 创建中
     */
    CREATING("CREATING", "创建中"),
    
    /**
     * 运行中
     */
    RUNNING("RUNNING", "运行中"),
    
    /**
     * 暂停
     */
    SUSPENDED("SUSPENDED", "暂停"),
    
    /**
     * 停止
     */
    STOPPED("STOPPED", "停止"),
    
    /**
     * 失败
     */
    FAILED("FAILED", "失败"),
    
    /**
     * 重启中
     */
    RESTARTING("RESTARTING", "重启中"),
    
    /**
     * 已完成
     */
    FINISHED("FINISHED", "已完成");
    
    private final String code;
    private final String description;
    
    StreamJobStatus(String code, String description) {
        this.code = code;
        this.description = description;
    }
    
    // getters...
}
```

## 数据湖管理服务

### 数据湖服务

```java
/**
 * 数据湖管理服务
 * 负责数据湖的存储、管理和访问
 */
@Service
@Slf4j
public class DataLakeService {
    
    @Autowired
    private DataLakeConfigRepository dataLakeConfigRepository;
    
    @Autowired
    private HadoopFileSystem hadoopFileSystem;
    
    @Autowired
    private DeltaLakeService deltaLakeService;
    
    @Autowired
    private MetadataService metadataService;
    
    /**
     * 写入数据到数据湖
     * 
     * @param data 数据内容
     * @param storageLayer 存储层级
     * @param dataType 数据类型
     * @param partitionKeys 分区键
     * @return 写入结果
     */
    @Transactional
    public DataWriteResult writeToDataLake(Object data, 
                                         DataStorageLayer storageLayer,
                                         String dataType,
                                         Map<String, String> partitionKeys) {
        log.info("写入数据到数据湖: layer={}, type={}, partitions={}", 
                storageLayer, dataType, partitionKeys);
        
        try {
            // 1. 获取存储配置
            DataLakeConfig config = getStorageConfig(storageLayer, dataType);
            
            // 2. 生成存储路径
            String storagePath = generateStoragePath(config, partitionKeys);
            
            // 3. 数据格式转换
            byte[] formattedData = formatData(data, config.getDataFormat());
            
            // 4. 写入数据
            String dataPath = writeDataToStorage(storagePath, formattedData, config);
            
            // 5. 更新元数据
            updateMetadata(dataPath, data, config, partitionKeys);
            
            // 6. 创建写入结果
            DataWriteResult result = new DataWriteResult();
            result.setSuccess(true);
            result.setDataPath(dataPath);
            result.setStorageLayer(storageLayer);
            result.setDataSize(formattedData.length);
            result.setWriteTime(LocalDateTime.now());
            
            log.info("数据写入成功: path={}, size={}", dataPath, formattedData.length);
            return result;
            
        } catch (Exception e) {
            log.error("写入数据到数据湖失败", e);
            
            DataWriteResult result = new DataWriteResult();
            result.setSuccess(false);
            result.setErrorMessage(e.getMessage());
            return result;
        }
    }
    
    /**
     * 从数据湖读取数据
     * 
     * @param query 查询条件
     * @return 查询结果
     */
    public DataQueryResult queryFromDataLake(DataLakeQuery query) {
        log.info("从数据湖查询数据: {}", query);
        
        try {
            // 1. 解析查询条件
            List<String> dataPaths = resolveDataPaths(query);
            
            // 2. 执行查询
            List<Object> results = new ArrayList<>();
            for (String dataPath : dataPaths) {
                List<Object> pathResults = queryDataFromPath(dataPath, query);
                results.addAll(pathResults);
            }
            
            // 3. 数据过滤和聚合
            List<Object> filteredResults = applyFiltersAndAggregations(results, query);
            
            // 4. 创建查询结果
            DataQueryResult result = new DataQueryResult();
            result.setSuccess(true);
            result.setData(filteredResults);
            result.setTotalCount(filteredResults.size());
            result.setQueryTime(LocalDateTime.now());
            
            log.info("数据查询成功: count={}", filteredResults.size());
            return result;
            
        } catch (Exception e) {
            log.error("从数据湖查询数据失败", e);
            
            DataQueryResult result = new DataQueryResult();
            result.setSuccess(false);
            result.setErrorMessage(e.getMessage());
            return result;
        }
    }
    
    /**
     * 数据分层处理
     * 
     * @param sourceLayer 源数据层
     * @param targetLayer 目标数据层
     * @param transformationRules 转换规则
     * @return 处理结果
     */
    @Async
    public CompletableFuture<DataTransformationResult> transformDataLayer(
            DataStorageLayer sourceLayer,
            DataStorageLayer targetLayer,
            List<DataTransformationRule> transformationRules) {
        
        log.info("数据分层处理: {} -> {}", sourceLayer, targetLayer);
        
        try {
            // 1. 获取源数据
            DataLakeQuery sourceQuery = createLayerQuery(sourceLayer);
            DataQueryResult sourceData = queryFromDataLake(sourceQuery);
            
            // 2. 应用转换规则
            List<Object> transformedData = applyTransformationRules(
                    sourceData.getData(), transformationRules);
            
            // 3. 写入目标层
            List<DataWriteResult> writeResults = new ArrayList<>();
            for (Object data : transformedData) {
                Map<String, String> partitionKeys = extractPartitionKeys(data);
                DataWriteResult writeResult = writeToDataLake(
                        data, targetLayer, "transformed", partitionKeys);
                writeResults.add(writeResult);
            }
            
            // 4. 创建转换结果
            DataTransformationResult result = new DataTransformationResult();
            result.setSuccess(true);
            result.setSourceLayer(sourceLayer);
            result.setTargetLayer(targetLayer);
            result.setProcessedCount(transformedData.size());
            result.setWriteResults(writeResults);
            result.setTransformationTime(LocalDateTime.now());
            
            log.info("数据分层处理完成: processed={}", transformedData.size());
            return CompletableFuture.completedFuture(result);
            
        } catch (Exception e) {
            log.error("数据分层处理失败", e);
            
            DataTransformationResult result = new DataTransformationResult();
            result.setSuccess(false);
            result.setErrorMessage(e.getMessage());
            return CompletableFuture.completedFuture(result);
        }
    }
    
    /**
     * 数据生命周期管理
     * 
     * @param storageLayer 存储层级
     * @return 清理结果
     */
    @Scheduled(cron = "0 0 2 * * ?")
    public DataLifecycleResult manageDataLifecycle(DataStorageLayer storageLayer) {
        log.info("执行数据生命周期管理: layer={}", storageLayer);
        
        try {
            // 1. 获取存储配置
            List<DataLakeConfig> configs = dataLakeConfigRepository
                    .findByStorageLayerAndEnabled(storageLayer, true);
            
            int totalCleaned = 0;
            long totalSizeFreed = 0;
            
            for (DataLakeConfig config : configs) {
                // 2. 查找过期数据
                List<String> expiredPaths = findExpiredData(config);
                
                // 3. 归档或删除过期数据
                for (String path : expiredPaths) {
                    long size = getDataSize(path);
                    
                    if (shouldArchive(config, path)) {
                        archiveData(path, config);
                    } else {
                        deleteData(path);
                    }
                    
                    totalCleaned++;
                    totalSizeFreed += size;
                }
            }
            
            // 4. 创建清理结果
            DataLifecycleResult result = new DataLifecycleResult();
            result.setSuccess(true);
            result.setStorageLayer(storageLayer);
            result.setCleanedCount(totalCleaned);
            result.setSizeFreed(totalSizeFreed);
            result.setCleanupTime(LocalDateTime.now());
            
            log.info("数据生命周期管理完成: cleaned={}, freed={}MB", 
                    totalCleaned, totalSizeFreed / 1024 / 1024);
            return result;
            
        } catch (Exception e) {
            log.error("数据生命周期管理失败", e);
            
            DataLifecycleResult result = new DataLifecycleResult();
            result.setSuccess(false);
            result.setErrorMessage(e.getMessage());
            return result;
        }
    }
    
    // 辅助方法实现...
    private DataLakeConfig getStorageConfig(DataStorageLayer layer, String dataType) {
        return dataLakeConfigRepository.findByStorageLayerAndDataTypeAndEnabled(layer, dataType, true)
                .orElseThrow(() -> new RuntimeException("存储配置不存在: " + layer + ", " + dataType));
    }
    
    private String generateStoragePath(DataLakeConfig config, Map<String, String> partitionKeys) {
        StringBuilder pathBuilder = new StringBuilder(config.getStoragePath());
        
        // 添加分区路径
        if (partitionKeys != null && !partitionKeys.isEmpty()) {
            for (Map.Entry<String, String> entry : partitionKeys.entrySet()) {
                pathBuilder.append("/").append(entry.getKey()).append("=").append(entry.getValue());
            }
        }
        
        // 添加时间分区
        LocalDateTime now = LocalDateTime.now();
        pathBuilder.append("/year=").append(now.getYear())
                  .append("/month=").append(String.format("%02d", now.getMonthValue()))
                  .append("/day=").append(String.format("%02d", now.getDayOfMonth()))
                  .append("/hour=").append(String.format("%02d", now.getHour()));
        
        return pathBuilder.toString();
    }
    
    private byte[] formatData(Object data, String format) {
        switch (format.toLowerCase()) {
            case "parquet":
                return convertToParquet(data);
            case "delta":
                return convertToDelta(data);
            case "json":
                return JsonUtils.toJson(data).getBytes(StandardCharsets.UTF_8);
            case "avro":
                return convertToAvro(data);
            default:
                throw new RuntimeException("不支持的数据格式: " + format);
        }
    }
    
    private String writeDataToStorage(String path, byte[] data, DataLakeConfig config) {
        // 实现数据写入逻辑
        String fileName = generateFileName(config.getDataFormat());
        String fullPath = path + "/" + fileName;
        
        hadoopFileSystem.writeFile(fullPath, data);
        
        return fullPath;
    }
    
    private void updateMetadata(String dataPath, Object data, DataLakeConfig config, Map<String, String> partitionKeys) {
        // 实现元数据更新逻辑
        metadataService.registerDataset(dataPath, data, config, partitionKeys);
    }
    
    private List<String> resolveDataPaths(DataLakeQuery query) {
        // 实现数据路径解析逻辑
        return metadataService.findDataPaths(query);
    }
    
    private List<Object> queryDataFromPath(String dataPath, DataLakeQuery query) {
        // 实现数据查询逻辑
        return hadoopFileSystem.readData(dataPath, query);
    }
    
    private List<Object> applyFiltersAndAggregations(List<Object> data, DataLakeQuery query) {
        // 实现过滤和聚合逻辑
        return data.stream()
                .filter(item -> matchesFilter(item, query.getFilters()))
                .collect(Collectors.toList());
    }
    
    private boolean matchesFilter(Object item, Map<String, Object> filters) {
        // 实现过滤匹配逻辑
        return true;
    }
    
    private DataLakeQuery createLayerQuery(DataStorageLayer layer) {
        // 实现层级查询创建逻辑
        DataLakeQuery query = new DataLakeQuery();
        query.setStorageLayer(layer);
        return query;
    }
    
    private List<Object> applyTransformationRules(List<Object> data, List<DataTransformationRule> rules) {
        // 实现转换规则应用逻辑
        return data.stream()
                .map(item -> applyRules(item, rules))
                .collect(Collectors.toList());
    }
    
    private Object applyRules(Object item, List<DataTransformationRule> rules) {
        // 实现单个数据项转换逻辑
        return item;
    }
    
    private Map<String, String> extractPartitionKeys(Object data) {
        // 实现分区键提取逻辑
        return new HashMap<>();
    }
    
    private List<String> findExpiredData(DataLakeConfig config) {
        // 实现过期数据查找逻辑
        return new ArrayList<>();
    }
    
    private long getDataSize(String path) {
        // 实现数据大小获取逻辑
        return hadoopFileSystem.getFileSize(path);
    }
    
    private boolean shouldArchive(DataLakeConfig config, String path) {
        // 实现归档判断逻辑
        return config.getStorageLayer() != DataStorageLayer.ARCHIVE_DATA_LAYER;
    }
    
    private void archiveData(String path, DataLakeConfig config) {
        // 实现数据归档逻辑
        String archivePath = path.replace(config.getStoragePath(), 
                config.getStoragePath().replace(config.getStorageLayer().getLayer(), "archive"));
        hadoopFileSystem.moveFile(path, archivePath);
    }
    
    private void deleteData(String path) {
        // 实现数据删除逻辑
        hadoopFileSystem.deleteFile(path);
    }
    
    private String generateFileName(String format) {
        // 实现文件名生成逻辑
        return UUID.randomUUID().toString() + "." + format;
    }
    
    private byte[] convertToParquet(Object data) {
        // 实现Parquet格式转换
        return new byte[0];
    }
    
    private byte[] convertToDelta(Object data) {
        // 实现Delta格式转换
        return new byte[0];
    }
    
    private byte[] convertToAvro(Object data) {
        // 实现Avro格式转换
        return new byte[0];
    }
}
```

## 实时计算服务

### Flink流处理服务

```java
/**
 * Flink流处理服务
 * 负责实时数据流的处理和分析
 */
@Service
@Slf4j
public class FlinkStreamProcessingService {
    
    @Autowired
    private StreamProcessingJobRepository streamJobRepository;
    
    @Autowired
    private FlinkClusterClient flinkClusterClient;
    
    @Autowired
    private KafkaProducerService kafkaProducerService;
    
    @Autowired
    private DataLakeService dataLakeService;
    
    /**
     * 创建并启动流处理作业
     * 
     * @param jobConfig 作业配置
     * @return 作业信息
     */
    @Transactional
    public StreamProcessingJob createAndStartJob(StreamJobConfig jobConfig) {
        log.info("创建并启动流处理作业: {}", jobConfig.getJobName());
        
        try {
            // 1. 创建作业记录
            StreamProcessingJob job = new StreamProcessingJob();
            job.setJobName(jobConfig.getJobName());
            job.setJobDescription(jobConfig.getJobDescription());
            job.setProcessingEngine(StreamProcessingEngine.FLINK);
            job.setJobConfig(JsonUtils.toJson(jobConfig));
            job.setInputSources(JsonUtils.toJson(jobConfig.getInputSources()));
            job.setOutputTargets(JsonUtils.toJson(jobConfig.getOutputTargets()));
            job.setJobStatus(StreamJobStatus.CREATING);
            job.setParallelism(jobConfig.getParallelism());
            job.setCheckpointInterval(jobConfig.getCheckpointInterval());
            job.setRestartStrategy(jobConfig.getRestartStrategy());
            job.setResourceConfig(JsonUtils.toJson(jobConfig.getResourceConfig()));
            job.setCreatedAt(LocalDateTime.now());
            
            job = streamJobRepository.save(job);
            
            // 2. 构建Flink作业
            StreamExecutionEnvironment env = buildFlinkJob(jobConfig);
            
            // 3. 提交作业到Flink集群
            JobExecutionResult executionResult = env.execute(jobConfig.getJobName());
            
            // 4. 更新作业状态
            job.setJobStatus(StreamJobStatus.RUNNING);
            job.setStartedAt(LocalDateTime.now());
            job.setUpdatedAt(LocalDateTime.now());
            
            log.info("流处理作业启动成功: id={}, name={}", job.getId(), job.getJobName());
            return streamJobRepository.save(job);
            
        } catch (Exception e) {
            log.error("创建并启动流处理作业失败", e);
            throw new RuntimeException("创建并启动流处理作业失败: " + e.getMessage());
        }
    }
    
    /**
     * 构建Flink作业
     */
    private StreamExecutionEnvironment buildFlinkJob(StreamJobConfig jobConfig) {
        // 1. 创建执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // 2. 配置检查点
        env.enableCheckpointing(jobConfig.getCheckpointInterval());
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);
        env.getCheckpointConfig().setCheckpointTimeout(60000);
        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);
        
        // 3. 配置重启策略
        env.setRestartStrategy(RestartStrategies.fixedDelayRestart(
                3, // 重启次数
                Time.of(10, TimeUnit.SECONDS) // 重启间隔
        ));
        
        // 4. 设置并行度
        env.setParallelism(jobConfig.getParallelism());
        
        // 5. 根据作业类型构建数据流
        switch (jobConfig.getJobType()) {
            case "sim_card_usage_analysis":
                buildSimCardUsageAnalysisJob(env, jobConfig);
                break;
            case "real_time_monitoring":
                buildRealTimeMonitoringJob(env, jobConfig);
                break;
            case "fraud_detection":
                buildFraudDetectionJob(env, jobConfig);
                break;
            case "data_quality_check":
                buildDataQualityCheckJob(env, jobConfig);
                break;
            default:
                throw new RuntimeException("不支持的作业类型: " + jobConfig.getJobType());
        }
        
        return env;
    }
    
    /**
     * 构建SIM卡使用分析作业
     */
    private void buildSimCardUsageAnalysisJob(StreamExecutionEnvironment env, StreamJobConfig jobConfig) {
        // 1. 创建Kafka数据源
        FlinkKafkaConsumer<String> kafkaSource = new FlinkKafkaConsumer<>(
                "sim-card-usage-events",
                new SimpleStringSchema(),
                getKafkaProperties(jobConfig)
        );
        
        // 2. 添加数据源
        DataStream<String> sourceStream = env.addSource(kafkaSource)
                .name("SIM卡使用事件源");
        
        // 3. 数据解析和转换
        DataStream<SimCardUsageEvent> parsedStream = sourceStream
                .map(new SimCardUsageEventParser())
                .name("事件解析");
        
        // 4. 数据清洗和过滤
        DataStream<SimCardUsageEvent> cleanedStream = parsedStream
                .filter(event -> event != null && event.getImsi() != null)
                .name("数据清洗");
        
        // 5. 按IMSI分组并计算使用统计
        DataStream<SimCardUsageStatistics> statisticsStream = cleanedStream
                .keyBy(SimCardUsageEvent::getImsi)
                .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
                .aggregate(new SimCardUsageAggregator())
                .name("使用统计计算");
        
        // 6. 异常检测
        DataStream<SimCardUsageAlert> alertStream = statisticsStream
                .filter(stats -> stats.getDataUsage() > 1000 || stats.getCallDuration() > 3600)
                .map(new UsageAlertGenerator())
                .name("异常检测");
        
        // 7. 输出到Kafka
        FlinkKafkaProducer<String> kafkaSink = new FlinkKafkaProducer<>(
                "sim-card-usage-alerts",
                new SimpleStringSchema(),
                getKafkaProperties(jobConfig)
        );
        
        alertStream
                .map(alert -> JsonUtils.toJson(alert))
                .addSink(kafkaSink)
                .name("告警输出");
        
        // 8. 输出到数据湖
        statisticsStream
                .addSink(new DataLakeSink<>(dataLakeService, DataStorageLayer.CLEANED_DATA_LAYER))
                .name("数据湖输出");
    }
    
    /**
     * 构建实时监控作业
     */
    private void buildRealTimeMonitoringJob(StreamExecutionEnvironment env, StreamJobConfig jobConfig) {
        // 1. 创建多个数据源
        DataStream<String> systemMetricsStream = env
                .addSource(new FlinkKafkaConsumer<>(
                        "system-metrics",
                        new SimpleStringSchema(),
                        getKafkaProperties(jobConfig)
                ))
                .name("系统指标源");
        
        DataStream<String> businessMetricsStream = env
                .addSource(new FlinkKafkaConsumer<>(
                        "business-metrics",
                        new SimpleStringSchema(),
                        getKafkaProperties(jobConfig)
                ))
                .name("业务指标源");
        
        // 2. 解析指标数据
        DataStream<SystemMetric> systemMetrics = systemMetricsStream
                .map(new SystemMetricParser())
                .name("系统指标解析");
        
        DataStream<BusinessMetric> businessMetrics = businessMetricsStream
                .map(new BusinessMetricParser())
                .name("业务指标解析");
        
        // 3. 实时聚合计算
        DataStream<SystemHealthStatus> systemHealthStream = systemMetrics
                .keyBy(SystemMetric::getServiceName)
                .window(SlidingProcessingTimeWindows.of(Time.minutes(5), Time.minutes(1)))
                .aggregate(new SystemHealthAggregator())
                .name("系统健康状态计算");
        
        DataStream<BusinessKPI> businessKPIStream = businessMetrics
                .keyBy(BusinessMetric::getMetricType)
                .window(TumblingProcessingTimeWindows.of(Time.minutes(1)))
                .aggregate(new BusinessKPIAggregator())
                .name("业务KPI计算");
        
        // 4. 告警检测
        DataStream<MonitoringAlert> systemAlerts = systemHealthStream
                .filter(health -> health.getCpuUsage() > 80 || health.getMemoryUsage() > 85)
                .map(new SystemAlertGenerator())
                .name("系统告警检测");
        
        DataStream<MonitoringAlert> businessAlerts = businessKPIStream
                .filter(kpi -> kpi.getValue() < kpi.getThreshold())
                .map(new BusinessAlertGenerator())
                .name("业务告警检测");
        
        // 5. 合并告警流
        DataStream<MonitoringAlert> allAlerts = systemAlerts
                .union(businessAlerts)
                .name("告警合并");
        
        // 6. 告警去重和聚合
        DataStream<MonitoringAlert> deduplicatedAlerts = allAlerts
                .keyBy(MonitoringAlert::getAlertKey)
                .window(TumblingProcessingTimeWindows.of(Time.minutes(1)))
                .reduce(new AlertDeduplicator())
                .name("告警去重");
        
        // 7. 输出告警
        deduplicatedAlerts
                .addSink(new FlinkKafkaProducer<>(
                        "monitoring-alerts",
                        new SimpleStringSchema(),
                        getKafkaProperties(jobConfig)
                ))
                .name("告警输出");
        
        // 8. 输出监控数据到数据湖
        systemHealthStream
                .addSink(new DataLakeSink<>(dataLakeService, DataStorageLayer.AGGREGATED_DATA_LAYER))
                .name("系统健康数据输出");
        
        businessKPIStream
                .addSink(new DataLakeSink<>(dataLakeService, DataStorageLayer.AGGREGATED_DATA_LAYER))
                .name("业务KPI数据输出");
    }
    
    /**
     * 构建欺诈检测作业
     */
    private void buildFraudDetectionJob(StreamExecutionEnvironment env, StreamJobConfig jobConfig) {
        // 1. 创建交易事件数据源
        DataStream<String> transactionStream = env
                .addSource(new FlinkKafkaConsumer<>(
                        "transaction-events",
                        new SimpleStringSchema(),
                        getKafkaProperties(jobConfig)
                ))
                .name("交易事件源");
        
        // 2. 解析交易事件
        DataStream<TransactionEvent> parsedTransactions = transactionStream
                .map(new TransactionEventParser())
                .name("交易事件解析");
        
        // 3. 特征提取
        DataStream<TransactionFeatures> featuresStream = parsedTransactions
                .keyBy(TransactionEvent::getUserId)
                .window(SlidingProcessingTimeWindows.of(Time.hours(1), Time.minutes(5)))
                .aggregate(new TransactionFeatureExtractor())
                .name("特征提取");
        
        // 4. 欺诈检测模型推理
        DataStream<FraudDetectionResult> detectionResults = featuresStream
                .map(new FraudDetectionModel())
                .name("欺诈检测");
        
        // 5. 风险评分和分级
        DataStream<RiskAssessment> riskAssessments = detectionResults
                .map(new RiskScoreCalculator())
                .name("风险评分");
        
        // 6. 高风险交易告警
        DataStream<FraudAlert> fraudAlerts = riskAssessments
                .filter(risk -> risk.getRiskScore() > 0.8)
                .map(new FraudAlertGenerator())
                .name("欺诈告警生成");
        
        // 7. 输出欺诈告警
        fraudAlerts
                .addSink(new FlinkKafkaProducer<>(
                        "fraud-alerts",
                        new SimpleStringSchema(),
                        getKafkaProperties(jobConfig)
                ))
                .name("欺诈告警输出");
        
        // 8. 输出风险评估结果到数据湖
        riskAssessments
                .addSink(new DataLakeSink<>(dataLakeService, DataStorageLayer.FEATURE_DATA_LAYER))
                .name("风险评估数据输出");
    }
    
    /**
     * 构建数据质量检查作业
     */
    private void buildDataQualityCheckJob(StreamExecutionEnvironment env, StreamJobConfig jobConfig) {
        // 1. 创建数据源
        DataStream<String> dataStream = env
                .addSource(new FlinkKafkaConsumer<>(
                        "data-quality-input",
                        new SimpleStringSchema(),
                        getKafkaProperties(jobConfig)
                ))
                .name("数据质量检查源");
        
        // 2. 数据解析
        DataStream<DataRecord> parsedData = dataStream
                .map(new DataRecordParser())
                .name("数据记录解析");
        
        // 3. 数据质量检查
        DataStream<DataQualityResult> qualityResults = parsedData
                .map(new DataQualityChecker())
                .name("数据质量检查");
        
        // 4. 质量问题聚合
        DataStream<DataQualitySummary> qualitySummary = qualityResults
                .keyBy(DataQualityResult::getDataSource)
                .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
                .aggregate(new DataQualityAggregator())
                .name("质量问题聚合");
        
        // 5. 质量告警
        DataStream<DataQualityAlert> qualityAlerts = qualitySummary
                .filter(summary -> summary.getErrorRate() > 0.05)
                .map(new DataQualityAlertGenerator())
                .name("数据质量告警");
        
        // 6. 输出质量告警
        qualityAlerts
                .addSink(new FlinkKafkaProducer<>(
                        "data-quality-alerts",
                        new SimpleStringSchema(),
                        getKafkaProperties(jobConfig)
                ))
                .name("质量告警输出");
        
        // 7. 输出质量报告到数据湖
        qualitySummary
                .addSink(new DataLakeSink<>(dataLakeService, DataStorageLayer.AGGREGATED_DATA_LAYER))
                .name("质量报告输出");
    }
    
    /**
     * 停止流处理作业
     */
    @Transactional
    public void stopJob(Long jobId) {
        log.info("停止流处理作业: jobId={}", jobId);
        
        StreamProcessingJob job = streamJobRepository.findById(jobId)
                .orElseThrow(() -> new RuntimeException("流处理作业不存在: " + jobId));
        
        try {
            // 1. 停止Flink作业
            flinkClusterClient.cancelJob(job.getJobName());
            
            // 2. 更新作业状态
            job.setJobStatus(StreamJobStatus.STOPPED);
            job.setStoppedAt(LocalDateTime.now());
            job.setUpdatedAt(LocalDateTime.now());
            
            streamJobRepository.save(job);
            
            log.info("流处理作业停止成功: id={}, name={}", job.getId(), job.getJobName());
            
        } catch (Exception e) {
            log.error("停止流处理作业失败", e);
            
            job.setJobStatus(StreamJobStatus.FAILED);
            job.setUpdatedAt(LocalDateTime.now());
            streamJobRepository.save(job);
            
            throw new RuntimeException("停止流处理作业失败: " + e.getMessage());
        }
    }
    
    /**
     * 获取作业状态
     */
    public StreamJobStatusInfo getJobStatus(Long jobId) {
        StreamProcessingJob job = streamJobRepository.findById(jobId)
                .orElseThrow(() -> new RuntimeException("流处理作业不存在: " + jobId));
        
        StreamJobStatusInfo statusInfo = new StreamJobStatusInfo();
        statusInfo.setJobId(job.getId());
        statusInfo.setJobName(job.getJobName());
        statusInfo.setJobStatus(job.getJobStatus());
        statusInfo.setStartedAt(job.getStartedAt());
        statusInfo.setStoppedAt(job.getStoppedAt());
        
        try {
            // 从Flink集群获取实时状态
            FlinkJobStatus flinkStatus = flinkClusterClient.getJobStatus(job.getJobName());
            statusInfo.setFlinkJobStatus(flinkStatus);
            statusInfo.setProcessedRecords(flinkStatus.getProcessedRecords());
            statusInfo.setThroughput(flinkStatus.getThroughput());
            statusInfo.setLatency(flinkStatus.getLatency());
            
        } catch (Exception e) {
            log.warn("获取Flink作业状态失败: {}", job.getJobName(), e);
            statusInfo.setErrorMessage(e.getMessage());
        }
        
        return statusInfo;
    }
    
    // 辅助方法实现...
    private Properties getKafkaProperties(StreamJobConfig jobConfig) {
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", jobConfig.getKafkaBootstrapServers());
        props.setProperty("group.id", jobConfig.getKafkaGroupId());
        props.setProperty("auto.offset.reset", "latest");
        props.setProperty("enable.auto.commit", "false");
        return props;
    }
}
```

## 大数据处理Demo脚本

### 数据湖部署Demo

```bash
#!/bin/bash
# 数据湖部署脚本

echo "开始部署数据湖环境..."

# 1. 创建Hadoop集群
echo "1. 部署Hadoop集群"
docker network create hadoop-network

# 启动NameNode
docker run -d \
  --name hadoop-namenode \
  --network hadoop-network \
  -p 9870:9870 \
  -p 8020:8020 \
  -e CLUSTER_NAME=hadoop-cluster \
  -v hadoop-namenode:/hadoop/dfs/name \
  bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8

# 启动DataNode
for i in {1..3}; do
  docker run -d \
    --name hadoop-datanode-$i \
    --network hadoop-network \
    -e SERVICE_PRECONDITION="hadoop-namenode:9870" \
    -v hadoop-datanode-$i:/hadoop/dfs/data \
    bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
done

# 2. 部署Hive Metastore
echo "2. 部署Hive Metastore"
docker run -d \
  --name hive-metastore-db \
  --network hadoop-network \
  -e POSTGRES_DB=metastore \
  -e POSTGRES_USER=hive \
  -e POSTGRES_PASSWORD=hive123 \
  -v hive-metastore-db:/var/lib/postgresql/data \
  postgres:11

# 等待数据库启动
sleep 30

# 初始化Hive Metastore
docker run -d \
  --name hive-metastore \
  --network hadoop-network \
  -p 9083:9083 \
  -e SERVICE_PRECONDITION="hive-metastore-db:5432" \
  -e DB_DRIVER=postgres \
  -e DB_URL=jdbc:postgresql://hive-metastore-db:5432/metastore \
  -e DB_USER=hive \
  -e DB_PASSWORD=hive123 \
  apache/hive:3.1.2

# 3. 部署MinIO作为对象存储
echo "3. 部署MinIO对象存储"
docker run -d \
  --name minio \
  --network hadoop-network \
  -p 9000:9000 \
  -p 9001:9001 \
  -e MINIO_ROOT_USER=admin \
  -e MINIO_ROOT_PASSWORD=admin123 \
  -v minio-data:/data \
  minio/minio server /data --console-address ":9001"

# 4. 部署Delta Lake
echo "4. 部署Delta Lake"
docker run -d \
  --name delta-lake \
  --network hadoop-network \
  -p 8888:8888 \
  -e JUPYTER_ENABLE_LAB=yes \
  -v delta-lake-notebooks:/home/jovyan/work \
  jupyter/pyspark-notebook:spark-3.2.0

# 5. 创建数据湖目录结构
echo "5. 创建数据湖目录结构"
docker exec hadoop-namenode hdfs dfs -mkdir -p /data-lake/bronze/sim-card-events
docker exec hadoop-namenode hdfs dfs -mkdir -p /data-lake/silver/sim-card-cleaned
docker exec hadoop-namenode hdfs dfs -mkdir -p /data-lake/gold/sim-card-aggregated
docker exec hadoop-namenode hdfs dfs -mkdir -p /data-lake/platinum/sim-card-features
docker exec hadoop-namenode hdfs dfs -mkdir -p /data-lake/archive/sim-card-historical

# 6. 配置数据湖权限
echo "6. 配置数据湖权限"
docker exec hadoop-namenode hdfs dfs -chmod -R 755 /data-lake
docker exec hadoop-namenode hdfs dfs -chown -R hdfs:hdfs /data-lake

echo "数据湖环境部署完成！"
echo "Hadoop NameNode Web UI: http://localhost:9870"
echo "MinIO Console: http://localhost:9001 (admin/admin123)"
echo "Delta Lake Jupyter: http://localhost:8888"
```

### Flink实时计算Demo

```bash
#!/bin/bash
# Flink实时计算部署脚本

echo "开始部署Flink实时计算环境..."

# 1. 创建Flink网络
docker network create flink-network

# 2. 启动Kafka集群
echo "1. 启动Kafka集群"
# Zookeeper
docker run -d \
  --name zookeeper \
  --network flink-network \
  -p 2181:2181 \
  -e ZOOKEEPER_CLIENT_PORT=2181 \
  -e ZOOKEEPER_TICK_TIME=2000 \
  confluentinc/cp-zookeeper:7.0.1

# Kafka Broker
docker run -d \
  --name kafka \
  --network flink-network \
  -p 9092:9092 \
  -e KAFKA_BROKER_ID=1 \
  -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \
  -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
  confluentinc/cp-kafka:7.0.1

# 等待Kafka启动
sleep 30

# 3. 创建Kafka主题
echo "2. 创建Kafka主题"
docker exec kafka kafka-topics --create \
  --topic sim-card-usage-events \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1

docker exec kafka kafka-topics --create \
  --topic sim-card-usage-alerts \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1

docker exec kafka kafka-topics --create \
  --topic system-metrics \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1

docker exec kafka kafka-topics --create \
  --topic monitoring-alerts \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1

# 4. 启动Flink集群
echo "3. 启动Flink集群"
# Flink JobManager
docker run -d \
  --name flink-jobmanager \
  --network flink-network \
  -p 8081:8081 \
  -e JOB_MANAGER_RPC_ADDRESS=flink-jobmanager \
  -v flink-checkpoints:/tmp/flink-checkpoints \
  flink:1.15.2-scala_2.12-java11 jobmanager

# Flink TaskManager
for i in {1..2}; do
  docker run -d \
    --name flink-taskmanager-$i \
    --network flink-network \
    -e JOB_MANAGER_RPC_ADDRESS=flink-jobmanager \
    -v flink-checkpoints:/tmp/flink-checkpoints \
    flink:1.15.2-scala_2.12-java11 taskmanager
done

# 5. 等待Flink集群启动
echo "4. 等待Flink集群启动"
sleep 60

# 6. 部署流处理作业
echo "5. 部署流处理作业"

# 创建SIM卡使用分析作业配置
cat > sim-card-usage-job.json << EOF
{
  "jobName": "sim-card-usage-analysis",
  "jobDescription": "SIM卡使用情况实时分析",
  "jobType": "sim_card_usage_analysis",
  "parallelism": 4,
  "checkpointInterval": 60000,
  "restartStrategy": "fixed-delay",
  "kafkaBootstrapServers": "kafka:9092",
  "kafkaGroupId": "sim-card-usage-group",
  "inputSources": [
    {
      "type": "kafka",
      "topic": "sim-card-usage-events",
      "format": "json"
    }
  ],
  "outputTargets": [
    {
      "type": "kafka",
      "topic": "sim-card-usage-alerts",
      "format": "json"
    },
    {
      "type": "data-lake",
      "layer": "silver",
      "format": "parquet"
    }
  ],
  "resourceConfig": {
    "taskManagerMemory": "2g",
    "taskManagerCpu": 2
  }
}
EOF

# 创建实时监控作业配置
cat > real-time-monitoring-job.json << EOF
{
  "jobName": "real-time-monitoring",
  "jobDescription": "系统实时监控",
  "jobType": "real_time_monitoring",
  "parallelism": 2,
  "checkpointInterval": 30000,
  "restartStrategy": "fixed-delay",
  "kafkaBootstrapServers": "kafka:9092",
  "kafkaGroupId": "monitoring-group",
  "inputSources": [
    {
      "type": "kafka",
      "topic": "system-metrics",
      "format": "json"
    }
  ],
  "outputTargets": [
    {
      "type": "kafka",
      "topic": "monitoring-alerts",
      "format": "json"
    }
  ],
  "resourceConfig": {
    "taskManagerMemory": "1g",
    "taskManagerCpu": 1
  }
}
EOF

# 7. 生成测试数据
echo "6. 生成测试数据"

# SIM卡使用事件生成器
cat > generate-sim-events.py << 'EOF'
import json
import random
import time
from kafka import KafkaProducer
from datetime import datetime

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

def generate_sim_event():
    return {
        "imsi": f"46000{random.randint(100000000, 999999999)}",
        "msisdn": f"1{random.randint(3000000000, 9999999999)}",
        "eventType": random.choice(["CALL", "SMS", "DATA"]),
        "dataUsage": random.randint(1, 2000),  # MB
        "callDuration": random.randint(10, 7200),  # seconds
        "location": {
            "latitude": random.uniform(39.0, 41.0),
            "longitude": random.uniform(115.0, 117.0)
        },
        "timestamp": datetime.now().isoformat(),
        "cellId": f"CELL_{random.randint(1000, 9999)}",
        "networkType": random.choice(["4G", "5G"]),
        "roaming": random.choice([True, False])
    }

print("开始生成SIM卡使用事件...")
while True:
    event = generate_sim_event()
    producer.send('sim-card-usage-events', event)
    print(f"发送事件: {event['imsi']} - {event['eventType']}")
    time.sleep(random.uniform(0.1, 1.0))
EOF

# 系统指标生成器
cat > generate-system-metrics.py << 'EOF'
import json
import random
import time
from kafka import KafkaProducer
from datetime import datetime

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

def generate_system_metric():
    return {
        "serviceName": random.choice(["sim-card-service", "user-service", "billing-service"]),
        "metricType": "SYSTEM",
        "cpuUsage": random.uniform(10, 95),
        "memoryUsage": random.uniform(20, 90),
        "diskUsage": random.uniform(30, 80),
        "networkIn": random.randint(1000, 100000),  # KB/s
        "networkOut": random.randint(1000, 100000),  # KB/s
        "responseTime": random.uniform(10, 500),  # ms
        "errorRate": random.uniform(0, 0.1),
        "timestamp": datetime.now().isoformat(),
        "hostname": f"server-{random.randint(1, 10)}"
    }

print("开始生成系统指标...")
while True:
    metric = generate_system_metric()
    producer.send('system-metrics', metric)
    print(f"发送指标: {metric['serviceName']} - CPU: {metric['cpuUsage']:.1f}%")
    time.sleep(random.uniform(1, 5))
EOF

# 8. 启动数据生成器
echo "7. 启动数据生成器"
pip install kafka-python

# 后台运行数据生成器
python generate-sim-events.py &
SIM_EVENTS_PID=$!

python generate-system-metrics.py &
SYSTEM_METRICS_PID=$!

echo "数据生成器已启动"
echo "SIM事件生成器PID: $SIM_EVENTS_PID"
echo "系统指标生成器PID: $SYSTEM_METRICS_PID"

# 9. 验证流处理
echo "8. 验证流处理"
sleep 30

echo "检查Kafka主题消息..."
docker exec kafka kafka-console-consumer \
  --topic sim-card-usage-events \
  --bootstrap-server localhost:9092 \
  --from-beginning \
  --max-messages 5

docker exec kafka kafka-console-consumer \
  --topic system-metrics \
  --bootstrap-server localhost:9092 \
  --from-beginning \
  --max-messages 5

echo "Flink实时计算环境部署完成！"
echo "Flink Web UI: http://localhost:8081"
echo "Kafka集群: localhost:9092"
echo ""
echo "停止数据生成器命令:"
echo "kill $SIM_EVENTS_PID $SYSTEM_METRICS_PID"
```

### Spark批处理Demo

```bash
#!/bin/bash
# Spark批处理部署脚本

echo "开始部署Spark批处理环境..."

# 1. 创建Spark网络
docker network create spark-network

# 2. 启动Spark Master
echo "1. 启动Spark Master"
docker run -d \
  --name spark-master \
  --network spark-network \
  -p 8080:8080 \
  -p 7077:7077 \
  -e SPARK_MODE=master \
  -e SPARK_RPC_AUTHENTICATION_ENABLED=no \
  -e SPARK_RPC_ENCRYPTION_ENABLED=no \
  -e SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no \
  -e SPARK_SSL_ENABLED=no \
  -v spark-data:/opt/bitnami/spark/data \
  bitnami/spark:3.3.0

# 3. 启动Spark Workers
echo "2. 启动Spark Workers"
for i in {1..2}; do
  docker run -d \
    --name spark-worker-$i \
    --network spark-network \
    -e SPARK_MODE=worker \
    -e SPARK_MASTER_URL=spark://spark-master:7077 \
    -e SPARK_WORKER_MEMORY=2G \
    -e SPARK_WORKER_CORES=2 \
    -e SPARK_RPC_AUTHENTICATION_ENABLED=no \
    -e SPARK_RPC_ENCRYPTION_ENABLED=no \
    -e SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no \
    -e SPARK_SSL_ENABLED=no \
    -v spark-data:/opt/bitnami/spark/data \
    bitnami/spark:3.3.0
done

# 4. 启动Jupyter Notebook
echo "3. 启动Jupyter Notebook"
docker run -d \
  --name spark-jupyter \
  --network spark-network \
  -p 8888:8888 \
  -e JUPYTER_ENABLE_LAB=yes \
  -e SPARK_MASTER=spark://spark-master:7077 \
  -v spark-notebooks:/home/jovyan/work \
  -v spark-data:/home/jovyan/data \
  jupyter/pyspark-notebook:spark-3.3.0

# 5. 等待服务启动
echo "4. 等待服务启动"
sleep 60

# 6. 创建批处理作业
echo "5. 创建批处理作业"

# SIM卡数据分析作业
cat > sim-card-batch-analysis.py << 'EOF'
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import json

# 创建Spark会话
spark = SparkSession.builder \
    .appName("SIM Card Batch Analysis") \
    .master("spark://spark-master:7077") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

# 定义数据模式
sim_usage_schema = StructType([
    StructField("imsi", StringType(), True),
    StructField("msisdn", StringType(), True),
    StructField("eventType", StringType(), True),
    StructField("dataUsage", IntegerType(), True),
    StructField("callDuration", IntegerType(), True),
    StructField("timestamp", TimestampType(), True),
    StructField("cellId", StringType(), True),
    StructField("networkType", StringType(), True),
    StructField("roaming", BooleanType(), True)
])

# 读取数据
print("读取SIM卡使用数据...")
df = spark.read \
    .schema(sim_usage_schema) \
    .option("multiline", "true") \
    .json("/home/jovyan/data/sim-usage/*.json")

print(f"总记录数: {df.count()}")

# 数据清洗
print("执行数据清洗...")
cleaned_df = df.filter(
    (col("imsi").isNotNull()) &
    (col("msisdn").isNotNull()) &
    (col("dataUsage") >= 0) &
    (col("callDuration") >= 0)
)

print(f"清洗后记录数: {cleaned_df.count()}")

# 按用户聚合分析
print("执行用户使用分析...")
user_stats = cleaned_df.groupBy("imsi", "msisdn") \
    .agg(
        sum("dataUsage").alias("totalDataUsage"),
        sum("callDuration").alias("totalCallDuration"),
        count("*").alias("eventCount"),
        countDistinct("cellId").alias("uniqueCells"),
        max("timestamp").alias("lastActivity")
    )

print("用户统计示例:")
user_stats.show(10)

# 按网络类型分析
print("执行网络类型分析...")
network_stats = cleaned_df.groupBy("networkType") \
    .agg(
        count("*").alias("eventCount"),
        avg("dataUsage").alias("avgDataUsage"),
        avg("callDuration").alias("avgCallDuration")
    )

print("网络类型统计:")
network_stats.show()

# 按时间段分析
print("执行时间段分析...")
hourly_stats = cleaned_df \
    .withColumn("hour", hour("timestamp")) \
    .groupBy("hour") \
    .agg(
        count("*").alias("eventCount"),
        sum("dataUsage").alias("totalDataUsage")
    ) \
    .orderBy("hour")

print("小时统计:")
hourly_stats.show(24)

# 异常检测
print("执行异常检测...")
high_usage_users = user_stats.filter(
    (col("totalDataUsage") > 10000) |
    (col("totalCallDuration") > 36000)
)

print(f"高使用量用户数: {high_usage_users.count()}")
high_usage_users.show()

# 保存结果
print("保存分析结果...")
user_stats.coalesce(1).write \
    .mode("overwrite") \
    .option("header", "true") \
    .csv("/home/jovyan/data/output/user-stats")

network_stats.coalesce(1).write \
    .mode("overwrite") \
    .option("header", "true") \
    .csv("/home/jovyan/data/output/network-stats")

hourly_stats.coalesce(1).write \
    .mode("overwrite") \
    .option("header", "true") \
    .csv("/home/jovyan/data/output/hourly-stats")

high_usage_users.coalesce(1).write \
    .mode("overwrite") \
    .option("header", "true") \
    .csv("/home/jovyan/data/output/high-usage-users")

print("批处理分析完成！")
spark.stop()
EOF

# 7. 生成测试数据
echo "6. 生成测试数据"

# 创建测试数据生成脚本
cat > generate-batch-data.py << 'EOF'
import json
import random
import os
from datetime import datetime, timedelta

# 创建输出目录
os.makedirs('/tmp/sim-usage', exist_ok=True)

def generate_sim_events(count=10000):
    events = []
    base_time = datetime.now() - timedelta(days=7)
    
    for i in range(count):
        event = {
            "imsi": f"46000{random.randint(100000000, 999999999)}",
            "msisdn": f"1{random.randint(3000000000, 9999999999)}",
            "eventType": random.choice(["CALL", "SMS", "DATA"]),
            "dataUsage": random.randint(1, 2000),
            "callDuration": random.randint(10, 7200),
            "timestamp": (base_time + timedelta(
                minutes=random.randint(0, 10080)
            )).isoformat(),
            "cellId": f"CELL_{random.randint(1000, 9999)}",
            "networkType": random.choice(["4G", "5G"]),
            "roaming": random.choice([True, False])
        }
        events.append(event)
    
    return events

print("生成批处理测试数据...")
events = generate_sim_events(50000)

# 按天分割数据
days_data = {}
for event in events:
    day = event['timestamp'][:10]
    if day not in days_data:
        days_data[day] = []
    days_data[day].append(event)

# 保存数据文件
for day, day_events in days_data.items():
    filename = f'/tmp/sim-usage/sim-usage-{day}.json'
    with open(filename, 'w') as f:
        for event in day_events:
            f.write(json.dumps(event) + '\n')
    print(f"保存 {len(day_events)} 条记录到 {filename}")

print(f"测试数据生成完成，总计 {len(events)} 条记录")
EOF

# 运行数据生成
python generate-batch-data.py

# 复制数据到Spark容器
docker cp /tmp/sim-usage spark-jupyter:/home/jovyan/data/

# 8. 运行批处理作业
echo "7. 运行批处理作业"
docker cp sim-card-batch-analysis.py spark-jupyter:/home/jovyan/work/

# 提交Spark作业
docker exec spark-jupyter spark-submit \
  --master spark://spark-master:7077 \
  --executor-memory 1g \
  --executor-cores 1 \
  --num-executors 2 \
  /home/jovyan/work/sim-card-batch-analysis.py

echo "Spark批处理环境部署完成！"
echo "Spark Master Web UI: http://localhost:8080"
echo "Jupyter Notebook: http://localhost:8888"
echo "批处理作业已提交执行"
```

## 最佳实践总结

### 数据湖架构最佳实践

1. **分层存储策略**
   - Bronze层：保存原始数据，确保数据完整性
   - Silver层：清洗和标准化数据，提高数据质量
   - Gold层：业务聚合数据，支持快速查询
   - Platinum层：特征工程数据，支持机器学习

2. **数据格式选择**
   - 使用列式存储格式（Parquet、Delta）提高查询性能
   - 采用压缩算法减少存储成本
   - 合理设计分区策略提高查询效率

3. **元数据管理**
   - 建立统一的元数据目录
   - 实施数据血缘追踪
   - 定义数据质量规则和监控

### 实时计算最佳实践

1. **流处理架构**
   - 选择合适的流处理引擎（Flink、Kafka Streams）
   - 设计容错和状态管理机制
   - 实施背压控制和资源管理

2. **数据处理策略**
   - 实现精确一次语义保证
   - 设计合理的窗口和触发器
   - 优化序列化和网络传输

3. **监控和运维**
   - 建立全面的监控体系
   - 实施自动化运维和告警
   - 定期进行性能调优

### 批处理优化实践

1. **资源配置**
   - 合理配置Executor内存和CPU
   - 优化分区数量和大小
   - 使用动态资源分配

2. **查询优化**
   - 启用自适应查询执行
   - 使用列式存储和向量化执行
   - 实施查询缓存和结果复用

3. **数据倾斜处理**
   - 识别和处理数据倾斜
   - 使用随机前缀和二次聚合
   - 实施动态分区裁剪

### 数据治理实践

1. **数据质量管理**
   - 建立数据质量评估体系
   - 实施数据清洗和标准化
   - 定期进行数据质量监控

2. **数据安全和隐私**
   - 实施数据加密和访问控制
   - 建立数据脱敏和匿名化机制
   - 遵循数据保护法规要求

3. **数据生命周期管理**
   - 定义数据保留策略
   - 实施自动化归档和清理
   - 建立数据恢复和备份机制
```