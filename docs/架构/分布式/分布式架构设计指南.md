# 分布式架构设计指南

基于NSRS号卡资源管理系统的分布式架构实施方案

## 目录

- [分布式架构概述](#分布式架构概述)
- [服务注册与发现](#服务注册与发现)
- [配置中心](#配置中心)
- [分布式事务](#分布式事务)
- [分布式锁](#分布式锁)
- [分布式ID生成](#分布式id生成)
- [分布式调度](#分布式调度)
- [服务网格](#服务网格)
- [部署架构](#部署架构)
- [监控与运维](#监控与运维)
- [最佳实践](#最佳实践)

## 分布式架构概述

### 架构目标

```
分布式架构目标：
┌─────────────────────────────────────────────────────────────┐
│                    分布式架构特性                              │
├─────────────────────────────────────────────────────────────┤
│ • 高可用性：服务故障自动恢复，系统整体可用性99.9%+              │
│ • 可扩展性：支持水平扩展，按需增减服务实例                      │
│ • 弹性伸缩：根据负载自动调整资源                               │
│ • 故障隔离：单个服务故障不影响整体系统                         │
│ • 数据一致性：分布式事务保证数据强一致性                       │
│ • 配置统一：集中化配置管理，动态配置更新                       │
└─────────────────────────────────────────────────────────────┘
```

### 技术架构图

```
NSRS分布式架构：

┌─────────────────────────────────────────────────────────────┐
│                      负载均衡层                              │
│                   (Nginx/HAProxy)                          │
└─────────────────────┬───────────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────────┐
│                   API网关层                                 │
│              (Spring Cloud Gateway)                        │
└─────────────────────┬───────────────────────────────────────┘
                      │
        ┌─────────────┼─────────────┐
        │             │             │
┌───────▼──────┐ ┌───▼────┐ ┌──────▼──────┐
│ User Service │ │IMSI Svc│ │MSISDN Svc   │
└──────┬───────┘ └───┬────┘ └──────┬──────┘
       │             │             │
┌──────▼──────┐ ┌───▼────┐ ┌──────▼──────┐
│ SIM Service │ │Bill Svc│ │Report Svc   │
└─────────────┘ └────────┘ └─────────────┘
       │             │             │
┌──────▼─────────────▼─────────────▼──────┐
│              服务治理层                  │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐    │
│  │  Nacos  │ │  Seata  │ │Sentinel │    │
│  │注册发现  │ │分布式事务│ │ 熔断限流 │    │
│  └─────────┘ └─────────┘ └─────────┘    │
└─────────────────────────────────────────┘
       │             │             │
┌──────▼─────────────▼─────────────▼──────┐
│              基础设施层                  │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐    │
│  │  MySQL  │ │  Redis  │ │RocketMQ │    │
│  │  集群   │ │  集群   │ │  集群   │    │
│  └─────────┘ └─────────┘ └─────────┘    │
└─────────────────────────────────────────┘
```

## 服务注册与发现

### 1. Nacos服务注册发现

#### Nacos集群部署

```yaml
# docker-compose-nacos-cluster.yml
version: '3.8'

services:
  # Nacos节点1
  nacos1:
    image: nacos/nacos-server:v2.2.0
    container_name: nacos1
    environment:
      - PREFER_HOST_MODE=hostname
      - MODE=cluster
      - NACOS_APPLICATION_PORT=8848
      - NACOS_SERVERS=nacos1:8848 nacos2:8848 nacos3:8848
      - SPRING_DATASOURCE_PLATFORM=mysql
      - MYSQL_SERVICE_HOST=mysql
      - MYSQL_SERVICE_DB_NAME=nacos
      - MYSQL_SERVICE_USER=nacos
      - MYSQL_SERVICE_PASSWORD=nacos123
      - MYSQL_SERVICE_DB_PARAM=characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useSSL=false&allowPublicKeyRetrieval=true
    ports:
      - "8848:8848"
      - "9848:9848"
    networks:
      - nacos-network
    depends_on:
      - mysql

  # Nacos节点2
  nacos2:
    image: nacos/nacos-server:v2.2.0
    container_name: nacos2
    environment:
      - PREFER_HOST_MODE=hostname
      - MODE=cluster
      - NACOS_APPLICATION_PORT=8848
      - NACOS_SERVERS=nacos1:8848 nacos2:8848 nacos3:8848
      - SPRING_DATASOURCE_PLATFORM=mysql
      - MYSQL_SERVICE_HOST=mysql
      - MYSQL_SERVICE_DB_NAME=nacos
      - MYSQL_SERVICE_USER=nacos
      - MYSQL_SERVICE_PASSWORD=nacos123
      - MYSQL_SERVICE_DB_PARAM=characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useSSL=false&allowPublicKeyRetrieval=true
    ports:
      - "8849:8848"
      - "9849:9848"
    networks:
      - nacos-network
    depends_on:
      - mysql

  # Nacos节点3
  nacos3:
    image: nacos/nacos-server:v2.2.0
    container_name: nacos3
    environment:
      - PREFER_HOST_MODE=hostname
      - MODE=cluster
      - NACOS_APPLICATION_PORT=8848
      - NACOS_SERVERS=nacos1:8848 nacos2:8848 nacos3:8848
      - SPRING_DATASOURCE_PLATFORM=mysql
      - MYSQL_SERVICE_HOST=mysql
      - MYSQL_SERVICE_DB_NAME=nacos
      - MYSQL_SERVICE_USER=nacos
      - MYSQL_SERVICE_PASSWORD=nacos123
      - MYSQL_SERVICE_DB_PARAM=characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useSSL=false&allowPublicKeyRetrieval=true
    ports:
      - "8850:8848"
      - "9850:9848"
    networks:
      - nacos-network
    depends_on:
      - mysql

  # MySQL for Nacos
  mysql:
    image: mysql:8.0
    container_name: nacos-mysql
    environment:
      - MYSQL_ROOT_PASSWORD=root123
      - MYSQL_DATABASE=nacos
      - MYSQL_USER=nacos
      - MYSQL_PASSWORD=nacos123
    ports:
      - "3306:3306"
    volumes:
      - ./mysql/data:/var/lib/mysql
      - ./mysql/init:/docker-entrypoint-initdb.d
    networks:
      - nacos-network

  # Nginx负载均衡
  nginx:
    image: nginx:alpine
    container_name: nacos-nginx
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - nacos1
      - nacos2
      - nacos3
    networks:
      - nacos-network

networks:
  nacos-network:
    driver: bridge
```

#### Nginx负载均衡配置

```nginx
# nginx/nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream nacos-cluster {
        server nacos1:8848 weight=1 max_fails=2 fail_timeout=10s;
        server nacos2:8848 weight=1 max_fails=2 fail_timeout=10s;
        server nacos3:8848 weight=1 max_fails=2 fail_timeout=10s;
    }
    
    server {
        listen 80;
        server_name localhost;
        
        location / {
            proxy_pass http://nacos-cluster;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_connect_timeout 10s;
            proxy_send_timeout 10s;
            proxy_read_timeout 10s;
        }
    }
}
```

#### 服务注册配置

```yaml
# application.yml - IMSI服务
spring:
  application:
    name: nsrs-imsi-service
  cloud:
    nacos:
      discovery:
        server-addr: nacos-nginx:80
        namespace: nsrs-prod
        group: NSRS_GROUP
        cluster-name: beijing
        metadata:
          version: 1.0.0
          region: beijing
          zone: zone-a
          weight: 100
        heart-beat-interval: 5000
        heart-beat-timeout: 15000
        ip-delete-timeout: 30000
        instance-enabled: true
        ephemeral: true
        secure: false
        access-key: nacos
        secret-key: nacos123
      config:
        server-addr: nacos-nginx:80
        namespace: nsrs-prod
        group: NSRS_GROUP
        file-extension: yml
        timeout: 3000
        max-retry: 3
        config-retry-time: 2000
        config-long-poll-timeout: 46000
        enable-remote-sync-config: true
```

#### 服务发现客户端

```java
@RestController
@RequestMapping("/api/v1/discovery")
public class ServiceDiscoveryController {
    
    @Autowired
    private DiscoveryClient discoveryClient;
    
    @Autowired
    private NacosDiscoveryProperties nacosDiscoveryProperties;
    
    /**
     * 获取所有服务列表
     */
    @GetMapping("/services")
    public Result<List<String>> getServices() {
        List<String> services = discoveryClient.getServices();
        return Result.success(services);
    }
    
    /**
     * 获取指定服务的实例列表
     */
    @GetMapping("/instances/{serviceId}")
    public Result<List<ServiceInstance>> getInstances(@PathVariable String serviceId) {
        List<ServiceInstance> instances = discoveryClient.getInstances(serviceId);
        return Result.success(instances);
    }
    
    /**
     * 获取健康的服务实例
     */
    @GetMapping("/healthy-instances/{serviceId}")
    public Result<List<ServiceInstance>> getHealthyInstances(@PathVariable String serviceId) {
        List<ServiceInstance> instances = discoveryClient.getInstances(serviceId)
            .stream()
            .filter(instance -> {
                // 检查实例健康状态
                Map<String, String> metadata = instance.getMetadata();
                return "UP".equals(metadata.get("health.status"));
            })
            .collect(Collectors.toList());
        return Result.success(instances);
    }
}

/**
 * 服务实例健康检查
 */
@Component
public class ServiceHealthIndicator implements HealthIndicator {
    
    @Autowired
    private DataSource dataSource;
    
    @Autowired
    private RedisTemplate<String, Object> redisTemplate;
    
    @Override
    public Health health() {
        try {
            // 检查数据库连接
            checkDatabase();
            
            // 检查Redis连接
            checkRedis();
            
            return Health.up()
                .withDetail("database", "UP")
                .withDetail("redis", "UP")
                .withDetail("timestamp", System.currentTimeMillis())
                .build();
                
        } catch (Exception e) {
            return Health.down()
                .withDetail("error", e.getMessage())
                .withDetail("timestamp", System.currentTimeMillis())
                .build();
        }
    }
    
    private void checkDatabase() throws SQLException {
        try (Connection connection = dataSource.getConnection()) {
            connection.prepareStatement("SELECT 1").executeQuery();
        }
    }
    
    private void checkRedis() {
        redisTemplate.opsForValue().get("health:check");
    }
}
```

### 2. Eureka服务注册发现（备选方案）

#### Eureka Server集群配置

```yaml
# eureka-server1.yml
spring:
  application:
    name: eureka-server
  profiles:
    active: peer1

server:
  port: 8761

eureka:
  instance:
    hostname: eureka-server1
    prefer-ip-address: false
  client:
    register-with-eureka: true
    fetch-registry: true
    service-url:
      defaultZone: http://eureka-server2:8762/eureka/,http://eureka-server3:8763/eureka/
  server:
    enable-self-preservation: true
    eviction-interval-timer-in-ms: 10000
    renewal-percent-threshold: 0.85
```

#### Eureka Client配置

```yaml
# IMSI服务Eureka配置
eureka:
  client:
    service-url:
      defaultZone: http://eureka-server1:8761/eureka/,http://eureka-server2:8762/eureka/,http://eureka-server3:8763/eureka/
    registry-fetch-interval-seconds: 30
    instance-info-replication-interval-seconds: 30
  instance:
    prefer-ip-address: true
    lease-renewal-interval-in-seconds: 30
    lease-expiration-duration-in-seconds: 90
    metadata-map:
      version: 1.0.0
      region: beijing
      zone: zone-a
```

## 配置中心

### 1. Nacos配置中心

#### 配置分层管理

```yaml
# 公共配置 (common-config.yml)
# Data ID: common-config.yml
# Group: NSRS_GROUP
# Namespace: nsrs-prod

# 数据库配置
spring:
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    hikari:
      minimum-idle: 5
      maximum-pool-size: 20
      auto-commit: true
      idle-timeout: 30000
      pool-name: HikariCP
      max-lifetime: 1800000
      connection-timeout: 30000
      connection-test-query: SELECT 1

# Redis配置
  redis:
    timeout: 3000
    lettuce:
      pool:
        max-active: 20
        max-wait: -1
        max-idle: 8
        min-idle: 0

# MyBatis配置
mybatis-plus:
  configuration:
    map-underscore-to-camel-case: true
    cache-enabled: false
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl
  global-config:
    db-config:
      id-type: auto
      logic-delete-field: deleted
      logic-delete-value: 1
      logic-not-delete-value: 0

# 日志配置
logging:
  level:
    com.nsrs: DEBUG
    org.springframework.cloud: DEBUG
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{50} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{50} - %msg%n"
```

```yaml
# IMSI服务专用配置 (nsrs-imsi-service.yml)
# Data ID: nsrs-imsi-service.yml
# Group: NSRS_GROUP
# Namespace: nsrs-prod

spring:
  datasource:
    url: jdbc:mysql://mysql-cluster:3306/nsrs_imsi?useUnicode=true&characterEncoding=utf8&useSSL=false&serverTimezone=Asia/Shanghai
    username: nsrs_imsi
    password: ${mysql.password}
  
  redis:
    host: redis-cluster
    port: 6379
    password: ${redis.password}
    database: 1

# 业务配置
nsrs:
  imsi:
    # IMSI生成配置
    generate:
      batch-size: 1000
      max-retry: 3
      timeout: 30000
    # IMSI分配配置
    allocate:
      pool-size: 10000
      threshold: 1000
      auto-replenish: true
    # 缓存配置
    cache:
      ttl: 3600
      max-size: 10000

# RocketMQ配置
rocketmq:
  name-server: rocketmq-cluster:9876
  producer:
    group: imsi-producer-group
    send-message-timeout: 3000
    retry-times-when-send-failed: 2
    retry-times-when-send-async-failed: 2
    max-message-size: 4194304
    compress-message-body-threshold: 4096
```

#### 动态配置更新

```java
@Component
@RefreshScope
public class ImsiConfigProperties {
    
    @Value("${nsrs.imsi.generate.batch-size:1000}")
    private Integer batchSize;
    
    @Value("${nsrs.imsi.generate.max-retry:3}")
    private Integer maxRetry;
    
    @Value("${nsrs.imsi.allocate.pool-size:10000}")
    private Integer poolSize;
    
    @Value("${nsrs.imsi.cache.ttl:3600}")
    private Integer cacheTtl;
    
    // getters and setters
}

@Service
public class ImsiResourceServiceImpl {
    
    @Autowired
    private ImsiConfigProperties configProperties;
    
    public List<ImsiResourceDTO> generateImsi(ImsiGenerateRequest request) {
        // 使用动态配置
        int batchSize = configProperties.getBatchSize();
        int maxRetry = configProperties.getMaxRetry();
        
        log.info("Generate IMSI with batchSize: {}, maxRetry: {}", batchSize, maxRetry);
        
        // 业务逻辑
        return doGenerateImsi(request, batchSize, maxRetry);
    }
}

/**
 * 配置变更监听器
 */
@Component
public class ConfigChangeListener {
    
    @NacosConfigListener(dataId = "nsrs-imsi-service.yml", groupId = "NSRS_GROUP")
    public void onConfigChange(String configInfo) {
        log.info("IMSI service config changed: {}", configInfo);
        
        // 可以在这里处理配置变更后的逻辑
        // 比如刷新缓存、重新初始化组件等
        refreshCache();
    }
    
    private void refreshCache() {
        // 刷新缓存逻辑
        log.info("Refreshing cache due to config change");
    }
}
```

#### 配置加密

```java
@Component
public class ConfigEncryptionProcessor {
    
    private static final String ENCRYPT_PREFIX = "ENC(";
    private static final String ENCRYPT_SUFFIX = ")";
    
    @Autowired
    private AESUtil aesUtil;
    
    @EventListener
    public void handleConfigChange(NacosConfigReceivedEvent event) {
        String configInfo = event.getConfigInfo();
        String decryptedConfig = decryptConfig(configInfo);
        // 处理解密后的配置
    }
    
    private String decryptConfig(String configInfo) {
        if (configInfo.contains(ENCRYPT_PREFIX)) {
            // 提取加密内容
            int startIndex = configInfo.indexOf(ENCRYPT_PREFIX) + ENCRYPT_PREFIX.length();
            int endIndex = configInfo.indexOf(ENCRYPT_SUFFIX, startIndex);
            String encryptedValue = configInfo.substring(startIndex, endIndex);
            
            // 解密
            String decryptedValue = aesUtil.decrypt(encryptedValue);
            
            // 替换配置
            return configInfo.replace(ENCRYPT_PREFIX + encryptedValue + ENCRYPT_SUFFIX, decryptedValue);
        }
        return configInfo;
    }
}

@Component
public class AESUtil {
    
    private static final String ALGORITHM = "AES";
    private static final String TRANSFORMATION = "AES/ECB/PKCS5Padding";
    
    @Value("${config.encrypt.key}")
    private String encryptKey;
    
    public String encrypt(String plainText) {
        try {
            SecretKeySpec secretKey = new SecretKeySpec(encryptKey.getBytes(), ALGORITHM);
            Cipher cipher = Cipher.getInstance(TRANSFORMATION);
            cipher.init(Cipher.ENCRYPT_MODE, secretKey);
            byte[] encryptedBytes = cipher.doFinal(plainText.getBytes());
            return Base64.getEncoder().encodeToString(encryptedBytes);
        } catch (Exception e) {
            throw new RuntimeException("Failed to encrypt config", e);
        }
    }
    
    public String decrypt(String encryptedText) {
        try {
            SecretKeySpec secretKey = new SecretKeySpec(encryptKey.getBytes(), ALGORITHM);
            Cipher cipher = Cipher.getInstance(TRANSFORMATION);
            cipher.init(Cipher.DECRYPT_MODE, secretKey);
            byte[] decryptedBytes = cipher.doFinal(Base64.getDecoder().decode(encryptedText));
            return new String(decryptedBytes);
        } catch (Exception e) {
            throw new RuntimeException("Failed to decrypt config", e);
        }
    }
}
```

### 2. Apollo配置中心（备选方案）

#### Apollo配置

```yaml
# application.yml
apollo:
  meta: http://apollo-config-service:8080
  bootstrap:
    enabled: true
    namespaces: application,nsrs.imsi,nsrs.common
  cacheDir: /opt/data/apollo-cache
  cluster: beijing
  autoUpdateInjectedSpringProperties: true
```

## 分布式事务

### 1. Seata分布式事务

#### Seata Server集群部署

```yaml
# docker-compose-seata-cluster.yml
version: '3.8'

services:
  # Seata Server 1
  seata-server1:
    image: seataio/seata-server:1.6.1
    container_name: seata-server1
    ports:
      - "8091:8091"
    environment:
      - SEATA_PORT=8091
      - STORE_MODE=db
      - SEATA_CONFIG_NAME=file:/root/seata-config/registry
    volumes:
      - ./seata/config:/root/seata-config
      - ./seata/logs:/root/logs
    networks:
      - seata-network
    depends_on:
      - mysql
      - nacos1

  # Seata Server 2
  seata-server2:
    image: seataio/seata-server:1.6.1
    container_name: seata-server2
    ports:
      - "8092:8091"
    environment:
      - SEATA_PORT=8091
      - STORE_MODE=db
      - SEATA_CONFIG_NAME=file:/root/seata-config/registry
    volumes:
      - ./seata/config:/root/seata-config
      - ./seata/logs:/root/logs
    networks:
      - seata-network
    depends_on:
      - mysql
      - nacos1

  # Seata Server 3
  seata-server3:
    image: seataio/seata-server:1.6.1
    container_name: seata-server3
    ports:
      - "8093:8091"
    environment:
      - SEATA_PORT=8091
      - STORE_MODE=db
      - SEATA_CONFIG_NAME=file:/root/seata-config/registry
    volumes:
      - ./seata/config:/root/seata-config
      - ./seata/logs:/root/logs
    networks:
      - seata-network
    depends_on:
      - mysql
      - nacos1

networks:
  seata-network:
    external: true
```

#### Seata配置文件

```properties
# seata/config/registry.conf
registry {
  type = "nacos"
  nacos {
    application = "seata-server"
    serverAddr = "nacos1:8848,nacos2:8848,nacos3:8848"
    group = "SEATA_GROUP"
    namespace = "seata"
    cluster = "default"
    username = "nacos"
    password = "nacos123"
  }
}

config {
  type = "nacos"
  nacos {
    serverAddr = "nacos1:8848,nacos2:8848,nacos3:8848"
    group = "SEATA_GROUP"
    namespace = "seata"
    dataId = "seataServer.properties"
    username = "nacos"
    password = "nacos123"
  }
}
```

```properties
# Nacos中的seataServer.properties配置
# 存储模式
store.mode=db
store.lock.mode=db
store.session.mode=db

# 数据库配置
store.db.datasource=druid
store.db.dbType=mysql
store.db.driverClassName=com.mysql.cj.jdbc.Driver
store.db.url=jdbc:mysql://mysql:3306/seata?useUnicode=true&characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useSSL=false&allowPublicKeyRetrieval=true
store.db.user=seata
store.db.password=seata123
store.db.minConn=5
store.db.maxConn=30
store.db.globalTable=global_table
store.db.branchTable=branch_table
store.db.lockTable=lock_table
store.db.queryLimit=100
store.db.maxWait=5000

# 事务日志存储模式
server.recovery.committingRetryPeriod=1000
server.recovery.asynCommittingRetryPeriod=1000
server.recovery.rollbackingRetryPeriod=1000
server.recovery.timeoutRetryPeriod=1000
server.maxCommitRetryTimeout=-1
server.maxRollbackRetryTimeout=-1
server.rollbackRetryTimeoutUnlockEnable=false
server.distributedLockExpireTime=10000

# 客户端配置
client.rm.asyncCommitBufferLimit=10000
client.rm.lock.retryInterval=10
client.rm.lock.retryTimes=30
client.rm.lock.retryPolicyBranchRollbackOnConflict=true
client.rm.reportRetryCount=5
client.rm.tableMetaCheckEnable=false
client.rm.tableMetaCheckerInterval=60000
client.rm.sqlParserType=druid
client.rm.reportSuccessEnable=false
client.rm.sagaBranchRegisterEnable=false
client.rm.sagaJsonParser=fastjson
client.rm.tccActionInterceptorOrder=-2147482648
client.tm.commitRetryCount=5
client.tm.rollbackRetryCount=5
client.tm.defaultGlobalTransactionTimeout=60000
client.tm.degradeCheck=false
client.tm.degradeCheckAllowTimes=10
client.tm.degradeCheckPeriod=2000
client.tm.interceptorOrder=-2147482648
client.undo.dataValidation=true
client.undo.logSerialization=jackson
client.undo.onlyCareUpdateColumns=true
server.undo.logSaveDays=7
server.undo.logDeletePeriod=86400000
client.undo.logTable=undo_log
client.undo.compress.enable=true
client.undo.compress.type=zip
client.undo.compress.threshold=64k
```

#### 客户端配置

```yaml
# IMSI服务Seata配置
seata:
  enabled: true
  application-id: nsrs-imsi-service
  tx-service-group: nsrs-tx-group
  enable-auto-data-source-proxy: true
  data-source-proxy-mode: AT
  use-jdk-proxy: false
  excludes-for-auto-proxying: firstClassNameForExclude,secondClassNameForExclude
  client:
    rm:
      async-commit-buffer-limit: 1000
      report-retry-count: 5
      table-meta-check-enable: false
      report-success-enable: false
      saga-branch-register-enable: false
      saga-json-parser: fastjson
      lock:
        retry-interval: 10
        retry-times: 30
        retry-policy-branch-rollback-on-conflict: true
    tm:
      commit-retry-count: 5
      rollback-retry-count: 5
      default-global-transaction-timeout: 60000
      degrade-check: false
      degrade-check-period: 2000
      degrade-check-allow-times: 10
      interceptor-order: -2147482648
    undo:
      data-validation: true
      log-serialization: jackson
      log-table: undo_log
      only-care-update-columns: true
      compress:
        enable: true
        type: zip
        threshold: 64k
  service:
    vgroup-mapping:
      nsrs-tx-group: default
    grouplist:
      default: seata-server1:8091,seata-server2:8092,seata-server3:8093
    enable-degrade: false
    disable-global-transaction: false
  transport:
    shutdown:
      wait: 3
    thread-factory:
      boss-thread-prefix: NettyBoss
      worker-thread-prefix: NettyServerNIOWorker
      server-executor-thread-prefix: NettyServerBizHandler
      share-boss-worker: false
      client-selector-thread-prefix: NettyClientSelector
      client-selector-thread-size: 1
      client-worker-thread-prefix: NettyClientWorkerThread
      worker-thread-size: default
      boss-thread-size: 1
    type: TCP
    server: NIO
    heartbeat: true
    serialization: seata
    compressor: none
    enable-client-batch-send-request: true
  config:
    type: nacos
    nacos:
      namespace: seata
      serverAddr: nacos-nginx:80
      group: SEATA_GROUP
      username: nacos
      password: nacos123
      data-id: seataServer.properties
  registry:
    type: nacos
    nacos:
      application: seata-server
      server-addr: nacos-nginx:80
      group: SEATA_GROUP
      namespace: seata
      username: nacos
      password: nacos123
      cluster: default
```

#### 分布式事务使用示例

```java
/**
 * IMSI分配业务场景
 * 涉及多个服务：IMSI服务、用户服务、计费服务
 */
@Service
public class ImsiAllocationBusinessService {
    
    @Autowired
    private ImsiResourceService imsiResourceService;
    
    @Autowired
    private UserServiceClient userServiceClient;
    
    @Autowired
    private BillingServiceClient billingServiceClient;
    
    @Autowired
    private NotificationServiceClient notificationServiceClient;
    
    /**
     * 分布式事务：IMSI分配业务
     * 1. 分配IMSI资源
     * 2. 扣减用户配额
     * 3. 创建计费记录
     * 4. 发送通知
     */
    @GlobalTransactional(rollbackFor = Exception.class, timeoutMills = 60000)
    public ImsiAllocationResult allocateImsiWithTransaction(ImsiAllocationRequest request) {
        String xid = RootContext.getXID();
        log.info("Start global transaction, XID: {}", xid);
        
        try {
            // 1. 分配IMSI资源
            log.info("Step 1: Allocating IMSI resources");
            List<ImsiResourceDTO> allocatedImsi = imsiResourceService.allocateImsi(
                ImsiAllocateRequest.builder()
                    .userId(request.getUserId())
                    .count(request.getCount())
                    .imsiType(request.getImsiType())
                    .supplierId(request.getSupplierId())
                    .build()
            );
            
            if (allocatedImsi.isEmpty()) {
                throw new BusinessException("No available IMSI resources");
            }
            
            // 2. 扣减用户配额
            log.info("Step 2: Deducting user quota");
            Result<Boolean> quotaResult = userServiceClient.deductQuota(
                UserQuotaDeductRequest.builder()
                    .userId(request.getUserId())
                    .quotaType("IMSI")
                    .amount(allocatedImsi.size())
                    .businessId(request.getBusinessId())
                    .build()
            );
            
            if (!quotaResult.isSuccess() || !quotaResult.getData()) {
                throw new BusinessException("Failed to deduct user quota: " + quotaResult.getMessage());
            }
            
            // 3. 创建计费记录
            log.info("Step 3: Creating billing record");
            BigDecimal totalAmount = calculateTotalAmount(allocatedImsi);
            Result<BillingDTO> billingResult = billingServiceClient.createBilling(
                BillingCreateRequest.builder()
                    .userId(request.getUserId())
                    .businessType("IMSI_ALLOCATION")
                    .businessId(request.getBusinessId())
                    .amount(totalAmount)
                    .imsiList(allocatedImsi.stream().map(ImsiResourceDTO::getImsi).collect(Collectors.toList()))
                    .description("IMSI allocation billing")
                    .build()
            );
            
            if (!billingResult.isSuccess()) {
                throw new BusinessException("Failed to create billing record: " + billingResult.getMessage());
            }
            
            // 4. 发送通知（可选，失败不回滚）
            try {
                log.info("Step 4: Sending notification");
                notificationServiceClient.sendAllocationNotification(
                    NotificationRequest.builder()
                        .userId(request.getUserId())
                        .type("IMSI_ALLOCATION")
                        .content("Successfully allocated " + allocatedImsi.size() + " IMSI resources")
                        .imsiList(allocatedImsi.stream().map(ImsiResourceDTO::getImsi).collect(Collectors.toList()))
                        .build()
                );
            } catch (Exception e) {
                log.warn("Failed to send notification, but transaction will continue", e);
            }
            
            log.info("Global transaction completed successfully, XID: {}", xid);
            
            return ImsiAllocationResult.builder()
                .success(true)
                .allocatedImsi(allocatedImsi)
                .billingId(billingResult.getData().getBillingId())
                .totalAmount(totalAmount)
                .build();
                
        } catch (Exception e) {
            log.error("Global transaction failed, XID: {}", xid, e);
            throw e;
        }
    }
    
    private BigDecimal calculateTotalAmount(List<ImsiResourceDTO> imsiList) {
        // 计算总金额逻辑
        return BigDecimal.valueOf(imsiList.size()).multiply(new BigDecimal("0.10"));
    }
}

/**
 * 用户服务中的配额扣减
 */
@Service
public class UserQuotaService {
    
    @Autowired
    private UserQuotaMapper userQuotaMapper;
    
    @Transactional(rollbackFor = Exception.class)
    public Boolean deductQuota(UserQuotaDeductRequest request) {
        String xid = RootContext.getXID();
        log.info("Deducting user quota in transaction, XID: {}", xid);
        
        // 查询用户配额
        UserQuota userQuota = userQuotaMapper.selectByUserIdAndType(
            request.getUserId(), request.getQuotaType());
        
        if (userQuota == null) {
            throw new BusinessException("User quota not found");
        }
        
        if (userQuota.getAvailableQuota() < request.getAmount()) {
            throw new BusinessException("Insufficient quota");
        }
        
        // 扣减配额
        int updated = userQuotaMapper.deductQuota(
            request.getUserId(), 
            request.getQuotaType(), 
            request.getAmount(),
            userQuota.getVersion()
        );
        
        if (updated == 0) {
            throw new BusinessException("Failed to deduct quota, concurrent modification detected");
        }
        
        // 记录配额变更日志
        UserQuotaLog quotaLog = UserQuotaLog.builder()
            .userId(request.getUserId())
            .quotaType(request.getQuotaType())
            .changeAmount(-request.getAmount())
            .businessId(request.getBusinessId())
            .businessType("DEDUCT")
            .createTime(new Date())
            .build();
        
        userQuotaMapper.insertQuotaLog(quotaLog);
        
        log.info("User quota deducted successfully, XID: {}", xid);
        return true;
    }
}

/**
 * 计费服务中的计费记录创建
 */
@Service
public class BillingService {
    
    @Autowired
    private BillingMapper billingMapper;
    
    @Autowired
    private SequenceService sequenceService;
    
    @Transactional(rollbackFor = Exception.class)
    public BillingDTO createBilling(BillingCreateRequest request) {
        String xid = RootContext.getXID();
        log.info("Creating billing record in transaction, XID: {}", xid);
        
        // 生成计费ID
        Long billingId = sequenceService.nextId("billing");
        
        // 创建计费记录
        Billing billing = Billing.builder()
            .billingId(billingId)
            .userId(request.getUserId())
            .businessType(request.getBusinessType())
            .businessId(request.getBusinessId())
            .amount(request.getAmount())
            .status(BillingStatus.PENDING.getCode())
            .description(request.getDescription())
            .createTime(new Date())
            .build();
        
        billingMapper.insert(billing);
        
        // 创建计费明细
        if (request.getImsiList() != null && !request.getImsiList().isEmpty()) {
            List<BillingDetail> details = request.getImsiList().stream()
                .map(imsi -> BillingDetail.builder()
                    .billingId(billingId)
                    .resourceType("IMSI")
                    .resourceId(imsi)
                    .unitPrice(new BigDecimal("0.10"))
                    .quantity(1)
                    .amount(new BigDecimal("0.10"))
                    .createTime(new Date())
                    .build())
                .collect(Collectors.toList());
            
            billingMapper.batchInsertDetails(details);
        }
        
        log.info("Billing record created successfully, XID: {}, billingId: {}", xid, billingId);
        
        return BillingDTO.builder()
            .billingId(billingId)
            .userId(request.getUserId())
            .amount(request.getAmount())
            .status(BillingStatus.PENDING.getCode())
            .createTime(billing.getCreateTime())
            .build();
    }
}
```

### 2. TCC事务模式

```java
/**
 * TCC事务模式示例
 */
@LocalTCC
public interface ImsiResourceTccService {
    
    /**
     * Try阶段：预留IMSI资源
     */
    @TwoPhaseBusinessAction(
        name = "imsiResourceTcc",
        commitMethod = "commitAllocateImsi",
        rollbackMethod = "rollbackAllocateImsi"
    )
    boolean tryAllocateImsi(
        BusinessActionContext actionContext,
        @BusinessActionContextParameter(paramName = "request") ImsiAllocateRequest request
    );
    
    /**
     * Confirm阶段：确认分配IMSI资源
     */
    boolean commitAllocateImsi(BusinessActionContext actionContext);
    
    /**
     * Cancel阶段：取消分配，释放预留资源
     */
    boolean rollbackAllocateImsi(BusinessActionContext actionContext);
}

@Service
public class ImsiResourceTccServiceImpl implements ImsiResourceTccService {
    
    @Autowired
    private ImsiResourceMapper imsiResourceMapper;
    
    @Autowired
    private RedisTemplate<String, Object> redisTemplate;
    
    @Override
    public boolean tryAllocateImsi(BusinessActionContext actionContext, ImsiAllocateRequest request) {
        String xid = actionContext.getXid();
        log.info("TCC Try: allocating IMSI resources, XID: {}", xid);
        
        try {
            // 1. 查询可用的IMSI资源
            List<ImsiResource> availableImsi = imsiResourceMapper.selectAvailableImsi(
                request.getImsiType(), 
                request.getSupplierId(), 
                request.getCount()
            );
            
            if (availableImsi.size() < request.getCount()) {
                log.warn("Insufficient IMSI resources, required: {}, available: {}", 
                    request.getCount(), availableImsi.size());
                return false;
            }
            
            // 2. 预留资源（更新状态为RESERVED）
            List<String> imsiList = availableImsi.stream()
                .map(ImsiResource::getImsi)
                .collect(Collectors.toList());
            
            int updated = imsiResourceMapper.batchUpdateStatusWithVersion(
                imsiList, 
                ImsiStatus.RESERVED.getCode(), 
                ImsiStatus.AVAILABLE.getCode()
            );
            
            if (updated != request.getCount()) {
                log.warn("Failed to reserve all IMSI resources, expected: {}, actual: {}", 
                    request.getCount(), updated);
                return false;
            }
            
            // 3. 缓存预留信息
            String reserveKey = "imsi:reserve:" + xid;
            ImsiReserveInfo reserveInfo = ImsiReserveInfo.builder()
                .xid(xid)
                .userId(request.getUserId())
                .imsiList(imsiList)
                .reserveTime(new Date())
                .build();
            
            redisTemplate.opsForValue().set(reserveKey, reserveInfo, Duration.ofMinutes(10));
            
            log.info("TCC Try completed successfully, XID: {}, reserved IMSI count: {}", xid, updated);
            return true;
            
        } catch (Exception e) {
            log.error("TCC Try failed, XID: {}", xid, e);
            return false;
        }
    }
    
    @Override
    public boolean commitAllocateImsi(BusinessActionContext actionContext) {
        String xid = actionContext.getXid();
        log.info("TCC Commit: confirming IMSI allocation, XID: {}", xid);
        
        try {
            // 1. 获取预留信息
            String reserveKey = "imsi:reserve:" + xid;
            ImsiReserveInfo reserveInfo = (ImsiReserveInfo) redisTemplate.opsForValue().get(reserveKey);
            
            if (reserveInfo == null) {
                log.warn("Reserve info not found for XID: {}", xid);
                return true; // 幂等性处理
            }
            
            // 2. 确认分配（更新状态为ALLOCATED）
            int updated = imsiResourceMapper.batchUpdateStatusWithCondition(
                reserveInfo.getImsiList(),
                ImsiStatus.ALLOCATED.getCode(),
                ImsiStatus.RESERVED.getCode()
            );
            
            // 3. 清理缓存
            redisTemplate.delete(reserveKey);
            
            log.info("TCC Commit completed successfully, XID: {}, confirmed IMSI count: {}", xid, updated);
            return true;
            
        } catch (Exception e) {
            log.error("TCC Commit failed, XID: {}", xid, e);
            return false;
        }
    }
    
    @Override
    public boolean rollbackAllocateImsi(BusinessActionContext actionContext) {
        String xid = actionContext.getXid();
        log.info("TCC Rollback: canceling IMSI allocation, XID: {}", xid);
        
        try {
            // 1. 获取预留信息
            String reserveKey = "imsi:reserve:" + xid;
            ImsiReserveInfo reserveInfo = (ImsiReserveInfo) redisTemplate.opsForValue().get(reserveKey);
            
            if (reserveInfo == null) {
                log.warn("Reserve info not found for XID: {}", xid);
                return true; // 幂等性处理
            }
            
            // 2. 释放预留资源（恢复状态为AVAILABLE）
            int updated = imsiResourceMapper.batchUpdateStatusWithCondition(
                reserveInfo.getImsiList(),
                ImsiStatus.AVAILABLE.getCode(),
                ImsiStatus.RESERVED.getCode()
            );
            
            // 3. 清理缓存
            redisTemplate.delete(reserveKey);
            
            log.info("TCC Rollback completed successfully, XID: {}, released IMSI count: {}", xid, updated);
            return true;
            
        } catch (Exception e) {
            log.error("TCC Rollback failed, XID: {}", xid, e);
            return false;
        }
    }
}
```

## 分布式锁

### 1. Redis分布式锁

```java
@Component
public class RedisDistributedLock {
    
    @Autowired
    private RedisTemplate<String, Object> redisTemplate;
    
    private static final String LOCK_PREFIX = "lock:";
    private static final String UNLOCK_SCRIPT = 
        "if redis.call('get', KEYS[1]) == ARGV[1] then " +
        "return redis.call('del', KEYS[1]) " +
        "else return 0 end";
    
    /**
     * 尝试获取锁
     */
    public boolean tryLock(String key, String value, long expireTime, TimeUnit timeUnit) {
        String lockKey = LOCK_PREFIX + key;
        Boolean result = redisTemplate.opsForValue().setIfAbsent(
            lockKey, value, expireTime, timeUnit);
        return Boolean.TRUE.equals(result);
    }
    
    /**
     * 释放锁
     */
    public boolean unlock(String key, String value) {
        String lockKey = LOCK_PREFIX + key;
        DefaultRedisScript<Long> script = new DefaultRedisScript<>();
        script.setScriptText(UNLOCK_SCRIPT);
        script.setResultType(Long.class);
        
        Long result = redisTemplate.execute(script, Collections.singletonList(lockKey), value);
        return Long.valueOf(1).equals(result);
    }
    
    /**
     * 自动续期锁
     */
    public boolean renewLock(String key, String value, long expireTime, TimeUnit timeUnit) {
        String lockKey = LOCK_PREFIX + key;
        String currentValue = (String) redisTemplate.opsForValue().get(lockKey);
        
        if (value.equals(currentValue)) {
            return redisTemplate.expire(lockKey, expireTime, timeUnit);
        }
        return false;
    }
}

/**
 * 分布式锁注解
 */
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
public @interface DistributedLock {
    
    /**
     * 锁的key，支持SpEL表达式
     */
    String key();
    
    /**
     * 锁的过期时间
     */
    long expireTime() default 30;
    
    /**
     * 时间单位
     */
    TimeUnit timeUnit() default TimeUnit.SECONDS;
    
    /**
     * 获取锁的等待时间
     */
    long waitTime() default 0;
    
    /**
     * 是否自动续期
     */
    boolean autoRenew() default false;
}

/**
 * 分布式锁AOP
 */
@Aspect
@Component
public class DistributedLockAspect {
    
    @Autowired
    private RedisDistributedLock distributedLock;
    
    @Autowired
    private SpelExpressionParser spelParser;
    
    @Around("@annotation(distributedLockAnnotation)")
    public Object around(ProceedingJoinPoint joinPoint, DistributedLock distributedLockAnnotation) throws Throwable {
        String lockKey = parseLockKey(distributedLockAnnotation.key(), joinPoint);
        String lockValue = UUID.randomUUID().toString();
        
        boolean acquired = false;
        long waitTime = distributedLockAnnotation.waitTime();
        long startTime = System.currentTimeMillis();
        
        try {
            // 尝试获取锁
            while (!acquired && (waitTime == 0 || System.currentTimeMillis() - startTime < waitTime * 1000)) {
                acquired = distributedLock.tryLock(
                    lockKey, 
                    lockValue, 
                    distributedLockAnnotation.expireTime(), 
                    distributedLockAnnotation.timeUnit()
                );
                
                if (!acquired && waitTime > 0) {
                    Thread.sleep(100); // 等待100ms后重试
                }
            }
            
            if (!acquired) {
                throw new BusinessException("Failed to acquire distributed lock: " + lockKey);
            }
            
            // 自动续期
            ScheduledFuture<?> renewTask = null;
            if (distributedLockAnnotation.autoRenew()) {
                renewTask = startRenewTask(lockKey, lockValue, distributedLockAnnotation);
            }
            
            try {
                return joinPoint.proceed();
            } finally {
                if (renewTask != null) {
                    renewTask.cancel(true);
                }
            }
            
        } finally {
            if (acquired) {
                distributedLock.unlock(lockKey, lockValue);
            }
        }
    }
    
    private String parseLockKey(String keyExpression, ProceedingJoinPoint joinPoint) {
        if (!keyExpression.contains("#")) {
            return keyExpression;
        }
        
        EvaluationContext context = new StandardEvaluationContext();
        MethodSignature signature = (MethodSignature) joinPoint.getSignature();
        String[] paramNames = signature.getParameterNames();
        Object[] args = joinPoint.getArgs();
        
        for (int i = 0; i < paramNames.length; i++) {
            context.setVariable(paramNames[i], args[i]);
        }
        
        Expression expression = spelParser.parseExpression(keyExpression);
        return expression.getValue(context, String.class);
    }
    
    private ScheduledFuture<?> startRenewTask(String lockKey, String lockValue, DistributedLock annotation) {
        ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor();
        long renewInterval = annotation.expireTime() / 3; // 每1/3过期时间续期一次
        
        return executor.scheduleAtFixedRate(() -> {
            try {
                distributedLock.renewLock(lockKey, lockValue, annotation.expireTime(), annotation.timeUnit());
            } catch (Exception e) {
                log.error("Failed to renew lock: {}", lockKey, e);
            }
        }, renewInterval, renewInterval, annotation.timeUnit());
    }
}

/**
 * 使用示例
 */
@Service
public class ImsiResourceServiceImpl {
    
    /**
     * IMSI生成需要分布式锁，防止并发生成重复IMSI
     */
    @DistributedLock(
        key = "'imsi:generate:' + #request.supplierId + ':' + #request.imsiType",
        expireTime = 60,
        waitTime = 10,
        autoRenew = true
    )
    public List<ImsiResourceDTO> generateImsi(ImsiGenerateRequest request) {
        log.info("Generating IMSI with distributed lock");
        
        // 生成IMSI的业务逻辑
        return doGenerateImsi(request);
    }
    
    /**
     * IMSI分配需要分布式锁，防止超分配
     */
    @DistributedLock(
        key = "'imsi:allocate:' + #request.userId",
        expireTime = 30,
        waitTime = 5
    )
    public List<ImsiResourceDTO> allocateImsi(ImsiAllocateRequest request) {
        log.info("Allocating IMSI with distributed lock");
        
        // 分配IMSI的业务逻辑
        return doAllocateImsi(request);
    }
}
```

### 2. Redisson分布式锁

```java
@Configuration
public class RedissonConfig {
    
    @Bean
    public RedissonClient redissonClient() {
        Config config = new Config();
        
        // 集群模式
        config.useClusterServers()
            .addNodeAddress("redis://redis-node1:6379")
            .addNodeAddress("redis://redis-node2:6379")
            .addNodeAddress("redis://redis-node3:6379")
            .setPassword("redis123")
            .setMasterConnectionMinimumIdleSize(10)
            .setMasterConnectionPoolSize(20)
            .setSlaveConnectionMinimumIdleSize(10)
            .setSlaveConnectionPoolSize(20)
            .setIdleConnectionTimeout(10000)
            .setConnectTimeout(10000)
            .setTimeout(3000)
            .setRetryAttempts(3)
            .setRetryInterval(1500);
        
        return Redisson.create(config);
    }
}

@Service
public class RedissonLockService {
    
    @Autowired
    private RedissonClient redissonClient;
    
    /**
     * 可重入锁
     */
    public void executeWithLock(String lockKey, Runnable task) {
        RLock lock = redissonClient.getLock(lockKey);
        try {
            if (lock.tryLock(10, 30, TimeUnit.SECONDS)) {
                task.run();
            } else {
                throw new BusinessException("Failed to acquire lock: " + lockKey);
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new BusinessException("Lock acquisition interrupted", e);
        } finally {
            if (lock.isHeldByCurrentThread()) {
                lock.unlock();
            }
        }
    }
    
    /**
     * 读写锁
     */
    public <T> T executeWithReadLock(String lockKey, Supplier<T> task) {
        RReadWriteLock readWriteLock = redissonClient.getReadWriteLock(lockKey);
        RLock readLock = readWriteLock.readLock();
        
        try {
            if (readLock.tryLock(10, 30, TimeUnit.SECONDS)) {
                return task.get();
            } else {
                throw new BusinessException("Failed to acquire read lock: " + lockKey);
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new BusinessException("Read lock acquisition interrupted", e);
        } finally {
            if (readLock.isHeldByCurrentThread()) {
                readLock.unlock();
            }
        }
    }
    
    /**
     * 信号量
     */
    public void executeWithSemaphore(String semaphoreKey, int permits, Runnable task) {
        RSemaphore semaphore = redissonClient.getSemaphore(semaphoreKey);
        try {
            if (semaphore.tryAcquire(permits, 10, TimeUnit.SECONDS)) {
                task.run();
            } else {
                throw new BusinessException("Failed to acquire semaphore: " + semaphoreKey);
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new BusinessException("Semaphore acquisition interrupted", e);
        } finally {
            semaphore.release(permits);
        }
    }
}
```

## 分布式ID生成

### 1. 雪花算法ID生成器

```java
@Component
public class SnowflakeIdGenerator {
    
    private static final long START_TIMESTAMP = 1640995200000L; // 2022-01-01 00:00:00
    private static final long SEQUENCE_BIT = 12;
    private static final long MACHINE_BIT = 5;
    private static final long DATACENTER_BIT = 5;
    
    private static final long MAX_DATACENTER_NUM = ~(-1L << DATACENTER_BIT);
    private static final long MAX_MACHINE_NUM = ~(-1L << MACHINE_BIT);
    private static final long MAX_SEQUENCE = ~(-1L << SEQUENCE_BIT);
    
    private static final long MACHINE_LEFT = SEQUENCE_BIT;
    private static final long DATACENTER_LEFT = SEQUENCE_BIT + MACHINE_BIT;
    private static final long TIMESTAMP_LEFT = DATACENTER_LEFT + DATACENTER_BIT;
    
    private final long datacenterId;
    private final long machineId;
    private long sequence = 0L;
    private long lastTimestamp = -1L;
    
    public SnowflakeIdGenerator(@Value("${snowflake.datacenter-id:1}") long datacenterId,
                               @Value("${snowflake.machine-id:1}") long machineId) {
        if (datacenterId > MAX_DATACENTER_NUM || datacenterId < 0) {
            throw new IllegalArgumentException("datacenterId can't be greater than " + MAX_DATACENTER_NUM + " or less than 0");
        }
        if (machineId > MAX_MACHINE_NUM || machineId < 0) {
            throw new IllegalArgumentException("machineId can't be greater than " + MAX_MACHINE_NUM + " or less than 0");
        }
        this.datacenterId = datacenterId;
        this.machineId = machineId;
    }
    
    public synchronized long nextId() {
        long currentTimestamp = getCurrentTimestamp();
        
        if (currentTimestamp < lastTimestamp) {
            throw new RuntimeException("Clock moved backwards. Refusing to generate id");
        }
        
        if (currentTimestamp == lastTimestamp) {
            sequence = (sequence + 1) & MAX_SEQUENCE;
            if (sequence == 0L) {
                currentTimestamp = getNextTimestamp();
            }
        } else {
            sequence = 0L;
        }
        
        lastTimestamp = currentTimestamp;
        
        return (currentTimestamp - START_TIMESTAMP) << TIMESTAMP_LEFT
                | datacenterId << DATACENTER_LEFT
                | machineId << MACHINE_LEFT
                | sequence;
    }
    
    private long getNextTimestamp() {
        long timestamp = getCurrentTimestamp();
        while (timestamp <= lastTimestamp) {
            timestamp = getCurrentTimestamp();
        }
        return timestamp;
    }
    
    private long getCurrentTimestamp() {
        return System.currentTimeMillis();
    }
}

/**
 * 序列号服务
 */
@Service
public class SequenceService {
    
    @Autowired
    private SnowflakeIdGenerator snowflakeIdGenerator;
    
    @Autowired
    private RedisTemplate<String, Object> redisTemplate;
    
    /**
     * 生成全局唯一ID
     */
    public Long nextId(String businessType) {
        return snowflakeIdGenerator.nextId();
    }
    
    /**
     * 生成业务序列号
     */
    public String nextSequence(String businessType) {
        String key = "sequence:" + businessType + ":" + LocalDate.now().format(DateTimeFormatter.ofPattern("yyyyMMdd"));
        Long sequence = redisTemplate.opsForValue().increment(key);
        
        if (sequence == 1) {
            // 设置过期时间为第二天凌晨
            LocalDateTime tomorrow = LocalDate.now().plusDays(1).atStartOfDay();
            Duration duration = Duration.between(LocalDateTime.now(), tomorrow);
            redisTemplate.expire(key, duration);
        }
        
        return businessType.toUpperCase() + LocalDate.now().format(DateTimeFormatter.ofPattern("yyyyMMdd")) + String.format("%06d", sequence);
    }
    
    /**
     * 生成IMSI序列号
     */
    public String nextImsiSequence(Long supplierId) {
        String key = "sequence:imsi:" + supplierId + ":" + LocalDate.now().format(DateTimeFormatter.ofPattern("yyyyMMdd"));
        Long sequence = redisTemplate.opsForValue().increment(key);
        
        if (sequence == 1) {
            LocalDateTime tomorrow = LocalDate.now().plusDays(1).atStartOfDay();
            Duration duration = Duration.between(LocalDateTime.now(), tomorrow);
            redisTemplate.expire(key, duration);
        }
        
        return String.format("%03d%s%08d", supplierId, LocalDate.now().format(DateTimeFormatter.ofPattern("yyyyMMdd")).substring(2), sequence);
    }
}
```

### 2. 数据库序列号生成

```sql
-- 序列号表
CREATE TABLE sequence_generator (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    sequence_name VARCHAR(50) NOT NULL UNIQUE,
    current_value BIGINT NOT NULL DEFAULT 0,
    increment_step INT NOT NULL DEFAULT 1,
    max_value BIGINT,
    cycle_flag TINYINT DEFAULT 0,
    create_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    update_time DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_sequence_name (sequence_name)
);

-- 初始化序列
INSERT INTO sequence_generator (sequence_name, current_value, increment_step, max_value) VALUES
('IMSI_ID', 1000000, 1, 9999999999999999),
('MSISDN_ID', 1000000, 1, 9999999999999999),
('SIM_CARD_ID', 1000000, 1, 9999999999999999),
('BILLING_ID', 1000000, 1, 9999999999999999);
```

```java
@Repository
public class SequenceGeneratorMapper {
    
    /**
     * 获取下一个序列值
     */
    @Update("UPDATE sequence_generator SET current_value = current_value + increment_step WHERE sequence_name = #{sequenceName}")
    int updateSequence(@Param("sequenceName") String sequenceName);
    
    @Select("SELECT current_value FROM sequence_generator WHERE sequence_name = #{sequenceName}")
    Long getCurrentValue(@Param("sequenceName") String sequenceName);
    
    /**
     * 批量获取序列值
     */
    @Update("UPDATE sequence_generator SET current_value = current_value + #{batchSize} * increment_step WHERE sequence_name = #{sequenceName}")
    int updateSequenceBatch(@Param("sequenceName") String sequenceName, @Param("batchSize") int batchSize);
}

@Service
public class DatabaseSequenceService {
    
    @Autowired
    private SequenceGeneratorMapper sequenceGeneratorMapper;
    
    @Autowired
    private RedisTemplate<String, Object> redisTemplate;
    
    /**
     * 获取下一个序列值
     */
    @Transactional
    public Long nextSequence(String sequenceName) {
        int updated = sequenceGeneratorMapper.updateSequence(sequenceName);
        if (updated == 0) {
            throw new BusinessException("Sequence not found: " + sequenceName);
        }
        return sequenceGeneratorMapper.getCurrentValue(sequenceName);
    }
    
    /**
     * 批量获取序列值（性能优化）
     */
    @Transactional
    public List<Long> nextSequenceBatch(String sequenceName, int batchSize) {
        int updated = sequenceGeneratorMapper.updateSequenceBatch(sequenceName, batchSize);
        if (updated == 0) {
            throw new BusinessException("Sequence not found: " + sequenceName);
        }
        
        Long endValue = sequenceGeneratorMapper.getCurrentValue(sequenceName);
        Long startValue = endValue - batchSize + 1;
        
        List<Long> sequences = new ArrayList<>();
        for (long i = startValue; i <= endValue; i++) {
            sequences.add(i);
        }
        
        return sequences;
    }
    
    /**
     * 缓存序列值（减少数据库访问）
     */
    public Long nextCachedSequence(String sequenceName) {
        String cacheKey = "sequence:cache:" + sequenceName;
        String countKey = cacheKey + ":count";
        
        // 检查缓存中是否有可用序列
        Long count = (Long) redisTemplate.opsForValue().get(countKey);
        if (count != null && count > 0) {
            Long sequence = redisTemplate.opsForList().leftPop(cacheKey, Long.class);
            if (sequence != null) {
                redisTemplate.opsForValue().decrement(countKey);
                return sequence;
            }
        }
        
        // 缓存中没有可用序列，从数据库批量获取
        synchronized (this) {
            // 双重检查
            count = (Long) redisTemplate.opsForValue().get(countKey);
            if (count != null && count > 0) {
                Long sequence = redisTemplate.opsForList().leftPop(cacheKey, Long.class);
                if (sequence != null) {
                    redisTemplate.opsForValue().decrement(countKey);
                    return sequence;
                }
            }
            
            // 从数据库批量获取序列
            List<Long> sequences = nextSequenceBatch(sequenceName, 100);
            
            // 缓存序列
            redisTemplate.opsForList().rightPushAll(cacheKey, sequences.toArray());
            redisTemplate.opsForValue().set(countKey, sequences.size());
            redisTemplate.expire(cacheKey, Duration.ofHours(1));
            redisTemplate.expire(countKey, Duration.ofHours(1));
            
            // 返回第一个序列
            Long sequence = redisTemplate.opsForList().leftPop(cacheKey, Long.class);
            redisTemplate.opsForValue().decrement(countKey);
            return sequence;
        }
    }
}
```

## 分布式调度

### 1. XXL-JOB分布式调度

```yaml
# XXL-JOB配置
xxl:
  job:
    admin:
      addresses: http://xxl-job-admin:8080/xxl-job-admin
    executor:
      appname: nsrs-imsi-service
      address:
      ip:
      port: 9999
      logpath: /data/applogs/xxl-job/jobhandler
      logretentiondays: 30
    accessToken: nsrs-job-token
```

```java
@Configuration
public class XxlJobConfig {
    
    @Value("${xxl.job.admin.addresses}")
    private String adminAddresses;
    
    @Value("${xxl.job.accessToken}")
    private String accessToken;
    
    @Value("${xxl.job.executor.appname}")
    private String appname;
    
    @Value("${xxl.job.executor.address}")
    private String address;
    
    @Value("${xxl.job.executor.ip}")
    private String ip;
    
    @Value("${xxl.job.executor.port}")
    private int port;
    
    @Value("${xxl.job.executor.logpath}")
    private String logPath;
    
    @Value("${xxl.job.executor.logretentiondays}")
    private int logRetentionDays;
    
    @Bean
    public XxlJobSpringExecutor xxlJobExecutor() {
        XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor();
        xxlJobSpringExecutor.setAdminAddresses(adminAddresses);
        xxlJobSpringExecutor.setAppname(appname);
        xxlJobSpringExecutor.setAddress(address);
        xxlJobSpringExecutor.setIp(ip);
        xxlJobSpringExecutor.setPort(port);
        xxlJobSpringExecutor.setAccessToken(accessToken);
        xxlJobSpringExecutor.setLogPath(logPath);
        xxlJobSpringExecutor.setLogRetentionDays(logRetentionDays);
        return xxlJobSpringExecutor;
    }
}

/**
 * 定时任务处理器
 */
@Component
public class ImsiJobHandler {
    
    @Autowired
    private ImsiResourceService imsiResourceService;
    
    @Autowired
    private ImsiGroupService imsiGroupService;
    
    /**
     * IMSI资源清理任务
     */
    @XxlJob("imsiResourceCleanupJob")
    public void imsiResourceCleanup() {
        XxlJobHelper.log("Starting IMSI resource cleanup job");
        
        try {
            // 清理过期的预留IMSI
            int cleanedReserved = imsiResourceService.cleanupExpiredReservedImsi();
            XxlJobHelper.log("Cleaned up {} expired reserved IMSI", cleanedReserved);
            
            // 清理无效的IMSI
            int cleanedInvalid = imsiResourceService.cleanupInvalidImsi();
            XxlJobHelper.log("Cleaned up {} invalid IMSI", cleanedInvalid);
            
            XxlJobHelper.handleSuccess("IMSI resource cleanup completed successfully");
            
        } catch (Exception e) {
            XxlJobHelper.log("IMSI resource cleanup failed: {}", e.getMessage());
            XxlJobHelper.handleFail("IMSI resource cleanup failed: " + e.getMessage());
        }
    }
    
    /**
     * IMSI组统计任务
     */
    @XxlJob("imsiGroupStatisticsJob")
    public void imsiGroupStatistics() {
        XxlJobHelper.log("Starting IMSI group statistics job");
        
        try {
            List<ImsiGroup> groups = imsiGroupService.getAllActiveGroups();
            
            for (ImsiGroup group : groups) {
                // 统计组内IMSI使用情况
                ImsiGroupStatistics statistics = imsiResourceService.calculateGroupStatistics(group.getGroupId());
                
                // 更新统计信息
                imsiGroupService.updateStatistics(group.getGroupId(), statistics);
                
                XxlJobHelper.log("Updated statistics for group {}: total={}, used={}, available={}", 
                    group.getGroupId(), statistics.getTotalCount(), statistics.getUsedCount(), statistics.getAvailableCount());
            }
            
            XxlJobHelper.handleSuccess("IMSI group statistics completed successfully");
            
        } catch (Exception e) {
            XxlJobHelper.log("IMSI group statistics failed: {}", e.getMessage());
            XxlJobHelper.handleFail("IMSI group statistics failed: " + e.getMessage());
        }
    }
    
    /**
     * IMSI资源预警任务
     */
    @XxlJob("imsiResourceAlertJob")
    public void imsiResourceAlert() {
        XxlJobHelper.log("Starting IMSI resource alert job");
        
        try {
            // 检查IMSI资源库存
            List<ImsiResourceAlert> alerts = imsiResourceService.checkResourceAlerts();
            
            for (ImsiResourceAlert alert : alerts) {
                // 发送预警通知
                sendAlert(alert);
                XxlJobHelper.log("Sent alert for supplier {}: available={}, threshold={}", 
                    alert.getSupplierId(), alert.getAvailableCount(), alert.getThreshold());
            }
            
            XxlJobHelper.handleSuccess("IMSI resource alert completed successfully");
            
        } catch (Exception e) {
            XxlJobHelper.log("IMSI resource alert failed: {}", e.getMessage());
            XxlJobHelper.handleFail("IMSI resource alert failed: " + e.getMessage());
        }
    }
    
    private void sendAlert(ImsiResourceAlert alert) {
        // 发送预警通知的逻辑
        // 可以通过邮件、短信、钉钉等方式发送
    }
}
```