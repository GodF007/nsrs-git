# 容灾方案设计指南

## 概述

本文档基于NSRS号卡资源管理系统，设计完整的容灾方案，确保系统在各种灾难场景下能够快速恢复服务，保障业务连续性。

## 容灾架构目标

- **RTO (Recovery Time Objective)**: 系统恢复时间目标 ≤ 30分钟
- **RPO (Recovery Point Objective)**: 数据恢复点目标 ≤ 5分钟
- **可用性目标**: 99.99% (年停机时间 ≤ 52.56分钟)
- **数据一致性**: 确保主备数据中心数据一致性

## 容灾架构设计

### 1. 双活数据中心架构

```
┌─────────────────┐    ┌─────────────────┐
│   主数据中心     │    │   备数据中心     │
│   (Beijing)     │    │   (Shanghai)    │
├─────────────────┤    ├─────────────────┤
│ Load Balancer   │◄──►│ Load Balancer   │
│ Application     │    │ Application     │
│ Database Master │◄──►│ Database Slave  │
│ Redis Master    │◄──►│ Redis Slave     │
│ File Storage    │◄──►│ File Storage    │
└─────────────────┘    └─────────────────┘
         │                       │
         └───────────────────────┘
              专线/VPN连接
```

### 2. 容灾级别定义

#### 2.1 同城容灾 (Tier 1)
- **距离**: 主备中心距离 < 100km
- **网络**: 专线连接，延迟 < 5ms
- **数据同步**: 实时同步
- **切换时间**: < 5分钟

#### 2.2 异地容灾 (Tier 2)
- **距离**: 主备中心距离 > 100km
- **网络**: 专线/VPN连接，延迟 < 50ms
- **数据同步**: 准实时同步
- **切换时间**: < 30分钟

#### 2.3 云端容灾 (Tier 3)
- **部署**: 混合云/多云架构
- **网络**: 互联网连接
- **数据同步**: 定时同步
- **切换时间**: < 2小时

## 数据库容灾方案

### 1. MySQL主从复制配置

#### 主库配置 (my.cnf)
```ini
# 主库配置
[mysqld]
server-id = 1
log-bin = mysql-bin
binlog-format = ROW
binlog-do-db = nsrs_sim_resource_0,nsrs_sim_resource_1,nsrs_sim_resource_2,nsrs_sim_resource_3
sync_binlog = 1
innodb_flush_log_at_trx_commit = 1

# 半同步复制
plugin-load = "rpl_semi_sync_master=semisync_master.so"
rpl_semi_sync_master_enabled = 1
rpl_semi_sync_master_timeout = 1000
```

#### 从库配置 (my.cnf)
```ini
# 从库配置
[mysqld]
server-id = 2
relay-log = relay-bin
log-slave-updates = 1
read_only = 1
super_read_only = 1

# 半同步复制
plugin-load = "rpl_semi_sync_slave=semisync_slave.so"
rpl_semi_sync_slave_enabled = 1
```

#### 主从复制监控脚本
```bash
#!/bin/bash
# mysql_replication_monitor.sh

DB_HOST="10.1.7.11"
DB_USER="monitor"
DB_PASS="monitor_password"
LOG_FILE="/var/log/mysql_replication.log"
ALERT_EMAIL="admin@nsrs.com"
MAX_DELAY=10  # 最大延迟秒数

# 检查复制状态
check_replication() {
    local result=$(mysql -h"$DB_HOST" -u"$DB_USER" -p"$DB_PASS" \
        -e "SHOW SLAVE STATUS\G" | grep -E "Slave_IO_Running|Slave_SQL_Running|Seconds_Behind_Master")
    
    local io_running=$(echo "$result" | grep "Slave_IO_Running" | awk '{print $2}')
    local sql_running=$(echo "$result" | grep "Slave_SQL_Running" | awk '{print $2}')
    local delay=$(echo "$result" | grep "Seconds_Behind_Master" | awk '{print $2}')
    
    echo "$(date): IO_Running=$io_running, SQL_Running=$sql_running, Delay=$delay" >> "$LOG_FILE"
    
    # 检查复制是否正常
    if [ "$io_running" != "Yes" ] || [ "$sql_running" != "Yes" ]; then
        send_alert "MySQL replication stopped: IO=$io_running, SQL=$sql_running"
        return 1
    fi
    
    # 检查延迟
    if [ "$delay" != "NULL" ] && [ "$delay" -gt "$MAX_DELAY" ]; then
        send_alert "MySQL replication delay too high: ${delay}s"
        return 1
    fi
    
    return 0
}

# 发送告警
send_alert() {
    local message="$1"
    echo "ALERT: $message" >> "$LOG_FILE"
    echo "$message" | mail -s "MySQL Replication Alert" "$ALERT_EMAIL"
    
    # 发送到监控系统
    curl -X POST "http://monitoring.nsrs.com/api/alerts" \
        -H "Content-Type: application/json" \
        -d "{
            \"level\": \"info\",
            \"message\": \"$message\",
            \"service\": \"network-$line_name\"
        }"
    
    log "Recovery alert sent for $line_name line"
}

# 发送严重告警
send_critical_alert() {
    local message="$1"
    
    # 发送邮件
    echo "CRITICAL: $message" | mail -s "CRITICAL Network Alert" "admin@nsrs.com"
    
    # 发送短信
    curl -X POST "https://sms.api.com/send" \
        -d "phone=13800138000&message=CRITICAL: $message"
    
    log "Critical alert sent: $message"
}

# 主监控循环
main() {
    log "Starting network monitoring"
    
    while true; do
        check_line_status "$TELECOM_GW" "telecom"
        check_line_status "$UNICOM_GW" "unicom"
        check_line_status "$MOBILE_GW" "mobile"
        
        sleep 60  # 每分钟检查一次
    done
}

# 执行主程序
main "$@"
```

## 容灾演练方案

### 1. 演练计划

#### 月度演练计划
```bash
#!/bin/bash
# disaster_drill.sh - 容灾演练脚本

DRILL_TYPE="$1"  # database, redis, application, network, full
LOG_FILE="/var/log/disaster_drill.log"
REPORT_FILE="/var/log/drill_report_$(date +%Y%m%d_%H%M%S).txt"

# 演练开始时间
START_TIME=$(date +%s)

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

# 数据库故障演练
database_drill() {
    log "Starting database failover drill"
    
    # 模拟主库故障
    log "Simulating primary database failure"
    kubectl patch service mysql-primary -p '{"spec":{"selector":{"app":"mysql-fake"}}}'
    
    # 等待故障检测
    sleep 30
    
    # 检查应用是否自动切换到从库
    local app_status=$(kubectl get pods -l app=nsrs-application -o jsonpath='{.items[*].status.phase}')
    if [[ "$app_status" == *"Running"* ]]; then
        log "✅ Application remained running during database failover"
    else
        log "❌ Application failed during database failover"
    fi
    
    # 恢复主库
    log "Restoring primary database"
    kubectl patch service mysql-primary -p '{"spec":{"selector":{"app":"mysql"}}}'
    
    sleep 30
    log "Database failover drill completed"
}

# Redis故障演练
redis_drill() {
    log "Starting Redis failover drill"
    
    # 模拟Redis主节点故障
    log "Simulating Redis master failure"
    kubectl delete pod redis-master-0
    
    # 等待Sentinel检测并切换
    sleep 60
    
    # 检查应用缓存功能
    local cache_test=$(curl -s http://nsrs.example.com/api/cache/test)
    if [[ "$cache_test" == *"success"* ]]; then
        log "✅ Cache functionality working after Redis failover"
    else
        log "❌ Cache functionality failed after Redis failover"
    fi
    
    log "Redis failover drill completed"
}

# 应用故障演练
application_drill() {
    log "Starting application failover drill"
    
    # 模拟主集群故障
    log "Simulating primary cluster failure"
    kubectl scale deployment nsrs-application --replicas=0 --context=primary-cluster
    
    # 等待流量切换
    sleep 120
    
    # 检查备集群是否接管流量
    local health_check=$(curl -s -o /dev/null -w "%{http_code}" https://nsrs.example.com/actuator/health)
    if [ "$health_check" = "200" ]; then
        log "✅ Backup cluster successfully took over traffic"
    else
        log "❌ Backup cluster failed to take over traffic"
    fi
    
    # 恢复主集群
    log "Restoring primary cluster"
    kubectl scale deployment nsrs-application --replicas=6 --context=primary-cluster
    
    sleep 180
    log "Application failover drill completed"
}

# 网络故障演练
network_drill() {
    log "Starting network failover drill"
    
    # 模拟电信线路故障
    log "Simulating telecom line failure"
    iptables -A OUTPUT -d 202.96.128.1 -j DROP
    
    # 等待路由切换
    sleep 60
    
    # 检查网络连通性
    if ping -c 3 8.8.8.8 >/dev/null 2>&1; then
        log "✅ Network connectivity maintained after line failure"
    else
        log "❌ Network connectivity lost after line failure"
    fi
    
    # 恢复网络
    log "Restoring telecom line"
    iptables -D OUTPUT -d 202.96.128.1 -j DROP
    
    sleep 30
    log "Network failover drill completed"
}

# 全面容灾演练
full_drill() {
    log "Starting full disaster recovery drill"
    
    database_drill
    redis_drill
    application_drill
    network_drill
    
    log "Full disaster recovery drill completed"
}

# 生成演练报告
generate_report() {
    local end_time=$(date +%s)
    local duration=$((end_time - START_TIME))
    
    cat > "$REPORT_FILE" << EOF
# 容灾演练报告

## 演练信息
- 演练类型: $DRILL_TYPE
- 开始时间: $(date -d @$START_TIME)
- 结束时间: $(date -d @$end_time)
- 演练时长: ${duration}秒

## 演练结果
$(grep -E "✅|❌" "$LOG_FILE" | tail -20)

## 详细日志
$(tail -100 "$LOG_FILE")

## 改进建议
- 检查故障检测时间是否符合RTO要求
- 验证数据一致性
- 优化自动切换流程
- 完善监控告警机制
EOF
    
    log "Drill report generated: $REPORT_FILE"
}

# 主程序
case "$DRILL_TYPE" in
    "database")
        database_drill
        ;;
    "redis")
        redis_drill
        ;;
    "application")
        application_drill
        ;;
    "network")
        network_drill
        ;;
    "full")
        full_drill
        ;;
    *)
        echo "Usage: $0 {database|redis|application|network|full}"
        exit 1
        ;;
esac

generate_report
```

### 2. 演练检查清单

#### 演练前检查
```markdown
## 容灾演练前检查清单

### 基础设施检查
- [ ] 主备数据中心网络连通性正常
- [ ] 数据库主从复制状态正常
- [ ] Redis集群状态正常
- [ ] 应用服务健康状态正常
- [ ] 监控系统运行正常
- [ ] 告警通知渠道畅通

### 数据备份检查
- [ ] 数据库备份完整性验证
- [ ] 文件存储备份验证
- [ ] 配置文件备份验证
- [ ] 应用代码备份验证

### 人员准备
- [ ] 演练人员到位
- [ ] 应急联系人确认
- [ ] 演练流程培训完成
- [ ] 回滚方案准备就绪

### 工具准备
- [ ] 监控工具正常
- [ ] 自动化脚本测试
- [ ] 通信工具准备
- [ ] 文档资料齐全
```

#### 演练后评估
```markdown
## 容灾演练后评估清单

### 时间指标评估
- [ ] RTO目标达成情况（目标：≤30分钟）
- [ ] RPO目标达成情况（目标：≤5分钟）
- [ ] 故障检测时间
- [ ] 切换执行时间
- [ ] 服务恢复时间

### 功能验证
- [ ] 数据完整性验证
- [ ] 业务功能验证
- [ ] 性能指标验证
- [ ] 用户体验验证

### 问题记录
- [ ] 故障点识别
- [ ] 改进建议记录
- [ ] 流程优化点
- [ ] 工具改进需求

### 后续行动
- [ ] 问题修复计划
- [ ] 流程优化实施
- [ ] 培训改进计划
- [ ] 下次演练安排
```

## 最佳实践总结

### 1. 设计原则
- **多层防护**: 从网络、应用、数据库、存储多个层面设计容灾方案
- **自动化优先**: 尽可能实现故障自动检测和自动切换
- **渐进式切换**: 采用灰度切换方式，降低切换风险
- **数据一致性**: 确保主备数据中心数据一致性

### 2. 监控要点
- **实时监控**: 7x24小时实时监控关键指标
- **多维度告警**: 从基础设施到应用层面的全方位告警
- **智能告警**: 避免告警风暴，实现智能告警聚合
- **可视化展示**: 提供直观的监控大屏和报表

### 3. 运维建议
- **定期演练**: 每月进行容灾演练，确保方案有效性
- **文档维护**: 及时更新容灾文档和操作手册
- **人员培训**: 定期进行容灾培训，提高应急响应能力
- **持续改进**: 根据演练结果持续优化容灾方案

### 4. 成本控制
- **资源复用**: 备用资源平时可用于测试环境
- **弹性扩容**: 根据业务需求动态调整备用资源规模
- **云端备份**: 利用云服务降低备份存储成本
- **自动化运维**: 通过自动化减少人工运维成本

通过以上容灾方案的实施，NSRS号卡资源管理系统能够在各种灾难场景下快速恢复服务，确保业务连续性，达到99.99%的高可用性目标。nsrs.com/api/alerts" \
        -H "Content-Type: application/json" \
        -d "{\"level\":\"critical\",\"message\":\"$message\",\"service\":\"mysql-replication\"}"
}

# 主循环
while true; do
    check_replication
    sleep 30
done
```

### 2. 数据库故障切换脚本

```bash
#!/bin/bash
# mysql_failover.sh - MySQL故障切换脚本

MASTER_HOST="10.1.7.10"
SLAVE_HOST="10.1.7.11"
DB_USER="root"
DB_PASS="root_password"
VIP="10.1.7.100"  # 虚拟IP
LOG_FILE="/var/log/mysql_failover.log"

# 日志函数
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

# 检查主库状态
check_master() {
    mysql -h"$MASTER_HOST" -u"$DB_USER" -p"$DB_PASS" \
        -e "SELECT 1" >/dev/null 2>&1
    return $?
}

# 提升从库为主库
promote_slave() {
    log "Promoting slave to master: $SLAVE_HOST"
    
    # 停止从库复制
    mysql -h"$SLAVE_HOST" -u"$DB_USER" -p"$DB_PASS" \
        -e "STOP SLAVE;"
    
    # 重置从库状态
    mysql -h"$SLAVE_HOST" -u"$DB_USER" -p"$DB_PASS" \
        -e "RESET SLAVE ALL;"
    
    # 设置为可写
    mysql -h"$SLAVE_HOST" -u"$DB_USER" -p"$DB_PASS" \
        -e "SET GLOBAL read_only = 0; SET GLOBAL super_read_only = 0;"
    
    # 切换VIP到新主库
    ssh root@"$SLAVE_HOST" "ip addr add $VIP/24 dev eth0"
    ssh root@"$MASTER_HOST" "ip addr del $VIP/24 dev eth0" 2>/dev/null || true
    
    log "Failover completed. New master: $SLAVE_HOST"
}

# 更新应用配置
update_application_config() {
    log "Updating application database configuration"
    
    # 更新Spring Boot配置
    kubectl patch configmap nsrs-config \
        --patch='{"data":{"spring.datasource.url":"jdbc:mysql://'$SLAVE_HOST':3306/nsrs_sim_resource_0"}}'
    
    # 重启应用服务
    kubectl rollout restart deployment/nsrs-application
    
    log "Application configuration updated"
}

# 发送通知
send_notification() {
    local message="MySQL failover completed. New master: $SLAVE_HOST"
    
    # 邮件通知
    echo "$message" | mail -s "MySQL Failover Notification" "admin@nsrs.com"
    
    # 钉钉通知
    curl -X POST "https://oapi.dingtalk.com/robot/send?access_token=YOUR_TOKEN" \
        -H "Content-Type: application/json" \
        -d "{
            \"msgtype\": \"text\",
            \"text\": {
                \"content\": \"$message\"
            }
        }"
}

# 主逻辑
main() {
    log "Starting MySQL failover check"
    
    if ! check_master; then
        log "Master database is down, initiating failover"
        
        promote_slave
        update_application_config
        send_notification
        
        log "Failover process completed"
    else
        log "Master database is healthy"
    fi
}

# 执行主逻辑
main "$@"
```

## Redis容灾方案

### 1. Redis Sentinel高可用配置

#### Redis主库配置 (redis.conf)
```conf
# Redis主库配置
port 6379
bind 0.0.0.0
protected-mode yes
requirepass "redis_password_2024"
masterauth "redis_password_2024"

# 持久化配置
save 900 1
save 300 10
save 60 10000
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir /var/lib/redis

# AOF配置
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# 复制配置
repl-diskless-sync no
repl-diskless-sync-delay 5
repl-ping-slave-period 10
repl-timeout 60
repl-disable-tcp-nodelay no
repl-backlog-size 1mb
repl-backlog-ttl 3600
```

#### Redis Sentinel配置 (sentinel.conf)
```conf
# Sentinel配置
port 26379
bind 0.0.0.0
protected-mode no

# 监控主库
sentinel monitor nsrs-redis 10.1.7.10 6379 2
sentinel auth-pass nsrs-redis redis_password_2024
sentinel down-after-milliseconds nsrs-redis 5000
sentinel parallel-syncs nsrs-redis 1
sentinel failover-timeout nsrs-redis 10000

# 通知脚本
sentinel notification-script nsrs-redis /opt/redis/scripts/notify.sh
sentinel client-reconfig-script nsrs-redis /opt/redis/scripts/reconfig.sh

# 日志配置
logfile /var/log/redis/sentinel.log
loglevel notice
```

#### Sentinel通知脚本
```bash
#!/bin/bash
# /opt/redis/scripts/notify.sh

EVENT_TYPE="$1"
EVENT_NAME="$2"
shift 2
EVENT_ARGS="$@"

LOG_FILE="/var/log/redis/sentinel-events.log"
ALERT_URL="http://monitoring.nsrs.com/api/alerts"

# 记录事件
echo "$(date): $EVENT_TYPE $EVENT_NAME $EVENT_ARGS" >> "$LOG_FILE"

# 发送告警
case "$EVENT_TYPE" in
    "+sdown")
        MESSAGE="Redis master is subjectively down: $EVENT_ARGS"
        LEVEL="warning"
        ;;
    "+odown")
        MESSAGE="Redis master is objectively down: $EVENT_ARGS"
        LEVEL="critical"
        ;;
    "+failover-triggered")
        MESSAGE="Redis failover triggered: $EVENT_ARGS"
        LEVEL="critical"
        ;;
    "+failover-end")
        MESSAGE="Redis failover completed: $EVENT_ARGS"
        LEVEL="info"
        ;;
    *)
        MESSAGE="Redis sentinel event: $EVENT_TYPE $EVENT_NAME $EVENT_ARGS"
        LEVEL="info"
        ;;
esac

# 发送到监控系统
curl -X POST "$ALERT_URL" \
    -H "Content-Type: application/json" \
    -d "{
        \"level\": \"$LEVEL\",
        \"message\": \"$MESSAGE\",
        \"service\": \"redis-sentinel\",
        \"timestamp\": \"$(date -Iseconds)\"
    }" 2>/dev/null || true

# 发送邮件通知（仅关键事件）
if [ "$LEVEL" = "critical" ]; then
    echo "$MESSAGE" | mail -s "Redis Sentinel Alert" "admin@nsrs.com"
fi
```

### 2. Redis集群容灾配置

#### Spring Boot Redis配置
```yaml
# application-disaster.yml
spring:
  redis:
    # Sentinel模式配置
    sentinel:
      master: nsrs-redis
      nodes:
        - 10.1.7.10:26379
        - 10.1.7.11:26379
        - 10.1.7.12:26379
      password: redis_password_2024
    password: redis_password_2024
    timeout: 3000ms
    
    # 连接池配置
    lettuce:
      pool:
        max-active: 200
        max-idle: 20
        min-idle: 5
        max-wait: 3000ms
      shutdown-timeout: 100ms
    
    # 集群配置（备选方案）
    cluster:
      nodes:
        - 10.1.7.10:7000
        - 10.1.7.10:7001
        - 10.1.7.11:7000
        - 10.1.7.11:7001
        - 10.1.7.12:7000
        - 10.1.7.12:7001
      max-redirects: 3
```

#### Redis容灾切换服务
```java
/**
 * Redis容灾切换服务
 */
@Service
@Slf4j
public class RedisDisasterRecoveryService {
    
    @Autowired
    private RedisTemplate<String, Object> redisTemplate;
    
    @Autowired
    private LettuceConnectionFactory connectionFactory;
    
    @Value("${spring.redis.sentinel.nodes}")
    private List<String> sentinelNodes;
    
    /**
     * 检查Redis连接状态
     */
    @Scheduled(fixedDelay = 30000) // 每30秒检查一次
    public void checkRedisHealth() {
        try {
            // 执行ping命令检查连接
            String result = redisTemplate.execute((RedisCallback<String>) connection -> {
                return connection.ping();
            });
            
            if (!"PONG".equals(result)) {
                log.warn("Redis health check failed: unexpected response: {}", result);
                handleRedisFailure();
            } else {
                log.debug("Redis health check passed");
            }
            
        } catch (Exception e) {
            log.error("Redis health check failed", e);
            handleRedisFailure();
        }
    }
    
    /**
     * 处理Redis故障
     */
    private void handleRedisFailure() {
        log.warn("Handling Redis failure, attempting to reconnect");
        
        try {
            // 重置连接
            connectionFactory.resetConnection();
            
            // 等待一段时间后重试
            Thread.sleep(5000);
            
            // 重新检查连接
            String result = redisTemplate.execute((RedisCallback<String>) connection -> {
                return connection.ping();
            });
            
            if ("PONG".equals(result)) {
                log.info("Redis connection restored successfully");
                sendRecoveryNotification();
            } else {
                log.error("Redis connection still failed after reset");
                triggerEmergencyMode();
            }
            
        } catch (Exception e) {
            log.error("Failed to handle Redis failure", e);
            triggerEmergencyMode();
        }
    }
    
    /**
     * 触发应急模式
     */
    private void triggerEmergencyMode() {
        log.error("Triggering emergency mode due to Redis failure");
        
        // 发送紧急告警
        sendEmergencyAlert();
        
        // 启用本地缓存模式
        enableLocalCacheMode();
        
        // 记录故障事件
        recordFailureEvent();
    }
    
    /**
     * 启用本地缓存模式
     */
    private void enableLocalCacheMode() {
        // 通过配置中心动态切换到本地缓存
        try {
            // 这里可以通过Nacos等配置中心动态修改缓存策略
            log.info("Switching to local cache mode");
            
            // 发布缓存模式切换事件
            ApplicationEventPublisher eventPublisher = 
                ApplicationContextHolder.getApplicationContext();
            eventPublisher.publishEvent(new CacheModeChangedEvent("LOCAL"));
            
        } catch (Exception e) {
            log.error("Failed to enable local cache mode", e);
        }
    }
    
    /**
     * 发送恢复通知
     */
    private void sendRecoveryNotification() {
        try {
            String message = "Redis connection has been restored at " + new Date();
            
            // 发送到监控系统
            sendToMonitoring("info", message);
            
            log.info("Redis recovery notification sent");
            
        } catch (Exception e) {
            log.error("Failed to send recovery notification", e);
        }
    }
    
    /**
     * 发送紧急告警
     */
    private void sendEmergencyAlert() {
        try {
            String message = "Redis cluster is down, emergency mode activated at " + new Date();
            
            // 发送到监控系统
            sendToMonitoring("critical", message);
            
            // 发送邮件告警
            sendEmailAlert(message);
            
            // 发送短信告警
            sendSmsAlert(message);
            
            log.error("Emergency alert sent for Redis failure");
            
        } catch (Exception e) {
            log.error("Failed to send emergency alert", e);
        }
    }
    
    /**
     * 发送到监控系统
     */
    private void sendToMonitoring(String level, String message) {
        // 实现发送到监控系统的逻辑
        RestTemplate restTemplate = new RestTemplate();
        
        Map<String, Object> alert = new HashMap<>();
        alert.put("level", level);
        alert.put("message", message);
        alert.put("service", "redis-cluster");
        alert.put("timestamp", Instant.now().toString());
        
        try {
            restTemplate.postForObject(
                "http://monitoring.nsrs.com/api/alerts", 
                alert, 
                String.class
            );
        } catch (Exception e) {
            log.error("Failed to send alert to monitoring system", e);
        }
    }
    
    /**
     * 发送邮件告警
     */
    private void sendEmailAlert(String message) {
        // 实现邮件告警逻辑
        // 可以使用Spring Mail或者第三方邮件服务
    }
    
    /**
     * 发送短信告警
     */
    private void sendSmsAlert(String message) {
        // 实现短信告警逻辑
        // 可以使用阿里云短信服务等
    }
    
    /**
     * 记录故障事件
     */
    private void recordFailureEvent() {
        try {
            // 记录到数据库或日志系统
            log.error("Redis failure event recorded at {}", new Date());
            
            // 可以将故障事件存储到数据库中用于后续分析
            
        } catch (Exception e) {
            log.error("Failed to record failure event", e);
        }
    }
}

/**
 * 缓存模式切换事件
 */
public class CacheModeChangedEvent extends ApplicationEvent {
    private final String mode;
    
    public CacheModeChangedEvent(String mode) {
        super(mode);
        this.mode = mode;
    }
    
    public String getMode() {
        return mode;
    }
}
```

## 应用服务容灾方案

### 1. Kubernetes多集群部署

#### 主集群部署配置
```yaml
# nsrs-deployment-primary.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nsrs-application
  namespace: nsrs-prod
  labels:
    app: nsrs-application
    cluster: primary
spec:
  replicas: 6
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 1
  selector:
    matchLabels:
      app: nsrs-application
  template:
    metadata:
      labels:
        app: nsrs-application
        cluster: primary
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - nsrs-application
              topologyKey: kubernetes.io/hostname
      containers:
      - name: nsrs-application
        image: nsrs/application:v1.0.0
        ports:
        - containerPort: 8080
          name: http
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "prod,primary"
        - name: DATABASE_URL
          value: "jdbc:mysql://mysql-primary.nsrs.com:3306/nsrs_sim_resource_0"
        - name: REDIS_SENTINEL_NODES
          value: "redis-sentinel-1:26379,redis-sentinel-2:26379,redis-sentinel-3:26379"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
        - name: logs-volume
          mountPath: /app/logs
      volumes:
      - name: config-volume
        configMap:
          name: nsrs-config
      - name: logs-volume
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: nsrs-application-service
  namespace: nsrs-prod
spec:
  selector:
    app: nsrs-application
  ports:
  - port: 80
    targetPort: 8080
    name: http
  type: ClusterIP
```

#### 备集群部署配置
```yaml
# nsrs-deployment-backup.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nsrs-application
  namespace: nsrs-prod
  labels:
    app: nsrs-application
    cluster: backup
spec:
  replicas: 3  # 备集群副本数较少
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: nsrs-application
  template:
    metadata:
      labels:
        app: nsrs-application
        cluster: backup
    spec:
      containers:
      - name: nsrs-application
        image: nsrs/application:v1.0.0
        ports:
        - containerPort: 8080
          name: http
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "prod,backup"
        - name: DATABASE_URL
          value: "jdbc:mysql://mysql-backup.nsrs.com:3306/nsrs_sim_resource_0"
        - name: REDIS_SENTINEL_NODES
          value: "redis-sentinel-backup-1:26379,redis-sentinel-backup-2:26379,redis-sentinel-backup-3:26379"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
```

### 2. 跨集群流量切换

#### Istio流量管理配置
```yaml
# traffic-management.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: nsrs-traffic-split
  namespace: nsrs-prod
spec:
  hosts:
  - nsrs.example.com
  gateways:
  - nsrs-gateway
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: nsrs-application-service
        subset: backup
      weight: 100
  - route:
    - destination:
        host: nsrs-application-service
        subset: primary
      weight: 100
    fault:
      abort:
        percentage:
          value: 0.1
        httpStatus: 500
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: nsrs-destination-rule
  namespace: nsrs-prod
spec:
  host: nsrs-application-service
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 10
    loadBalancer:
      simple: LEAST_CONN
    outlierDetection:
      consecutiveErrors: 3
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
  subsets:
  - name: primary
    labels:
      cluster: primary
    trafficPolicy:
      connectionPool:
        tcp:
          maxConnections: 200
  - name: backup
    labels:
      cluster: backup
    trafficPolicy:
      connectionPool:
        tcp:
          maxConnections: 100
```

#### 自动故障切换脚本
```bash
#!/bin/bash
# cluster_failover.sh - 集群故障切换脚本

PRIMARY_CLUSTER="primary"
BACKUP_CLUSTER="backup"
HEALTH_CHECK_URL="https://nsrs.example.com/actuator/health"
KUBECTL_PRIMARY="kubectl --context=primary-cluster"
KUBECTL_BACKUP="kubectl --context=backup-cluster"
LOG_FILE="/var/log/cluster_failover.log"
ALERT_WEBHOOK="https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"

# 日志函数
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

# 健康检查
health_check() {
    local cluster="$1"
    local url="$2"
    
    local response=$(curl -s -o /dev/null -w "%{http_code}" "$url" --max-time 10)
    
    if [ "$response" = "200" ]; then
        return 0
    else
        log "Health check failed for $cluster: HTTP $response"
        return 1
    fi
}

# 切换到备集群
failover_to_backup() {
    log "Initiating failover to backup cluster"
    
    # 扩容备集群
    log "Scaling up backup cluster"
    $KUBECTL_BACKUP scale deployment/nsrs-application --replicas=6 -n nsrs-prod
    
    # 等待备集群就绪
    log "Waiting for backup cluster to be ready"
    $KUBECTL_BACKUP wait --for=condition=available --timeout=300s deployment/nsrs-application -n nsrs-prod
    
    if [ $? -eq 0 ]; then
        # 更新Istio流量路由
        log "Updating traffic routing to backup cluster"
        
        cat <<EOF | kubectl apply -f -
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: nsrs-traffic-split
  namespace: nsrs-prod
spec:
  hosts:
  - nsrs.example.com
  gateways:
  - nsrs-gateway
  http:
  - route:
    - destination:
        host: nsrs-application-service
        subset: backup
      weight: 100
EOF
        
        log "Failover to backup cluster completed successfully"
        send_notification "✅ Failover completed: Traffic switched to backup cluster"
        
        return 0
    else
        log "ERROR: Backup cluster failed to become ready"
        send_notification "❌ Failover failed: Backup cluster not ready"
        return 1
    fi
}

# 切换回主集群
failback_to_primary() {
    log "Initiating failback to primary cluster"
    
    # 检查主集群健康状态
    if health_check "primary" "$HEALTH_CHECK_URL"; then
        # 逐步切换流量
        log "Gradually switching traffic back to primary cluster"
        
        # 50/50 分流
        cat <<EOF | kubectl apply -f -
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: nsrs-traffic-split
  namespace: nsrs-prod
spec:
  hosts:
  - nsrs.example.com
  gateways:
  - nsrs-gateway
  http:
  - route:
    - destination:
        host: nsrs-application-service
        subset: primary
      weight: 50
    - destination:
        host: nsrs-application-service
        subset: backup
      weight: 50
EOF
        
        sleep 60
        
        # 检查主集群是否稳定
        if health_check "primary" "$HEALTH_CHECK_URL"; then
            # 完全切换到主集群
            cat <<EOF | kubectl apply -f -
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: nsrs-traffic-split
  namespace: nsrs-prod
spec:
  hosts:
  - nsrs.example.com
  gateways:
  - nsrs-gateway
  http:
  - route:
    - destination:
        host: nsrs-application-service
        subset: primary
      weight: 100
EOF
            
            # 缩容备集群
            log "Scaling down backup cluster"
            $KUBECTL_BACKUP scale deployment/nsrs-application --replicas=2 -n nsrs-prod
            
            log "Failback to primary cluster completed successfully"
            send_notification "✅ Failback completed: Traffic switched back to primary cluster"
            
            return 0
        else
            log "ERROR: Primary cluster still unhealthy, keeping backup active"
            return 1
        fi
    else
        log "Primary cluster is still unhealthy, failback aborted"
        return 1
    fi
}

# 发送通知
send_notification() {
    local message="$1"
    
    # Slack通知
    curl -X POST "$ALERT_WEBHOOK" \
        -H "Content-Type: application/json" \
        -d "{
            \"text\": \"$message\",
            \"username\": \"NSRS Failover Bot\",
            \"icon_emoji\": \":warning:\"
        }"
    
    # 邮件通知
    echo "$message" | mail -s "NSRS Cluster Failover Notification" "admin@nsrs.com"
    
    log "Notification sent: $message"
}

# 主监控循环
main_monitor() {
    local consecutive_failures=0
    local max_failures=3
    local check_interval=30
    local current_cluster="primary"
    
    log "Starting cluster monitoring"
    
    while true; do
        if health_check "$current_cluster" "$HEALTH_CHECK_URL"; then
            consecutive_failures=0
            
            # 如果当前在备集群且主集群恢复，尝试切换回主集群
            if [ "$current_cluster" = "backup" ]; then
                if health_check "primary" "$HEALTH_CHECK_URL"; then
                    log "Primary cluster appears healthy, attempting failback"
                    if failback_to_primary; then
                        current_cluster="primary"
                    fi
                fi
            fi
        else
            consecutive_failures=$((consecutive_failures + 1))
            log "Health check failed ($consecutive_failures/$max_failures)"
            
            if [ $consecutive_failures -ge $max_failures ]; then
                if [ "$current_cluster" = "primary" ]; then
                    log "Primary cluster failed, initiating failover"
                    if failover_to_backup; then
                        current_cluster="backup"
                        consecutive_failures=0
                    fi
                else
                    log "ERROR: Backup cluster also failed!"
                    send_notification "🚨 CRITICAL: Both clusters are down!"
                fi
            fi
        fi
        
        sleep $check_interval
    done
}

# 执行主监控
case "${1:-monitor}" in
    "monitor")
        main_monitor
        ;;
    "failover")
        failover_to_backup
        ;;
    "failback")
        failback_to_primary
        ;;
    *)
        echo "Usage: $0 {monitor|failover|failback}"
        exit 1
        ;;
esac
```

## 文件存储容灾方案

### 1. 分布式文件系统配置

#### MinIO集群部署
```yaml
# minio-cluster.yaml
apiVersion: v1
kind: Service
metadata:
  name: minio-service
  namespace: nsrs-storage
spec:
  clusterIP: None
  ports:
  - port: 9000
    name: minio
  - port: 9001
    name: console
  selector:
    app: minio
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minio
  namespace: nsrs-storage
spec:
  serviceName: minio-service
  replicas: 4
  selector:
    matchLabels:
      app: minio
  template:
    metadata:
      labels:
        app: minio
    spec:
      containers:
      - name: minio
        image: minio/minio:RELEASE.2024-01-01T00-00-00Z
        args:
        - server
        - http://minio-{0...3}.minio-service.nsrs-storage.svc.cluster.local/data
        - --console-address
        - ":9001"
        env:
        - name: MINIO_ROOT_USER
          value: "admin"
        - name: MINIO_ROOT_PASSWORD
          value: "admin123456"
        - name: MINIO_STORAGE_CLASS_STANDARD
          value: "EC:2"
        ports:
        - containerPort: 9000
          name: minio
        - containerPort: 9001
          name: console
        volumeMounts:
        - name: data
          mountPath: /data
        livenessProbe:
          httpGet:
            path: /minio/health/live
            port: 9000
          initialDelaySeconds: 120
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /minio/health/ready
            port: 9000
          initialDelaySeconds: 120
          periodSeconds: 20
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 100Gi
      storageClassName: fast-ssd
```

#### 文件同步服务
```java
/**
 * 文件同步服务
 */
@Service
@Slf4j
public class FileReplicationService {
    
    @Autowired
    private MinioClient primaryMinioClient;
    
    @Autowired
    private MinioClient backupMinioClient;
    
    @Value("${minio.bucket.name}")
    private String bucketName;
    
    /**
     * 同步文件到备份存储
     */
    @Async("fileReplicationExecutor")
    public CompletableFuture<Void> replicateFile(String objectName) {
        try {
            log.debug("Starting file replication for: {}", objectName);
            
            // 从主存储下载文件
            try (InputStream inputStream = primaryMinioClient.getObject(
                    GetObjectArgs.builder()
                            .bucket(bucketName)
                            .object(objectName)
                            .build())) {
                
                // 获取文件元数据
                StatObjectResponse stat = primaryMinioClient.statObject(
                        StatObjectArgs.builder()
                                .bucket(bucketName)
                                .object(objectName)
                                .build());
                
                // 上传到备份存储
                backupMinioClient.putObject(
                        PutObjectArgs.builder()
                                .bucket(bucketName)
                                .object(objectName)
                                .stream(inputStream, stat.size(), -1)
                                .contentType(stat.contentType())
                                .userMetadata(stat.userMetadata())
                                .build());
                
                log.info("File replication completed for: {}", objectName);
                
            }
            
        } catch (Exception e) {
            log.error("File replication failed for: {}", objectName, e);
            throw new RuntimeException("File replication failed", e);
        }
        
        return CompletableFuture.completedFuture(null);
    }
    
    /**
     * 验证文件同步状态
     */
    public boolean verifyReplication(String objectName) {
        try {
            // 获取主存储文件信息
            StatObjectResponse primaryStat = primaryMinioClient.statObject(
                    StatObjectArgs.builder()
                            .bucket(bucketName)
                            .object(objectName)
                            .build());
            
            // 获取备份存储文件信息
            StatObjectResponse backupStat = backupMinioClient.statObject(
                    StatObjectArgs.builder()
                            .bucket(bucketName)
                            .object(objectName)
                            .build());
            
            // 比较文件大小和ETag
            boolean sizeMatch = primaryStat.size() == backupStat.size();
            boolean etagMatch = Objects.equals(primaryStat.etag(), backupStat.etag());
            
            if (sizeMatch && etagMatch) {
                log.debug("File replication verified for: {}", objectName);
                return true;
            } else {
                log.warn("File replication verification failed for: {} (size: {}/{}, etag: {}/{})", 
                        objectName, primaryStat.size(), backupStat.size(), 
                        primaryStat.etag(), backupStat.etag());
                return false;
            }
            
        } catch (Exception e) {
            log.error("File replication verification failed for: {}", objectName, e);
            return false;
        }
    }
    
    /**
     * 批量同步文件
     */
    @Scheduled(fixedDelay = 300000) // 每5分钟执行一次
    public void batchReplication() {
        try {
            log.debug("Starting batch file replication");
            
            // 获取主存储中的所有文件
            Iterable<Result<Item>> results = primaryMinioClient.listObjects(
                    ListObjectsArgs.builder()
                            .bucket(bucketName)
                            .recursive(true)
                            .build());
            
            List<CompletableFuture<Void>> replicationTasks = new ArrayList<>();
            
            for (Result<Item> result : results) {
                Item item = result.get();
                String objectName = item.objectName();
                
                // 检查文件是否需要同步
                if (!verifyReplication(objectName)) {
                    CompletableFuture<Void> task = replicateFile(objectName);
                    replicationTasks.add(task);
                }
            }
            
            // 等待所有同步任务完成
            CompletableFuture.allOf(replicationTasks.toArray(new CompletableFuture[0]))
                    .get(30, TimeUnit.MINUTES);
            
            log.info("Batch file replication completed, {} files processed", 
                    replicationTasks.size());
            
        } catch (Exception e) {
            log.error("Batch file replication failed", e);
        }
    }
    
    /**
     * 故障切换到备份存储
     */
    public void failoverToBackupStorage() {
        log.warn("Failing over to backup storage");
        
        try {
            // 更新配置，将备份存储设置为主存储
            // 这里可以通过配置中心动态修改存储配置
            
            // 发布存储切换事件
            ApplicationEventPublisher eventPublisher = 
                ApplicationContextHolder.getApplicationContext();
            eventPublisher.publishEvent(new StorageFailoverEvent("BACKUP"));
            
            log.info("Storage failover completed");
            
        } catch (Exception e) {
            log.error("Storage failover failed", e);
        }
    }
}

/**
 * 存储故障切换事件
 */
public class StorageFailoverEvent extends ApplicationEvent {
    private final String targetStorage;
    
    public StorageFailoverEvent(String targetStorage) {
        super(targetStorage);
        this.targetStorage = targetStorage;
    }
    
    public String getTargetStorage() {
        return targetStorage;
    }
}
```

## 网络容灾方案

### 1. 多线路接入配置

#### BGP多线路配置
```bash
# /etc/frr/frr.conf - FRRouting BGP配置
!
frr version 8.0
frr defaults traditional
!
router bgp 65001
 bgp router-id 10.1.7.1
 bgp log-neighbor-changes
 
 ! 电信线路
 neighbor 202.96.128.1 remote-as 4134
 neighbor 202.96.128.1 description "China Telecom"
 neighbor 202.96.128.1 password telecom_password
 
 ! 联通线路
 neighbor 221.5.88.1 remote-as 4837
 neighbor 221.5.88.1 description "China Unicom"
 neighbor 221.5.88.1 password unicom_password
 
 ! 移动线路
 neighbor 211.136.112.1 remote-as 9808
 neighbor 211.136.112.1 description "China Mobile"
 neighbor 211.136.112.1 password mobile_password
 
 ! 网络宣告
 network 10.1.7.0/24
 network 192.168.100.0/24
 
 ! 路由策略
 neighbor 202.96.128.1 route-map TELECOM-OUT out
 neighbor 221.5.88.1 route-map UNICOM-OUT out
 neighbor 211.136.112.1 route-map MOBILE-OUT out
!
! 路由映射
route-map TELECOM-OUT permit 10
 set local-preference 100
 set community 4134:100
!
route-map UNICOM-OUT permit 10
 set local-preference 90
 set community 4837:100
!
route-map MOBILE-OUT permit 10
 set local-preference 80
 set community 9808:100
!
line vty
!
```

#### 网络监控脚本
```bash
#!/bin/bash
# network_monitor.sh - 网络线路监控脚本

# 配置参数
TELECOM_GW="202.96.128.1"
UNICOM_GW="221.5.88.1"
MOBILE_GW="211.136.112.1"
TEST_HOSTS=("8.8.8.8" "114.114.114.114" "223.5.5.5")
LOG_FILE="/var/log/network_monitor.log"
ALERT_THRESHOLD=3  # 连续失败次数阈值

# 线路状态
declare -A line_status
declare -A failure_count

line_status["telecom"]=1
line_status["unicom"]=1
line_status["mobile"]=1

failure_count["telecom"]=0
failure_count["unicom"]=0
failure_count["mobile"]=0

# 日志函数
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

# 测试网络连通性
test_connectivity() {
    local gateway="$1"
    local line_name="$2"
    
    # 通过指定网关ping测试主机
    for host in "${TEST_HOSTS[@]}"; do
        if ping -c 3 -W 5 -I "$gateway" "$host" >/dev/null 2>&1; then
            return 0
        fi
    done
    
    return 1
}

# 检查线路状态
check_line_status() {
    local gateway="$1"
    local line_name="$2"
    
    if test_connectivity "$gateway" "$line_name"; then
        if [ "${line_status[$line_name]}" -eq 0 ]; then
            log "$line_name line recovered"
            line_status["$line_name"]=1
            failure_count["$line_name"]=0
            send_recovery_alert "$line_name"
            update_routing_policy
        fi
        failure_count["$line_name"]=0
    else
        failure_count["$line_name"]=$((failure_count["$line_name"] + 1))
        log "$line_name line test failed (${failure_count[$line_name]}/$ALERT_THRESHOLD)"
        
        if [ "${failure_count[$line_name]}" -ge "$ALERT_THRESHOLD" ] && [ "${line_status[$line_name]}" -eq 1 ]; then
            log "$line_name line is down"
            line_status["$line_name"]=0
            send_failure_alert "$line_name"
            update_routing_policy
        fi
    fi
}

# 更新路由策略
update_routing_policy() {
    log "Updating routing policy based on line status"
    
    # 计算可用线路
    local available_lines=()
    
    if [ "${line_status[telecom]}" -eq 1 ]; then
        available_lines+=("telecom")
    fi
    
    if [ "${line_status[unicom]}" -eq 1 ]; then
        available_lines+=("unicom")
    fi
    
    if [ "${line_status[mobile]}" -eq 1 ]; then
        available_lines+=("mobile")
    fi
    
    if [ ${#available_lines[@]} -eq 0 ]; then
        log "ERROR: All network lines are down!"
        send_critical_alert "All network lines are down"
        return
    fi
    
    # 根据可用线路数量调整权重
    case ${#available_lines[@]} in
        3)
            # 所有线路可用，使用默认权重
            vtysh -c "configure terminal" -c "router bgp 65001" \
                -c "neighbor 202.96.128.1 weight 100" \
                -c "neighbor 221.5.88.1 weight 90" \
                -c "neighbor 211.136.112.1 weight 80"
            ;;
        2)
            # 两条线路可用，平均分配权重
            if [[ " ${available_lines[@]} " =~ " telecom " ]] && [[ " ${available_lines[@]} " =~ " unicom " ]]; then
                vtysh -c "configure terminal" -c "router bgp 65001" \
                    -c "neighbor 202.96.128.1 weight 100" \
                    -c "neighbor 221.5.88.1 weight 100" \
                    -c "neighbor 211.136.112.1 weight 0"
            elif [[ " ${available_lines[@]} " =~ " telecom " ]] && [[ " ${available_lines[@]} " =~ " mobile " ]]; then
                vtysh -c "configure terminal" -c "router bgp 65001" \
                    -c "neighbor 202.96.128.1 weight 100" \
                    -c "neighbor 221.5.88.1 weight 0" \
                    -c "neighbor 211.136.112.1 weight 100"
            else
                vtysh -c "configure terminal" -c "router bgp 65001" \
                    -c "neighbor 202.96.128.1 weight 0" \
                    -c "neighbor 221.5.88.1 weight 100" \
                    -c "neighbor 211.136.112.1 weight 100"
            fi
            ;;
        1)
            # 只有一条线路可用，全部权重给该线路
            if [[ " ${available_lines[@]} " =~ " telecom " ]]; then
                vtysh -c "configure terminal" -c "router bgp 65001" \
                    -c "neighbor 202.96.128.1 weight 100" \
                    -c "neighbor 221.5.88.1 weight 0" \
                    -c "neighbor 211.136.112.1 weight 0"
            elif [[ " ${available_lines[@]} " =~ " unicom " ]]; then
                vtysh -c "configure terminal" -c "router bgp 65001" \
                    -c "neighbor 202.96.128.1 weight 0" \
                    -c "neighbor 221.5.88.1 weight 100" \
                    -c "neighbor 211.136.112.1 weight 0"
            else
                vtysh -c "configure terminal" -c "router bgp 65001" \
                    -c "neighbor 202.96.128.1 weight 0" \
                    -c "neighbor 221.5.88.1 weight 0" \
                    -c "neighbor 211.136.112.1 weight 100"
            fi
            ;;
    esac
    
    log "Routing policy updated, available lines: ${available_lines[*]}"
}

# 发送故障告警
send_failure_alert() {
    local line_name="$1"
    local message="Network line $line_name is down at $(date)"
    
    # 发送邮件
    echo "$message" | mail -s "Network Line Failure Alert" "admin@nsrs.com"
    
    # 发送到监控系统
    curl -X POST "http://monitoring.nsrs.com/api/alerts" \
        -H "Content-Type: application/json" \
        -d "{
            \"level\": \"critical\",
            \"message\": \"$message\",
            \"service\": \"network-$line_name\"
        }"
    
    log "Failure alert sent for $line_name line"
}

# 发送恢复通知
send_recovery_alert() {
    local line_name="$1"
    local message="Network line $line_name has recovered at $(date)"
    
    # 发送邮件
    echo "$message" | mail -s "Network Line Recovery Notification" "admin@nsrs.com"
    
    # 发送到监控系统
    curl -X POST "http://monitoring.